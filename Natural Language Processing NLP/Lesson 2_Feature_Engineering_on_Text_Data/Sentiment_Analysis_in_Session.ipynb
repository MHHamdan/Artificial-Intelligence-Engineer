{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"Sentiment_Analysis_in_Session.ipynb","provenance":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"S09sBJq30rj4"},"source":["# <font color=green>Sentiment Analysis</font>"]},{"cell_type":"markdown","metadata":{"id":"dqJtzSrq0rj5"},"source":["# <font color=orange>Click bellow for redirecting to repsective pages</font>"]},{"cell_type":"markdown","metadata":{"id":"JemLVUwz0rj6"},"source":["1. [Exploring the dataset](#Exploring-the-dataset)\n","\n","   \n","2. [Conversion of text to Cross Sectional Data](#Conversion-of-text-to-Cross-Sectional-Data)\n","\n","\n","3. [Naive Bayes Model](#Naive-Bayes-Model)"]},{"cell_type":"markdown","metadata":{"id":"WZlYaChW0rj6"},"source":["### Load the data \n","\n","##### 1 is positive Review and 0 is Negative Review"]},{"cell_type":"code","metadata":{"id":"Nb7yvEmp0rj7","executionInfo":{"status":"ok","timestamp":1603349197828,"user_tz":240,"elapsed":375,"user":{"displayName":"Mohammed Hamdan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7-0C4n1c_uDVPoywaYa91Jx17qP1YRlmJadqc=s64","userId":"00647759825092258022"}}},"source":["import pandas as pd\n","import numpy as np\n","import warnings\n","warnings.filterwarnings('ignore')\n","train_ds = pd.read_csv( \"/content/drive/My Drive/DeepLearning_Simili/Projects/Natural Language Processing NLP/Lesson 2_Feature_Engineering_on_Text_Data/data_for_sentiment_analysis\", delimiter=\"\\t\" )\n"],"execution_count":156,"outputs":[]},{"cell_type":"code","metadata":{"id":"ndfaGA-a0rj_","executionInfo":{"status":"ok","timestamp":1603349200005,"user_tz":240,"elapsed":348,"user":{"displayName":"Mohammed Hamdan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7-0C4n1c_uDVPoywaYa91Jx17qP1YRlmJadqc=s64","userId":"00647759825092258022"}},"outputId":"0d93b021-1d87-4b46-d110-750d9dbee9a4","colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["train_ds.head()"],"execution_count":157,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentiment</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>The Da Vinci Code book is just awesome.</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>this was the first clive cussler i've ever rea...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>i liked the Da Vinci Code a lot.</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>i liked the Da Vinci Code a lot.</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>I liked the Da Vinci Code but it ultimatly did...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   sentiment                                               text\n","0          1            The Da Vinci Code book is just awesome.\n","1          1  this was the first clive cussler i've ever rea...\n","2          1                   i liked the Da Vinci Code a lot.\n","3          1                   i liked the Da Vinci Code a lot.\n","4          1  I liked the Da Vinci Code but it ultimatly did..."]},"metadata":{"tags":[]},"execution_count":157}]},{"cell_type":"code","metadata":{"id":"ydiutA2_4QMe"},"source":["train_ds.tail()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"heciHRp4vHru"},"source":["train_ds.columns"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t9rX2oj_0rkD"},"source":["train_ds[train_ds.sentiment == 1][0:5]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cL5DXU4n0rkG"},"source":["train_ds[train_ds.sentiment == 0][0:5]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"82IQ3gu30rkJ"},"source":["# Exploring the dataset"]},{"cell_type":"code","metadata":{"id":"KIdwDDqO0rkK","executionInfo":{"status":"ok","timestamp":1603349204357,"user_tz":240,"elapsed":346,"user":{"displayName":"Mohammed Hamdan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7-0C4n1c_uDVPoywaYa91Jx17qP1YRlmJadqc=s64","userId":"00647759825092258022"}},"outputId":"c6c783c5-0d6e-4954-b257-91aaac75c161","colab":{"base_uri":"https://localhost:8080/","height":170}},"source":["train_ds.info()"],"execution_count":158,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 6918 entries, 0 to 6917\n","Data columns (total 2 columns):\n"," #   Column     Non-Null Count  Dtype \n","---  ------     --------------  ----- \n"," 0   sentiment  6918 non-null   int64 \n"," 1   text       6918 non-null   object\n","dtypes: int64(1), object(1)\n","memory usage: 108.2+ KB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VHm7h_M7vE1O"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DBQt_PMc0rkM"},"source":["import matplotlib.pyplot as plt\n","import seaborn as sn\n","%matplotlib inline\n","plt.figure( figsize=(6,5))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YOx-NSyN0rkP"},"source":["# create count plot\n","ax = sn.countplot(x='sentiment', data=train_ds)\n","# annotate\n","for p in ax.patches:\n","    ax.annotate(p.get_height(), (p.get_x()+0.1, p.get_height()+50))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V5fvVeOm0rkS"},"source":["review_volume = train_ds[\"sentiment\"].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cft0vdXM0rkV"},"source":["(review_volume[0]/train_ds.shape[0])*100"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5OClVY_s0rkX"},"source":["(review_volume[1]/train_ds.shape[0])*100"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CyrFy-Od0rkZ"},"source":["### Inference : The no. of data points for both the catagres are balanced and hence it is good to proceed with Classification"]},{"cell_type":"markdown","metadata":{"id":"TXkloHyU0rkd"},"source":["# Conversion of text to Cross Sectional Data\n","\n","* Count Vector Model (Bag Of Word)\n","* Term Frequency model\n","* Term Frequency - Inverse Document Frequency (TF-IDF) model\n","* Ngram(s) model"]},{"cell_type":"markdown","metadata":{"id":"hmq2Bvy30rke"},"source":["### Count Vector Model"]},{"cell_type":"code","metadata":{"id":"t9uNRyE80rke"},"source":["from sklearn.feature_extraction.text import CountVectorizer\n","# Initialize the CountVectorizer\n","count_vectorizer = CountVectorizer()\n","# Create the dictionary from the corpus\n","feature_vector = count_vectorizer.fit(train_ds.text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d37kB7Ks0rkh"},"source":["feature_vector  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZToOyKZF0rkk"},"source":["# Get the feature names\n","features = feature_vector.get_feature_names()\n","print( \"Total number of features: \", len(features))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AV7jGRz40rkm"},"source":["import random\n","sampleWords = random.sample(features, 3)\n","print(sampleWords, end=' ')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sBsCUdKh0rkp"},"source":["train_ds_features = count_vectorizer.transform( train_ds.text )\n","type(train_ds_features)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pb-HPvZc0rks"},"source":["print(train_ds_features.shape)\n","print(train_ds_features[0:1,0:1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xUef1uuu0rkv"},"source":["## Displaying Document Vectors"]},{"cell_type":"code","metadata":{"id":"6AzbPKdj0rkv"},"source":["# Converting the matrix to a dataframe\n","train_ds_df = pd.DataFrame(train_ds_features.todense())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pJD_XWwA0rkx"},"source":["# Setting the column names to the features i.e. words\n","train_ds_df.columns = features"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DhPMPAut0rk0"},"source":["train_ds_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eixV_Had0rk2"},"source":["train_ds[0:1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1Ss76oo30rk4"},"source":["train_ds_df.iloc[0:1, 150:157]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gAD1dLhh0rk7"},"source":["### Removing low frequency words"]},{"cell_type":"code","metadata":{"id":"Qdh1xnJH0rk7"},"source":["# summing up the occurances of features column wise\n","features_counts = np.sum( train_ds_features.toarray(), axis = 0 )\n","features_counts"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HWwsrM4p0rk-"},"source":["feature_counts_df = pd.DataFrame( dict( features = features,\n","counts = features_counts ) )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gfj1ufpA0rlA"},"source":["feature_counts_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KDC5_QM10rlD"},"source":["plt.figure( figsize=(12,5))\n","plt.hist(feature_counts_df.counts, bins=50, range = (0, 2000));\n","plt.xlabel( 'Frequency of words' )\n","plt.ylabel( 'Density' );"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f8rOkRjO0rlG"},"source":["# Initialize the CountVectorizer\n","count_vectorizer = CountVectorizer(max_features=1000)\n","# Create the dictionary from the corpus\n","feature_vector = count_vectorizer.fit( train_ds.text )\n","# Get the feature names\n","features = feature_vector.get_feature_names()\n","# Transform the document into vectors\n","train_ds_features = count_vectorizer.transform( train_ds.text )\n","# Count the frequency of the features\n","features_counts = np.sum( train_ds_features.toarray(), axis = 0 )\n","feature_counts = pd.DataFrame( dict( features = features,counts = features_counts ) )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZKSJBu360rlI"},"source":["feature_counts"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uaZZhEPI0rlK"},"source":["### Removing Stop Words "]},{"cell_type":"code","metadata":{"id":"ou_CWTZV0rlK"},"source":["from sklearn.feature_extraction import text\n","my_stop_words = text.ENGLISH_STOP_WORDS"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BMjyPy7S0rlM"},"source":["len(my_stop_words) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XrHixHNa0rlP"},"source":["# Adding custom words to the list of stop words\n","my_stop_words = text.ENGLISH_STOP_WORDS.union( ['harry', 'potter', 'code', 'vinci', 'da','harry', 'mountain', 'movie', 'movies'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OcvN021C8hJi"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QTuwky2e0rlR"},"source":["len(my_stop_words)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-fIAuqFG0rlT"},"source":["## Creating Count Vectors with removal of Stop Words and Considering highly frequent words "]},{"cell_type":"code","metadata":{"id":"o9yc035d0rlT"},"source":["count_vectorizer = CountVectorizer( stop_words = my_stop_words,max_features = 1000 )\n","feature_vector = count_vectorizer.fit( train_ds.text )\n","train_ds_features = count_vectorizer.transform( train_ds.text )\n","features = feature_vector.get_feature_names()\n","features_counts = np.sum( train_ds_features.toarray(), axis = 0 )\n","feature_counts = pd.DataFrame( dict( features = features,counts = features_counts ) )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C3fNIopw0rlV"},"source":["feature_counts.sort_values( \"counts\", ascending = False )[0:15]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"McVo4FQJ0rlY"},"source":["from nltk.stem.snowball import PorterStemmer\n","stemmer = PorterStemmer()\n","analyzer = CountVectorizer().build_analyzer()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RWmYvN4V0rla"},"source":["def stemmed_words(doc):\n","    ### Stemming of words\n","    stemmed_words = (stemmer.stem(w) for w in analyzer(doc))\n","    ### Remove the words in stop words list\n","    non_stop_words = [ word for word in list(set(stemmed_words) - set(my_stop_words)) ]\n","    return non_stop_words"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gErip_iz0rlc"},"source":["### Features with Stemmed Words and filtered by Stop Words which are highly frequent word  "]},{"cell_type":"code","metadata":{"id":"-R7wXV8E0rlc"},"source":["count_vectorizer = CountVectorizer( analyzer=stemmed_words, max_features = 1000)\n","feature_vector = count_vectorizer.fit( train_ds.text )\n","train_ds_features = count_vectorizer.transform( train_ds.text )\n","features = feature_vector.get_feature_names()\n","features_counts = np.sum( train_ds_features.toarray(), axis = 0 )\n","feature_counts = pd.DataFrame( dict( features = features,\n","counts = features_counts ) )\n","feature_counts.sort_values( \"counts\", ascending = False )[0:15]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aiAxaqoa0rle"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U0o4lYtH0rlg"},"source":["# Convert the document vector matrix into dataframe\n","train_ds_df = pd.DataFrame(train_ds_features.todense())\n","# Assign the features names to the column\n","train_ds_df.columns = features\n","# Assign the sentiment labels to the train_ds\n","train_ds_df['sentiment'] = train_ds.sentiment"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QbZc7wqi0rli"},"source":["from sklearn.model_selection import train_test_split\n","train_X, test_X, train_y, test_y = train_test_split( train_ds_features,train_ds.sentiment,test_size = 0.3,random_state = 42 )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bv5smYlw0rll"},"source":["### Naive Bayes Model"]},{"cell_type":"code","metadata":{"id":"fC0RFOYY0rll"},"source":["from sklearn.naive_bayes import BernoulliNB\n","nb_clf = BernoulliNB()\n","nb_clf.fit( train_X.toarray(), train_y )\n","\n","test_ds_predicted = nb_clf.predict( test_X.toarray() )\n","\n","from sklearn import metrics\n","print( metrics.classification_report( test_y, test_ds_predicted ) )\n","\n","from sklearn import metrics\n","cm = metrics.confusion_matrix( test_y, test_ds_predicted )\n","sn.heatmap(cm, annot=True, fmt='.2f' );"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LBKWG3Mb0rln"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2J1tWSiG0rlq"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M0ZDbkmB0rlr"},"source":["COLUMN_NAMES = [\"Process\",\"Model Name\", \"F1 Scores\",\"Range of F1 Scores\",\"Std Deviation of F1 Scores\"]\n","df_model_selection = pd.DataFrame(columns=COLUMN_NAMES)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A14dj5er0rlu"},"source":["df_model_selection"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z13WglFc0rlw"},"source":["from sklearn.model_selection import StratifiedKFold\n","from sklearn.metrics import f1_score\n","from sklearn import metrics"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"no_J3r250rlz"},"source":["def stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y):\n","    global df_model_selection\n","    \n","    skf = StratifiedKFold(n_splits, random_state=29)\n","    \n","    weighted_f1_score = []\n","    print(skf.split(X,y))\n","    for train_index, val_index in skf.split(X,y):\n","        X_train, X_test = X[train_index], X[val_index] \n","        y_train, y_test = y[train_index], y[val_index]\n","        \n","        \n","        model_obj.fit(X_train, y_train)##### HERE ###\n","        test_ds_predicted = model_obj.predict( X_test ) ##### HERE ####   \n","        #print( metrics.classification_report( y_test, test_ds_predicted ) )    \n","        weighted_f1_score.append(round(f1_score(y_test, test_ds_predicted , average='weighted'),2))\n","        \n","    sd_weighted_f1_score = np.std(weighted_f1_score, ddof=1)\n","    range_of_f1_scores = \"{}-{}\".format(min(weighted_f1_score),max(weighted_f1_score))    \n","    df_model_selection = pd.concat([df_model_selection,pd.DataFrame([[process,model_name,sorted(weighted_f1_score),range_of_f1_scores,sd_weighted_f1_score]], columns =COLUMN_NAMES) ])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dlo09a5_0rl0"},"source":["from sklearn.naive_bayes import BernoulliNB\n","nb_clf = BernoulliNB()\n","nb_clf.fit( train_X.toarray(), train_y )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Te02D81q0rl2"},"source":["model_obj = nb_clf\n","model_name = \"Binomial Naive Bayes Classifier\"\n","process = \"Bag Of Words with NLTK Stemming\"\n","n_splits = 5\n","X = train_ds_features.toarray()\n","y = train_ds.sentiment\n","stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n","\n","\n","df_model_selection"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JEFN_J840rl5"},"source":["# Logistic Regression "]},{"cell_type":"code","metadata":{"id":"GhKorLGq0rl6"},"source":["from sklearn.linear_model import LogisticRegression\n","logreg = LogisticRegression()\n","logreg.fit(train_X.toarray(), train_y)\n","test_ds_predicted = logreg.predict( test_X.toarray() )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WipYTgOU0rl7"},"source":["from sklearn import metrics\n","print( metrics.classification_report( test_y, test_ds_predicted ) )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M8BEAJ6R0rl_"},"source":["model_obj = logreg\n","model_name = \"Logistic Regression\"\n","process = \"Bag Of Words with NLTK Stemming\"\n","n_splits = 5\n","X = train_ds_features.toarray()\n","y = train_ds.sentiment\n","stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n","df_model_selection"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jgxDOZvu0rmB"},"source":["# Decision Tree"]},{"cell_type":"code","metadata":{"id":"tZnlTX8B0rmC"},"source":["from sklearn.tree import DecisionTreeClassifier\n","decision_tree = DecisionTreeClassifier(criterion='entropy')\n","\n","decision_tree.fit(train_X.toarray(), train_y)\n","test_ds_predicted = decision_tree.predict( test_X.toarray() )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"flasX-qn0rmE"},"source":["from sklearn import metrics\n","print( metrics.classification_report( test_y, test_ds_predicted ) )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e2FUGrm60rmG"},"source":["model_obj = decision_tree\n","model_name = \"Decission Tree\"\n","process = \"Bag Of Words with NLTK Stemming\"\n","n_splits = 5\n","X = train_ds_features.toarray()\n","y = train_ds.sentiment\n","stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n","df_model_selection"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GyXz3E7f0rmL"},"source":["# Random Forest"]},{"cell_type":"code","metadata":{"id":"nmdbPz6X0rmM"},"source":["from sklearn.ensemble import RandomForestClassifier\n","random_forest = RandomForestClassifier(n_estimators=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T1L_7Dpt0rmN"},"source":["random_forest.fit(train_X.toarray(), train_y)\n","test_ds_predicted = random_forest.predict( test_X.toarray() )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tnb27Eob0rmQ"},"source":["from sklearn import metrics\n","print( metrics.classification_report( test_y, test_ds_predicted ) )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bW-eLuhg0rmS"},"source":["model_obj = random_forest\n","model_name = \"Random Forest\"\n","process = \"Bag Of Words with NLTK Stemming\"\n","n_splits = 5\n","X = train_ds_features.toarray()\n","y = train_ds.sentiment\n","stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n","df_model_selection"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EO8svL2Y0rmT"},"source":["# XG Boost"]},{"cell_type":"code","metadata":{"id":"YxsidOGx0rmU"},"source":["from xgboost import XGBClassifier\n","xgboost = XGBClassifier()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e5xipHRo0rmW"},"source":["xgboost.fit(train_X.toarray(), train_y)\n","test_ds_predicted = xgboost.predict( test_X.toarray() )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"roRCxN8M0rmY"},"source":["from sklearn import metrics\n","print( metrics.classification_report( test_y, test_ds_predicted ) )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zI_Nz6Wn0rma"},"source":["model_obj = xgboost\n","model_name = \"XG Boost\"\n","process = \"Bag Of Words with NLTK Stemming\"\n","n_splits = 5\n","X = train_ds_features.toarray()\n","y = train_ds.sentiment\n","stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n","df_model_selection"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rYpxXpWA0rmc"},"source":["# SGD Classifier"]},{"cell_type":"code","metadata":{"id":"7mkEzJbU0rmc"},"source":["from sklearn.linear_model import SGDClassifier\n","from sklearn.multiclass import OneVsRestClassifier\n","\n","sgd = OneVsRestClassifier(SGDClassifier())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E-qgQWpV0rmd"},"source":["sgd.fit(train_X.toarray(), train_y)\n","test_ds_predicted = sgd.predict( test_X.toarray() )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tWcGgTDB0rmf"},"source":["from sklearn import metrics\n","print( metrics.classification_report( test_y, test_ds_predicted ) )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WXfraBe90rmg"},"source":["model_obj = sgd\n","model_name = \"Stochastic Gradient Descent\"\n","process = \"Bag Of Words with NLTK Stemming\"\n","n_splits = 5\n","X = train_ds_features.toarray()\n","y = train_ds.sentiment\n","stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n","df_model_selection"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NA05kpn00rmi"},"source":["# Gaussian Process Classifier"]},{"cell_type":"code","metadata":{"id":"p0N3cuFZ0rmi"},"source":["from sklearn.gaussian_process import GaussianProcessClassifier\n","gausian_process = GaussianProcessClassifier()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0DDf74qe0rml"},"source":["gausian_process.fit(train_X.toarray(), train_y)\n","test_ds_predicted = gausian_process.predict( test_X.toarray() )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wUXE8y-M0rmn"},"source":["from sklearn import metrics\n","print( metrics.classification_report( test_y, test_ds_predicted ) )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q4yT-eL90rmp"},"source":["model_obj = gausian_process\n","model_name = \"Gausian Process\"\n","process = \"Bag Of Words with NLTK Stemming\"\n","n_splits = 5\n","X = train_ds_features.toarray()\n","y = train_ds.sentiment\n","stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n","df_model_selection"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dHN_DYoP0rmq"},"source":["# KNN Classifier"]},{"cell_type":"code","metadata":{"id":"znzMSBJ30rmr"},"source":["from sklearn.neighbors import KNeighborsClassifier\n","knn = KNeighborsClassifier()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hUZcKP6D0rms"},"source":["knn.fit(train_X.toarray(), train_y)\n","test_ds_predicted = knn.predict( test_X.toarray() )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PlVbnd4Q0rmu"},"source":["from sklearn import metrics\n","print( metrics.classification_report( test_y, test_ds_predicted ) )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_bD7wikN0rmv"},"source":["model_obj = knn\n","model_name = \"K Nearst Neighbour\"\n","process = \"Bag Of Words with NLTK Stemming\"\n","n_splits = 5\n","X = train_ds_features.toarray()\n","y = train_ds.sentiment\n","stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n","df_model_selection"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n2A-Ozms0rmx"},"source":["# Linear Discriminant Analysis"]},{"cell_type":"code","metadata":{"id":"VoQjEV2G0rmy"},"source":["from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","lda = LinearDiscriminantAnalysis()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"El2T2w6u0rmz"},"source":["lda.fit(train_X.toarray(), train_y)\n","test_ds_predicted = lda.predict( test_X.toarray() )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M7Qw6VQU0rm1"},"source":["from sklearn import metrics\n","print( metrics.classification_report( test_y, test_ds_predicted ) )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VhnUsJpx0rm3"},"source":["model_obj = lda\n","model_name = \"Linear Discriminant Analysis\"\n","process = \"Bag Of Words with NLTK Stemming\"\n","n_splits = 5\n","X = train_ds_features.toarray()\n","y = train_ds.sentiment\n","stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n","df_model_selection"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3VweWpXP0rm5"},"source":["# Support Vector Machine"]},{"cell_type":"code","metadata":{"id":"p-LwEYAF0rm5"},"source":["from sklearn.svm import SVC\n","svm = SVC()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xLdVx3eb0rm7"},"source":["svm.fit(train_X.toarray(), train_y)\n","test_ds_predicted = svm.predict( test_X.toarray() )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7LI3AJFG0rm9"},"source":["model_obj = svm\n","model_name = \"Support Vector Machine\"\n","process = \"Bag Of Words with NLTK Stemming\"\n","n_splits = 5\n","X = train_ds_features.toarray()\n","y = train_ds.sentiment\n","stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n","df_model_selection"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EM10OfoX0rm_"},"source":["df_model_selection.to_csv(\"Model_statistics.csv\",index = False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wCioQQQ50rnA"},"source":[""],"execution_count":null,"outputs":[]}]}