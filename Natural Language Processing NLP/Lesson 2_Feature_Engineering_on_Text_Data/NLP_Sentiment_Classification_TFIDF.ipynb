{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The Da Vinci Code book is just awesome.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>this was the first clive cussler i've ever rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>i liked the Da Vinci Code a lot.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>i liked the Da Vinci Code a lot.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>I liked the Da Vinci Code but it ultimatly did...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text\n",
       "0          1            The Da Vinci Code book is just awesome.\n",
       "1          1  this was the first clive cussler i've ever rea...\n",
       "2          1                   i liked the Da Vinci Code a lot.\n",
       "3          1                   i liked the Da Vinci Code a lot.\n",
       "4          1  I liked the Da Vinci Code but it ultimatly did..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "train_ds = pd.read_csv( \"data_for_sentiment_analysis\", delimiter=\"\\t\" )\n",
    "train_ds.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_selection = pd.read_csv(\"Model_statistics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMN_NAMES = [\"Process\",\"Model Name\", \"F1 Scores\",\"Range of F1 Scores\",\"Std Deviation of F1 Scores\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import metrics\n",
    "def stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y):\n",
    "    global df_model_selection\n",
    "    skf = StratifiedKFold(n_splits=5, random_state=29)\n",
    "    weighted_f1_score = []\n",
    "    for train_index, val_index in skf.split(X,y):\n",
    "        X_train, X_test = X[train_index], X[val_index] \n",
    "        y_train, y_test = y[train_index], y[val_index]\n",
    "        model_obj.fit(X_train, y_train)##### HERE ###\n",
    "        test_ds_predicted = model_obj.predict( X_test ) ##### HERE ####   \n",
    "        #print( metrics.classification_report( y_test, test_ds_predicted ) )    \n",
    "        weighted_f1_score.append(round(f1_score(y_test, test_ds_predicted , average='weighted'),2))\n",
    "        \n",
    "    sd_weighted_f1_score = np.std(weighted_f1_score, ddof=1)\n",
    "    range_of_f1_scores = \"{}-{}\".format(min(weighted_f1_score),max(weighted_f1_score))    \n",
    "    df_model_selection = pd.concat([df_model_selection,pd.DataFrame([[process,model_name,sorted(weighted_f1_score),range_of_f1_scores,sd_weighted_f1_score]], columns =COLUMN_NAMES) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few stop words:  ['another', 'rather', 'these', 'mine', 'nobody', 'twelve', 'his', 'forty', 'became', 'us']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "my_stop_words = text.ENGLISH_STOP_WORDS\n",
    "#Printing first few stop words\n",
    "print(\"Few stop words: \", list(my_stop_words)[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding custom words to the list of stop words\n",
    "my_stop_words = text.ENGLISH_STOP_WORDS.union( ['harry', 'potter', 'code', 'vinci', 'da',\n",
    "'harri', 'mountain', 'movie', 'movies'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting stop words list\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vectorizer = CountVectorizer( stop_words = my_stop_words,\n",
    "max_features = 1000 )\n",
    "feature_vector = count_vectorizer.fit( train_ds.text )\n",
    "train_ds_features = count_vectorizer.transform( train_ds.text )\n",
    "features = feature_vector.get_feature_names()\n",
    "features_counts = np.sum( train_ds_features.toarray(), axis = 0 )\n",
    "feature_counts = pd.DataFrame( dict( features = features,\n",
    "counts = features_counts ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning - Stemming or Lemmatization\n",
    "\n",
    "### To get words into root form and hence in a motivation of decreasing few more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. PorterStemmer\n",
    "#2. LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "\n",
    "\n",
    "#Custom function for stemming and stop word removal\n",
    "def stemmed_words(doc):\n",
    "    ### Stemming of words\n",
    "    stemmed_words = (stemmer.stem(w) for w in analyzer(doc))\n",
    "    ### Remove the words in stop words list\n",
    "    non_stop_words = [ word for word in list(set(stemmed_words) - set(my_stop_words)) ]\n",
    "    return non_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>brokeback</td>\n",
       "      <td>1930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>love</td>\n",
       "      <td>1837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>suck</td>\n",
       "      <td>1378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>922</th>\n",
       "      <td>wa</td>\n",
       "      <td>1142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>awesom</td>\n",
       "      <td>1116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>mission</td>\n",
       "      <td>1090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>imposs</td>\n",
       "      <td>1090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>movi</td>\n",
       "      <td>1052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>like</td>\n",
       "      <td>823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>hate</td>\n",
       "      <td>636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>becaus</td>\n",
       "      <td>524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>realli</td>\n",
       "      <td>370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794</th>\n",
       "      <td>stupid</td>\n",
       "      <td>364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>know</td>\n",
       "      <td>354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>read</td>\n",
       "      <td>284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      features  counts\n",
       "80   brokeback    1930\n",
       "406       love    1837\n",
       "801       suck    1378\n",
       "922         wa    1142\n",
       "43      awesom    1116\n",
       "432    mission    1090\n",
       "344     imposs    1090\n",
       "438       movi    1052\n",
       "392       like     823\n",
       "298       hate     636\n",
       "54      becaus     524\n",
       "602     realli     370\n",
       "794     stupid     364\n",
       "378       know     354\n",
       "597       read     284"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer( analyzer=stemmed_words,\n",
    "max_features = 1000)\n",
    "feature_vector = count_vectorizer.fit( train_ds.text )\n",
    "train_ds_features = count_vectorizer.transform( train_ds.text )\n",
    "features = feature_vector.get_feature_names()\n",
    "features_counts = np.sum( train_ds_features.toarray(), axis = 0 )\n",
    "feature_counts = pd.DataFrame( dict( features = features,\n",
    "counts = features_counts ) )\n",
    "feature_counts.sort_values( \"counts\", ascending = False )[0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer( analyzer=stemmed_words,max_features = 1000)\n",
    "\n",
    "\n",
    "feature_vector = tfidf_vectorizer.fit( train_ds.text )\n",
    "train_ds_features = tfidf_vectorizer.transform( train_ds.text )\n",
    "features = feature_vector.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10',\n",
       " '17',\n",
       " '33',\n",
       " '6th',\n",
       " 'abl',\n",
       " 'absolut',\n",
       " 'absurd',\n",
       " 'academi',\n",
       " 'accept',\n",
       " 'accompani',\n",
       " 'ach',\n",
       " 'acn',\n",
       " 'act',\n",
       " 'action',\n",
       " 'actor',\n",
       " 'actual',\n",
       " 'admir',\n",
       " 'ador',\n",
       " 'adult',\n",
       " 'ago',\n",
       " 'agre',\n",
       " 'alreadi',\n",
       " 'alway',\n",
       " 'amaz',\n",
       " 'ang',\n",
       " 'angel',\n",
       " 'ani',\n",
       " 'anim',\n",
       " 'anyon',\n",
       " 'anyth',\n",
       " 'appar',\n",
       " 'appeal',\n",
       " 'articl',\n",
       " 'asian',\n",
       " 'ask',\n",
       " 'asleep',\n",
       " 'ass',\n",
       " 'attempt',\n",
       " 'attract',\n",
       " 'audrey',\n",
       " 'author',\n",
       " 'aw',\n",
       " 'award',\n",
       " 'awesom',\n",
       " 'awesomest',\n",
       " 'azkaban',\n",
       " 'bad',\n",
       " 'ball',\n",
       " 'ban',\n",
       " 'bang',\n",
       " 'basic',\n",
       " 'bean',\n",
       " 'beat',\n",
       " 'beauti',\n",
       " 'becaus',\n",
       " 'becom',\n",
       " 'befor',\n",
       " 'begin',\n",
       " 'believ',\n",
       " 'besid',\n",
       " 'best',\n",
       " 'better',\n",
       " 'bias',\n",
       " 'big',\n",
       " 'bit',\n",
       " 'bitch',\n",
       " 'black',\n",
       " 'blame',\n",
       " 'blond',\n",
       " 'blood',\n",
       " 'board',\n",
       " 'bobbypin',\n",
       " 'bodi',\n",
       " 'bogu',\n",
       " 'bonker',\n",
       " 'book',\n",
       " 'bore',\n",
       " 'bought',\n",
       " 'boycot',\n",
       " 'brilliant',\n",
       " 'brokeback',\n",
       " 'brown',\n",
       " 'btw',\n",
       " 'bullshit',\n",
       " 'butt',\n",
       " 'buy',\n",
       " 'bye',\n",
       " 'came',\n",
       " 'capot',\n",
       " 'car',\n",
       " 'care',\n",
       " 'case',\n",
       " 'catch',\n",
       " 'catcher',\n",
       " 'challeng',\n",
       " 'chang',\n",
       " 'charact',\n",
       " 'children',\n",
       " 'chines',\n",
       " 'choic',\n",
       " 'chri',\n",
       " 'christian',\n",
       " 'christma',\n",
       " 'class',\n",
       " 'clean',\n",
       " 'club',\n",
       " 'cock',\n",
       " 'cocktail',\n",
       " 'cold',\n",
       " 'colleg',\n",
       " 'colour',\n",
       " 'combin',\n",
       " 'come',\n",
       " 'comment',\n",
       " 'commun',\n",
       " 'compar',\n",
       " 'conclus',\n",
       " 'conquer',\n",
       " 'consid',\n",
       " 'controversi',\n",
       " 'convers',\n",
       " 'cool',\n",
       " 'copi',\n",
       " 'costum',\n",
       " 'count',\n",
       " 'coupl',\n",
       " 'cours',\n",
       " 'cowboy',\n",
       " 'coz',\n",
       " 'crap',\n",
       " 'crappi',\n",
       " 'crash',\n",
       " 'craze',\n",
       " 'crazi',\n",
       " 'creatur',\n",
       " 'cri',\n",
       " 'cring',\n",
       " 'critic',\n",
       " 'cruis',\n",
       " 'cultur',\n",
       " 'cut',\n",
       " 'cute',\n",
       " 'cuz',\n",
       " 'dad',\n",
       " 'damn',\n",
       " 'dan',\n",
       " 'danc',\n",
       " 'daniel',\n",
       " 'dash',\n",
       " 'date',\n",
       " 'day',\n",
       " 'deal',\n",
       " 'death',\n",
       " 'decent',\n",
       " 'decid',\n",
       " 'deep',\n",
       " 'defin',\n",
       " 'definit',\n",
       " 'dementor',\n",
       " 'demon',\n",
       " 'depress',\n",
       " 'deserv',\n",
       " 'desper',\n",
       " 'despis',\n",
       " 'dick',\n",
       " 'dictat',\n",
       " 'did',\n",
       " 'didn',\n",
       " 'didnt',\n",
       " 'die',\n",
       " 'differ',\n",
       " 'disappoint',\n",
       " 'discuss',\n",
       " 'dislik',\n",
       " 'disney',\n",
       " 'doe',\n",
       " 'doesn',\n",
       " 'don',\n",
       " 'donkey',\n",
       " 'dont',\n",
       " 'dork',\n",
       " 'draco',\n",
       " 'drag',\n",
       " 'dragon',\n",
       " 'draw',\n",
       " 'dream',\n",
       " 'dress',\n",
       " 'drive',\n",
       " 'dudee',\n",
       " 'dumb',\n",
       " 'dure',\n",
       " 'dvd',\n",
       " 'eat',\n",
       " 'educ',\n",
       " 'egg',\n",
       " 'els',\n",
       " 'emma',\n",
       " 'empti',\n",
       " 'end',\n",
       " 'enjoy',\n",
       " 'equal',\n",
       " 'eragon',\n",
       " 'erm',\n",
       " 'escapad',\n",
       " 'especi',\n",
       " 'event',\n",
       " 'everi',\n",
       " 'everybodi',\n",
       " 'everyon',\n",
       " 'everyth',\n",
       " 'evil',\n",
       " 'exampl',\n",
       " 'excel',\n",
       " 'excit',\n",
       " 'expect',\n",
       " 'explain',\n",
       " 'exquisit',\n",
       " 'extrem',\n",
       " 'eye',\n",
       " 'eyr',\n",
       " 'fabul',\n",
       " 'fact',\n",
       " 'fall',\n",
       " 'fan',\n",
       " 'fandom',\n",
       " 'fanfic',\n",
       " 'fanfict',\n",
       " 'fantasi',\n",
       " 'far',\n",
       " 'fat',\n",
       " 'fault',\n",
       " 'favor',\n",
       " 'favorit',\n",
       " 'favourit',\n",
       " 'feel',\n",
       " 'felicia',\n",
       " 'fell',\n",
       " 'felt',\n",
       " 'fic',\n",
       " 'figur',\n",
       " 'film',\n",
       " 'final',\n",
       " 'finish',\n",
       " 'firework',\n",
       " 'fit',\n",
       " 'flick',\n",
       " 'food',\n",
       " 'forgotten',\n",
       " 'form',\n",
       " 'franchis',\n",
       " 'freak',\n",
       " 'freakin',\n",
       " 'friday',\n",
       " 'friend',\n",
       " 'friendship',\n",
       " 'fuck',\n",
       " 'fun',\n",
       " 'funni',\n",
       " 'funniest',\n",
       " 'futur',\n",
       " 'gaither',\n",
       " 'game',\n",
       " 'gari',\n",
       " 'gay',\n",
       " 'geek',\n",
       " 'gener',\n",
       " 'genr',\n",
       " 'georgia',\n",
       " 'gift',\n",
       " 'gin',\n",
       " 'girl',\n",
       " 'given',\n",
       " 'glad',\n",
       " 'goblet',\n",
       " 'god',\n",
       " 'goin',\n",
       " 'gonna',\n",
       " 'good',\n",
       " 'gorgeou',\n",
       " 'got',\n",
       " 'goth',\n",
       " 'gotta',\n",
       " 'grab',\n",
       " 'great',\n",
       " 'groan',\n",
       " 'gun',\n",
       " 'guy',\n",
       " 'ha',\n",
       " 'half',\n",
       " 'hall',\n",
       " 'halloween',\n",
       " 'hand',\n",
       " 'hank',\n",
       " 'happen',\n",
       " 'happi',\n",
       " 'hard',\n",
       " 'hardcor',\n",
       " 'hat',\n",
       " 'hate',\n",
       " 'haunt',\n",
       " 'haven',\n",
       " 'head',\n",
       " 'hear',\n",
       " 'heard',\n",
       " 'heart',\n",
       " 'heath',\n",
       " 'hedg',\n",
       " 'hell',\n",
       " 'hella',\n",
       " 'help',\n",
       " 'hero',\n",
       " 'het',\n",
       " 'hey',\n",
       " 'hi',\n",
       " 'hide',\n",
       " 'hill',\n",
       " 'hip',\n",
       " 'hoffman',\n",
       " 'hogwart',\n",
       " 'hold',\n",
       " 'hollywood',\n",
       " 'home',\n",
       " 'homophob',\n",
       " 'homosexu',\n",
       " 'honor',\n",
       " 'hoot',\n",
       " 'hoover',\n",
       " 'hope',\n",
       " 'horribl',\n",
       " 'hot',\n",
       " 'hour',\n",
       " 'hous',\n",
       " 'hp',\n",
       " 'hung',\n",
       " 'hype',\n",
       " 'icon',\n",
       " 'idea',\n",
       " 'idiot',\n",
       " 'idk',\n",
       " 'ignor',\n",
       " 'iii',\n",
       " 'im',\n",
       " 'imag',\n",
       " 'immedi',\n",
       " 'imposs',\n",
       " 'inaccur',\n",
       " 'includ',\n",
       " 'incred',\n",
       " 'independ',\n",
       " 'industri',\n",
       " 'insan',\n",
       " 'instead',\n",
       " 'invis',\n",
       " 'involv',\n",
       " 'isn',\n",
       " 'issu',\n",
       " 'ive',\n",
       " 'jack',\n",
       " 'jake',\n",
       " 'jane',\n",
       " 'jesu',\n",
       " 'job',\n",
       " 'john',\n",
       " 'johnni',\n",
       " 'join',\n",
       " 'joke',\n",
       " 'just',\n",
       " 'kate',\n",
       " 'kelsi',\n",
       " 'key',\n",
       " 'kick',\n",
       " 'kid',\n",
       " 'kind',\n",
       " 'kinda',\n",
       " 'kirsten',\n",
       " 'kiss',\n",
       " 'knew',\n",
       " 'knight',\n",
       " 'know',\n",
       " 'la',\n",
       " 'lah',\n",
       " 'laid',\n",
       " 'lame',\n",
       " 'laugh',\n",
       " 'leah',\n",
       " 'lee',\n",
       " 'left',\n",
       " 'let',\n",
       " 'level',\n",
       " 'librari',\n",
       " 'lie',\n",
       " 'life',\n",
       " 'like',\n",
       " 'listen',\n",
       " 'lit',\n",
       " 'littl',\n",
       " 'live',\n",
       " 'll',\n",
       " 'loath',\n",
       " 'lol',\n",
       " 'long',\n",
       " 'look',\n",
       " 'lord',\n",
       " 'lost',\n",
       " 'lot',\n",
       " 'lousi',\n",
       " 'love',\n",
       " 'lubb',\n",
       " 'luck',\n",
       " 'luv',\n",
       " 'lynn',\n",
       " 'magic',\n",
       " 'main',\n",
       " 'mainstream',\n",
       " 'major',\n",
       " 'majorli',\n",
       " 'make',\n",
       " 'malfoy',\n",
       " 'man',\n",
       " 'mani',\n",
       " 'marcia',\n",
       " 'materi',\n",
       " 'matter',\n",
       " 'mayb',\n",
       " 'mean',\n",
       " 'meet',\n",
       " 'men',\n",
       " 'mention',\n",
       " 'mi3',\n",
       " 'minut',\n",
       " 'mirror',\n",
       " 'miss',\n",
       " 'mission',\n",
       " 'mom',\n",
       " 'money',\n",
       " 'month',\n",
       " 'mother',\n",
       " 'mouth',\n",
       " 'movi',\n",
       " 'mr',\n",
       " 'mtv',\n",
       " 'murderbal',\n",
       " 'music',\n",
       " 'narnia',\n",
       " 'nc',\n",
       " 'nearli',\n",
       " 'need',\n",
       " 'nerd',\n",
       " 'new',\n",
       " 'news',\n",
       " 'nice',\n",
       " 'night',\n",
       " 'nois',\n",
       " 'normal',\n",
       " 'noth',\n",
       " 'novel',\n",
       " 'offens',\n",
       " 'offici',\n",
       " 'oh',\n",
       " 'ok',\n",
       " 'okay',\n",
       " 'old',\n",
       " 'omg',\n",
       " 'onc',\n",
       " 'oni',\n",
       " 'onli',\n",
       " 'onlin',\n",
       " 'ootp',\n",
       " 'open',\n",
       " 'opinion',\n",
       " 'optimu',\n",
       " 'oreo',\n",
       " 'orig',\n",
       " 'oscar',\n",
       " 'ossana',\n",
       " 'otp',\n",
       " 'otter',\n",
       " 'outnumb',\n",
       " 'outshin',\n",
       " 'outsid',\n",
       " 'outta',\n",
       " 'overal',\n",
       " 'overcom',\n",
       " 'overexager',\n",
       " 'overlook',\n",
       " 'oversimplifi',\n",
       " 'overslept',\n",
       " 'pack',\n",
       " 'packag',\n",
       " 'page',\n",
       " 'pale',\n",
       " 'pant',\n",
       " 'panti',\n",
       " 'paper',\n",
       " 'parent',\n",
       " 'park',\n",
       " 'parti',\n",
       " 'partyin',\n",
       " 'passion',\n",
       " 'past',\n",
       " 'patirot',\n",
       " 'paul',\n",
       " 'pc',\n",
       " 'pegg',\n",
       " 'peopl',\n",
       " 'perfect',\n",
       " 'perform',\n",
       " 'perhap',\n",
       " 'period',\n",
       " 'person',\n",
       " 'personali',\n",
       " 'phase',\n",
       " 'phenomenon',\n",
       " 'phillip',\n",
       " 'philosoph',\n",
       " 'phoenix',\n",
       " 'phone',\n",
       " 'photographi',\n",
       " 'phrase',\n",
       " 'pic',\n",
       " 'picard',\n",
       " 'picki',\n",
       " 'picnic',\n",
       " 'pictur',\n",
       " 'picturesqu',\n",
       " 'piec',\n",
       " 'pink',\n",
       " 'pirat',\n",
       " 'place',\n",
       " 'plain',\n",
       " 'plan',\n",
       " 'plastic',\n",
       " 'plausibl',\n",
       " 'play',\n",
       " 'pleas',\n",
       " 'plot',\n",
       " 'plu',\n",
       " 'pocket',\n",
       " 'poem',\n",
       " 'point',\n",
       " 'polic',\n",
       " 'polit',\n",
       " 'poorli',\n",
       " 'pop',\n",
       " 'popular',\n",
       " 'portug',\n",
       " 'portugues',\n",
       " 'poseidon',\n",
       " 'posit',\n",
       " 'possibl',\n",
       " 'possum',\n",
       " 'post',\n",
       " 'postpon',\n",
       " 'potterhol',\n",
       " 'power',\n",
       " 'preciou',\n",
       " 'predict',\n",
       " 'present',\n",
       " 'press',\n",
       " 'pretend',\n",
       " 'pretti',\n",
       " 'preview',\n",
       " 'primari',\n",
       " 'prime',\n",
       " 'princ',\n",
       " 'prison',\n",
       " 'prize',\n",
       " 'probabl',\n",
       " 'problem',\n",
       " 'professor',\n",
       " 'profound',\n",
       " 'project',\n",
       " 'protest',\n",
       " 'proud',\n",
       " 'ps',\n",
       " 'psycholog',\n",
       " 'public',\n",
       " 'publicli',\n",
       " 'pud',\n",
       " 'pull',\n",
       " 'pup',\n",
       " 'purchas',\n",
       " 'quaintli',\n",
       " 'queen',\n",
       " 'queer',\n",
       " 'question',\n",
       " 'quick',\n",
       " 'quip',\n",
       " 'quirki',\n",
       " 'quit',\n",
       " 'quiz',\n",
       " 'quizz',\n",
       " 'racism',\n",
       " 'ran',\n",
       " 'rant',\n",
       " 'react',\n",
       " 'reaction',\n",
       " 'read',\n",
       " 'reader',\n",
       " 'real',\n",
       " 'realiti',\n",
       " 'realiz',\n",
       " 'realli',\n",
       " 'reason',\n",
       " 'receiv',\n",
       " 'recent',\n",
       " 'record',\n",
       " 'refer',\n",
       " 'refus',\n",
       " 'regardless',\n",
       " 'rehears',\n",
       " 'relat',\n",
       " 'relax',\n",
       " 'releas',\n",
       " 'relic',\n",
       " 'religi',\n",
       " 'religion',\n",
       " 'remind',\n",
       " 'remix',\n",
       " 'rent',\n",
       " 'reopen',\n",
       " 'repli',\n",
       " 'request',\n",
       " 'respect',\n",
       " 'rest',\n",
       " 'retart',\n",
       " 'review',\n",
       " 'ride',\n",
       " 'right',\n",
       " 'ring',\n",
       " 'rob',\n",
       " 'rock',\n",
       " 'ron',\n",
       " 'row',\n",
       " 'rowl',\n",
       " 'royal',\n",
       " 'rp',\n",
       " 'ruin',\n",
       " 'rule',\n",
       " 'run',\n",
       " 'runaway',\n",
       " 'runner',\n",
       " 'russotti',\n",
       " 'rv',\n",
       " 'sad',\n",
       " 'said',\n",
       " 'sake',\n",
       " 'sale',\n",
       " 'sam',\n",
       " 'sarcast',\n",
       " 'saturday',\n",
       " 'save',\n",
       " 'saw',\n",
       " 'sawyer',\n",
       " 'say',\n",
       " 'scar',\n",
       " 'scare',\n",
       " 'scarf',\n",
       " 'scenario',\n",
       " 'scene',\n",
       " 'sceneri',\n",
       " 'scent',\n",
       " 'school',\n",
       " 'scientolog',\n",
       " 'scientologist',\n",
       " 'scifi',\n",
       " 'score',\n",
       " 'screen',\n",
       " 'screenplay',\n",
       " 'screw',\n",
       " 'sean',\n",
       " 'search',\n",
       " 'second',\n",
       " 'secret',\n",
       " 'section',\n",
       " 'seek',\n",
       " 'seen',\n",
       " 'selfish',\n",
       " 'semest',\n",
       " 'sens',\n",
       " 'sent',\n",
       " 'sentri',\n",
       " 'sequel',\n",
       " 'seri',\n",
       " 'seriou',\n",
       " 'set',\n",
       " 'settin',\n",
       " 'sexi',\n",
       " 'sexual',\n",
       " 'seymor',\n",
       " 'seymour',\n",
       " 'shade',\n",
       " 'shadeslay',\n",
       " 'shame',\n",
       " 'shape',\n",
       " 'share',\n",
       " 'shatter',\n",
       " 'shell',\n",
       " 'shield',\n",
       " 'ship',\n",
       " 'shipmat',\n",
       " 'shit',\n",
       " 'shitti',\n",
       " 'shittiest',\n",
       " 'shoe',\n",
       " 'shop',\n",
       " 'short',\n",
       " 'shout',\n",
       " 'showcas',\n",
       " 'shraddha',\n",
       " 'shut',\n",
       " 'si',\n",
       " 'sick',\n",
       " 'sign',\n",
       " 'silent',\n",
       " 'silli',\n",
       " 'silver',\n",
       " 'simmon',\n",
       " 'simon',\n",
       " 'simpl',\n",
       " 'simpli',\n",
       " 'sinc',\n",
       " 'sing',\n",
       " 'sister',\n",
       " 'sit',\n",
       " 'sivullinen',\n",
       " 'sixth',\n",
       " 'skin',\n",
       " 'sky',\n",
       " 'slap',\n",
       " 'slash',\n",
       " 'slow',\n",
       " 'small',\n",
       " 'smoke',\n",
       " 'snow',\n",
       " 'snuck',\n",
       " 'sold',\n",
       " 'someon',\n",
       " 'someth',\n",
       " 'sometim',\n",
       " 'song',\n",
       " 'soo',\n",
       " 'soon',\n",
       " 'sooo',\n",
       " 'soooo',\n",
       " 'soooooo',\n",
       " 'soooooooo',\n",
       " 'sorcer',\n",
       " 'sorri',\n",
       " 'sort',\n",
       " 'soul',\n",
       " 'sound',\n",
       " 'soundtrack',\n",
       " 'south',\n",
       " 'spanish',\n",
       " 'speak',\n",
       " 'speaker',\n",
       " 'spec',\n",
       " 'special',\n",
       " 'specif',\n",
       " 'spectacularli',\n",
       " 'spell',\n",
       " 'spend',\n",
       " 'spi',\n",
       " 'spin',\n",
       " 'spine',\n",
       " 'spite',\n",
       " 'spoke',\n",
       " 'spontan',\n",
       " 'sport',\n",
       " 'springer',\n",
       " 'stand',\n",
       " 'standpoint',\n",
       " 'star',\n",
       " 'start',\n",
       " 'state',\n",
       " 'station',\n",
       " 'stay',\n",
       " 'stink',\n",
       " 'stinkin',\n",
       " 'stitch',\n",
       " 'stite',\n",
       " 'stone',\n",
       " 'stop',\n",
       " 'stori',\n",
       " 'storytim',\n",
       " 'straight',\n",
       " 'strang',\n",
       " 'street',\n",
       " 'strip',\n",
       " 'struggl',\n",
       " 'student',\n",
       " 'studi',\n",
       " 'stuff',\n",
       " 'stupid',\n",
       " 'stupidest',\n",
       " 'style',\n",
       " 'su',\n",
       " 'subject',\n",
       " 'subtitl',\n",
       " 'success',\n",
       " 'suck',\n",
       " 'sucki',\n",
       " 'sue',\n",
       " 'suicid',\n",
       " 'sum',\n",
       " 'summer',\n",
       " 'super',\n",
       " 'support',\n",
       " 'suppos',\n",
       " 'sure',\n",
       " 'tabl',\n",
       " 'tale',\n",
       " 'talk',\n",
       " 'tautou',\n",
       " 'tc',\n",
       " 'teach',\n",
       " 'tell',\n",
       " 'terribl',\n",
       " 'thank',\n",
       " 'theater',\n",
       " 'theme',\n",
       " 'thesi',\n",
       " 'thi',\n",
       " 'thing',\n",
       " 'think',\n",
       " 'thirdli',\n",
       " 'tho',\n",
       " 'thought',\n",
       " 'thousand',\n",
       " 'threw',\n",
       " 'thriller',\n",
       " 'throat',\n",
       " 'throw',\n",
       " 'thursday',\n",
       " 'ti',\n",
       " 'ticket',\n",
       " 'tie',\n",
       " 'tiffani',\n",
       " 'till',\n",
       " 'time',\n",
       " 'tini',\n",
       " 'tire',\n",
       " 'titan',\n",
       " 'titu',\n",
       " 'today',\n",
       " 'togeth',\n",
       " 'told',\n",
       " 'tom',\n",
       " 'tome',\n",
       " 'tomkat',\n",
       " 'tomorrow',\n",
       " 'ton',\n",
       " 'tonight',\n",
       " 'took',\n",
       " 'total',\n",
       " 'track',\n",
       " 'tragic',\n",
       " 'trailer',\n",
       " 'transamerica',\n",
       " 'travel',\n",
       " 'trece',\n",
       " 'tree',\n",
       " 'tri',\n",
       " 'trip',\n",
       " 'trivia',\n",
       " 'trouser',\n",
       " 'true',\n",
       " 'truli',\n",
       " 'truth',\n",
       " 'tun',\n",
       " 'turn',\n",
       " 'turner',\n",
       " 'tv',\n",
       " 'twice',\n",
       " 'twilight',\n",
       " 'twist',\n",
       " 'tye',\n",
       " 'type',\n",
       " 'uh',\n",
       " 'ultim',\n",
       " 'ultimatli',\n",
       " 'um',\n",
       " 'unabl',\n",
       " 'unauthor',\n",
       " 'unbeliev',\n",
       " 'undercov',\n",
       " 'understand',\n",
       " 'undoubtedli',\n",
       " 'unexpect',\n",
       " 'unfortun',\n",
       " 'unless',\n",
       " 'unpredict',\n",
       " 'updat',\n",
       " 'ur',\n",
       " 'url',\n",
       " 'usag',\n",
       " 'use',\n",
       " 'useless',\n",
       " 'usual',\n",
       " 'vacat',\n",
       " 'val',\n",
       " 'vampir',\n",
       " 'van',\n",
       " 'variou',\n",
       " 'vault',\n",
       " 've',\n",
       " 'veil',\n",
       " 'veri',\n",
       " 'versa',\n",
       " 'version',\n",
       " 'vic',\n",
       " 'vice',\n",
       " 'view',\n",
       " 'vigor',\n",
       " 'villain',\n",
       " 'vintag',\n",
       " 'virgin',\n",
       " 'visual',\n",
       " 'vito',\n",
       " 'vote',\n",
       " 'vs',\n",
       " 'wa',\n",
       " 'waaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',\n",
       " 'wack',\n",
       " 'wait',\n",
       " 'wal',\n",
       " 'walk',\n",
       " 'wanna',\n",
       " 'want',\n",
       " 'war',\n",
       " 'warn',\n",
       " 'wasn',\n",
       " 'watch',\n",
       " 'watson',\n",
       " 'way',\n",
       " 'weeeellllllll',\n",
       " 'week',\n",
       " 'weekend',\n",
       " 'weiner',\n",
       " 'weird',\n",
       " 'went',\n",
       " 'wept',\n",
       " 'wesley',\n",
       " 'west',\n",
       " 'whatev',\n",
       " 'whenev',\n",
       " 'wherea',\n",
       " 'wherev',\n",
       " 'whi',\n",
       " 'whimper',\n",
       " 'whini',\n",
       " 'whistl',\n",
       " 'white',\n",
       " 'wholesom',\n",
       " 'wicca',\n",
       " 'wiccan',\n",
       " 'wick',\n",
       " 'wide',\n",
       " 'wif',\n",
       " 'win',\n",
       " 'winter',\n",
       " 'wish',\n",
       " 'witchcraft',\n",
       " 'witha',\n",
       " 'won',\n",
       " 'wonder',\n",
       " 'woo',\n",
       " 'word',\n",
       " 'work',\n",
       " 'world',\n",
       " 'wors',\n",
       " 'worst',\n",
       " 'worth',\n",
       " 'worthless',\n",
       " 'wotshisfac',\n",
       " 'wow',\n",
       " 'wrangler',\n",
       " 'write',\n",
       " 'writer',\n",
       " 'written',\n",
       " 'wrong',\n",
       " 'wrote',\n",
       " 'wussi',\n",
       " 'x3',\n",
       " 'xd',\n",
       " 'ya',\n",
       " 'yahoo',\n",
       " 'ye',\n",
       " 'yea',\n",
       " 'yeah',\n",
       " 'year',\n",
       " 'yesterday',\n",
       " 'yip',\n",
       " 'young',\n",
       " 'younger',\n",
       " 'yuck',\n",
       " 'yuh',\n",
       " 'zach',\n",
       " 'zen']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>17</th>\n",
       "      <th>33</th>\n",
       "      <th>6th</th>\n",
       "      <th>abl</th>\n",
       "      <th>absolut</th>\n",
       "      <th>absurd</th>\n",
       "      <th>academi</th>\n",
       "      <th>accept</th>\n",
       "      <th>accompani</th>\n",
       "      <th>...</th>\n",
       "      <th>year</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yip</th>\n",
       "      <th>young</th>\n",
       "      <th>younger</th>\n",
       "      <th>yuck</th>\n",
       "      <th>yuh</th>\n",
       "      <th>zach</th>\n",
       "      <th>zen</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6913</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6914</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6915</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6916</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6917</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6918 rows × 1001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       10   17   33  6th  abl  absolut  absurd  academi  accept  accompani  \\\n",
       "0     0.0  0.0  0.0  0.0  0.0      0.0     0.0      0.0     0.0        0.0   \n",
       "1     0.0  0.0  0.0  0.0  0.0      0.0     0.0      0.0     0.0        0.0   \n",
       "2     0.0  0.0  0.0  0.0  0.0      0.0     0.0      0.0     0.0        0.0   \n",
       "3     0.0  0.0  0.0  0.0  0.0      0.0     0.0      0.0     0.0        0.0   \n",
       "4     0.0  0.0  0.0  0.0  0.0      0.0     0.0      0.0     0.0        0.0   \n",
       "...   ...  ...  ...  ...  ...      ...     ...      ...     ...        ...   \n",
       "6913  0.0  0.0  0.0  0.0  0.0      0.0     0.0      0.0     0.0        0.0   \n",
       "6914  0.0  0.0  0.0  0.0  0.0      0.0     0.0      0.0     0.0        0.0   \n",
       "6915  0.0  0.0  0.0  0.0  0.0      0.0     0.0      0.0     0.0        0.0   \n",
       "6916  0.0  0.0  0.0  0.0  0.0      0.0     0.0      0.0     0.0        0.0   \n",
       "6917  0.0  0.0  0.0  0.0  0.0      0.0     0.0      0.0     0.0        0.0   \n",
       "\n",
       "      ...  year  yesterday  yip  young  younger  yuck  yuh  zach  zen  \\\n",
       "0     ...   0.0        0.0  0.0    0.0      0.0   0.0  0.0   0.0  0.0   \n",
       "1     ...   0.0        0.0  0.0    0.0      0.0   0.0  0.0   0.0  0.0   \n",
       "2     ...   0.0        0.0  0.0    0.0      0.0   0.0  0.0   0.0  0.0   \n",
       "3     ...   0.0        0.0  0.0    0.0      0.0   0.0  0.0   0.0  0.0   \n",
       "4     ...   0.0        0.0  0.0    0.0      0.0   0.0  0.0   0.0  0.0   \n",
       "...   ...   ...        ...  ...    ...      ...   ...  ...   ...  ...   \n",
       "6913  ...   0.0        0.0  0.0    0.0      0.0   0.0  0.0   0.0  0.0   \n",
       "6914  ...   0.0        0.0  0.0    0.0      0.0   0.0  0.0   0.0  0.0   \n",
       "6915  ...   0.0        0.0  0.0    0.0      0.0   0.0  0.0   0.0  0.0   \n",
       "6916  ...   0.0        0.0  0.0    0.0      0.0   0.0  0.0   0.0  0.0   \n",
       "6917  ...   0.0        0.0  0.0    0.0      0.0   0.0  0.0   0.0  0.0   \n",
       "\n",
       "      sentiment  \n",
       "0             1  \n",
       "1             1  \n",
       "2             1  \n",
       "3             1  \n",
       "4             1  \n",
       "...         ...  \n",
       "6913          0  \n",
       "6914          0  \n",
       "6915          0  \n",
       "6916          0  \n",
       "6917          0  \n",
       "\n",
       "[6918 rows x 1001 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the document vector matrix into dataframe\n",
    "train_ds_df = pd.DataFrame(train_ds_features.todense())\n",
    "# Assign the features names to the column\n",
    "train_ds_df.columns = features\n",
    "# Assign the sentiment labels to the train_ds\n",
    "train_ds_df['sentiment'] = train_ds.sentiment\n",
    "train_ds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_X, test_X, train_y, test_y = train_test_split( train_ds_features,train_ds.sentiment,test_size = 0.3,random_state = 42 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Model for Sentiment Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "nb_clf = BernoulliNB()\n",
    "nb_clf.fit( train_X.toarray(), train_y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds_predicted = nb_clf.predict( test_X.toarray() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.98       873\n",
      "           1       0.98      0.99      0.98      1203\n",
      "\n",
      "    accuracy                           0.98      2076\n",
      "   macro avg       0.98      0.98      0.98      2076\n",
      "weighted avg       0.98      0.98      0.98      2076\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print( metrics.classification_report( test_y, test_ds_predicted ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Process</th>\n",
       "      <th>Model Name</th>\n",
       "      <th>F1 Scores</th>\n",
       "      <th>Range of F1 Scores</th>\n",
       "      <th>Std Deviation of F1 Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Binomial Naive Bayes Classifier</td>\n",
       "      <td>[0.92, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.032094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>[0.95, 0.99, 0.99, 1.0, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.020736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Decission Tree</td>\n",
       "      <td>[0.92, 0.98, 0.98, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.031305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>[0.93, 0.97, 0.98, 0.99, 1.0]</td>\n",
       "      <td>0.93-1.0</td>\n",
       "      <td>0.027019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>XG Boost</td>\n",
       "      <td>[0.95, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.019235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Stochastic Gradient Descent</td>\n",
       "      <td>[0.95, 0.99, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.019494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Gausian Process</td>\n",
       "      <td>[0.94, 0.99, 0.99, 1.0, 1.0]</td>\n",
       "      <td>0.94-1.0</td>\n",
       "      <td>0.025100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>K Nearst Neighbour</td>\n",
       "      <td>[0.92, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.032094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>[0.98, 0.99, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.98-1.0</td>\n",
       "      <td>0.007071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>[0.95, 0.97, 0.98, 0.99, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.019235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Binomial Naive Bayes Classifier</td>\n",
       "      <td>[0.92, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.032094</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Process                       Model Name  \\\n",
       "0  Bag Of Words with NLTK Stemming  Binomial Naive Bayes Classifier   \n",
       "1  Bag Of Words with NLTK Stemming              Logistic Regression   \n",
       "2  Bag Of Words with NLTK Stemming                   Decission Tree   \n",
       "3  Bag Of Words with NLTK Stemming                    Random Forest   \n",
       "4  Bag Of Words with NLTK Stemming                         XG Boost   \n",
       "5  Bag Of Words with NLTK Stemming      Stochastic Gradient Descent   \n",
       "6  Bag Of Words with NLTK Stemming                  Gausian Process   \n",
       "7  Bag Of Words with NLTK Stemming               K Nearst Neighbour   \n",
       "8  Bag Of Words with NLTK Stemming     Linear Discriminant Analysis   \n",
       "9  Bag Of Words with NLTK Stemming           Support Vector Machine   \n",
       "0         TFIDF with NLTK Stemming  Binomial Naive Bayes Classifier   \n",
       "\n",
       "                       F1 Scores Range of F1 Scores  \\\n",
       "0  [0.92, 0.98, 0.99, 0.99, 1.0]           0.92-1.0   \n",
       "1   [0.95, 0.99, 0.99, 1.0, 1.0]           0.95-1.0   \n",
       "2  [0.92, 0.98, 0.98, 0.99, 1.0]           0.92-1.0   \n",
       "3  [0.93, 0.97, 0.98, 0.99, 1.0]           0.93-1.0   \n",
       "4  [0.95, 0.98, 0.99, 0.99, 1.0]           0.95-1.0   \n",
       "5  [0.95, 0.99, 0.99, 0.99, 1.0]           0.95-1.0   \n",
       "6   [0.94, 0.99, 0.99, 1.0, 1.0]           0.94-1.0   \n",
       "7  [0.92, 0.97, 0.99, 0.99, 1.0]           0.92-1.0   \n",
       "8  [0.98, 0.99, 0.99, 0.99, 1.0]           0.98-1.0   \n",
       "9  [0.95, 0.97, 0.98, 0.99, 1.0]           0.95-1.0   \n",
       "0  [0.92, 0.98, 0.99, 0.99, 1.0]           0.92-1.0   \n",
       "\n",
       "   Std Deviation of F1 Scores  \n",
       "0                    0.032094  \n",
       "1                    0.020736  \n",
       "2                    0.031305  \n",
       "3                    0.027019  \n",
       "4                    0.019235  \n",
       "5                    0.019494  \n",
       "6                    0.025100  \n",
       "7                    0.032094  \n",
       "8                    0.007071  \n",
       "9                    0.019235  \n",
       "0                    0.032094  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_obj = nb_clf\n",
    "model_name = \"Binomial Naive Bayes Classifier\"\n",
    "process = \"TFIDF with NLTK Stemming\"\n",
    "n_splits = 5\n",
    "X = train_ds_features.toarray()\n",
    "y = train_ds.sentiment\n",
    "stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n",
    "df_model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(train_X.toarray(), train_y)\n",
    "test_ds_predicted = logreg.predict( test_X.toarray() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98       873\n",
      "           1       0.98      0.99      0.99      1203\n",
      "\n",
      "    accuracy                           0.99      2076\n",
      "   macro avg       0.99      0.99      0.99      2076\n",
      "weighted avg       0.99      0.99      0.99      2076\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print( metrics.classification_report( test_y, test_ds_predicted ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Process</th>\n",
       "      <th>Model Name</th>\n",
       "      <th>F1 Scores</th>\n",
       "      <th>Range of F1 Scores</th>\n",
       "      <th>Std Deviation of F1 Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Binomial Naive Bayes Classifier</td>\n",
       "      <td>[0.92, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.032094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>[0.95, 0.99, 0.99, 1.0, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.020736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Decission Tree</td>\n",
       "      <td>[0.92, 0.98, 0.98, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.031305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>[0.93, 0.97, 0.98, 0.99, 1.0]</td>\n",
       "      <td>0.93-1.0</td>\n",
       "      <td>0.027019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>XG Boost</td>\n",
       "      <td>[0.95, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.019235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Stochastic Gradient Descent</td>\n",
       "      <td>[0.95, 0.99, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.019494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Gausian Process</td>\n",
       "      <td>[0.94, 0.99, 0.99, 1.0, 1.0]</td>\n",
       "      <td>0.94-1.0</td>\n",
       "      <td>0.025100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>K Nearst Neighbour</td>\n",
       "      <td>[0.92, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.032094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>[0.98, 0.99, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.98-1.0</td>\n",
       "      <td>0.007071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>[0.95, 0.97, 0.98, 0.99, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.019235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Binomial Naive Bayes Classifier</td>\n",
       "      <td>[0.92, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.032094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>[0.95, 0.97, 1.0, 1.0, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.023022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Process                       Model Name  \\\n",
       "0  Bag Of Words with NLTK Stemming  Binomial Naive Bayes Classifier   \n",
       "1  Bag Of Words with NLTK Stemming              Logistic Regression   \n",
       "2  Bag Of Words with NLTK Stemming                   Decission Tree   \n",
       "3  Bag Of Words with NLTK Stemming                    Random Forest   \n",
       "4  Bag Of Words with NLTK Stemming                         XG Boost   \n",
       "5  Bag Of Words with NLTK Stemming      Stochastic Gradient Descent   \n",
       "6  Bag Of Words with NLTK Stemming                  Gausian Process   \n",
       "7  Bag Of Words with NLTK Stemming               K Nearst Neighbour   \n",
       "8  Bag Of Words with NLTK Stemming     Linear Discriminant Analysis   \n",
       "9  Bag Of Words with NLTK Stemming           Support Vector Machine   \n",
       "0         TFIDF with NLTK Stemming  Binomial Naive Bayes Classifier   \n",
       "0         TFIDF with NLTK Stemming              Logistic Regression   \n",
       "\n",
       "                       F1 Scores Range of F1 Scores  \\\n",
       "0  [0.92, 0.98, 0.99, 0.99, 1.0]           0.92-1.0   \n",
       "1   [0.95, 0.99, 0.99, 1.0, 1.0]           0.95-1.0   \n",
       "2  [0.92, 0.98, 0.98, 0.99, 1.0]           0.92-1.0   \n",
       "3  [0.93, 0.97, 0.98, 0.99, 1.0]           0.93-1.0   \n",
       "4  [0.95, 0.98, 0.99, 0.99, 1.0]           0.95-1.0   \n",
       "5  [0.95, 0.99, 0.99, 0.99, 1.0]           0.95-1.0   \n",
       "6   [0.94, 0.99, 0.99, 1.0, 1.0]           0.94-1.0   \n",
       "7  [0.92, 0.97, 0.99, 0.99, 1.0]           0.92-1.0   \n",
       "8  [0.98, 0.99, 0.99, 0.99, 1.0]           0.98-1.0   \n",
       "9  [0.95, 0.97, 0.98, 0.99, 1.0]           0.95-1.0   \n",
       "0  [0.92, 0.98, 0.99, 0.99, 1.0]           0.92-1.0   \n",
       "0    [0.95, 0.97, 1.0, 1.0, 1.0]           0.95-1.0   \n",
       "\n",
       "   Std Deviation of F1 Scores  \n",
       "0                    0.032094  \n",
       "1                    0.020736  \n",
       "2                    0.031305  \n",
       "3                    0.027019  \n",
       "4                    0.019235  \n",
       "5                    0.019494  \n",
       "6                    0.025100  \n",
       "7                    0.032094  \n",
       "8                    0.007071  \n",
       "9                    0.019235  \n",
       "0                    0.032094  \n",
       "0                    0.023022  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_obj = logreg\n",
    "model_name = \"Logistic Regression\"\n",
    "process = \"TFIDF with NLTK Stemming\"\n",
    "n_splits = 5\n",
    "X = train_ds_features.toarray()\n",
    "y = train_ds.sentiment\n",
    "stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n",
    "df_model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "decision_tree = DecisionTreeClassifier(criterion='entropy')\n",
    "\n",
    "decision_tree.fit(train_X.toarray(), train_y)\n",
    "test_ds_predicted = decision_tree.predict( test_X.toarray() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       873\n",
      "           1       0.99      0.99      0.99      1203\n",
      "\n",
      "    accuracy                           0.99      2076\n",
      "   macro avg       0.99      0.99      0.99      2076\n",
      "weighted avg       0.99      0.99      0.99      2076\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print( metrics.classification_report( test_y, test_ds_predicted ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Process</th>\n",
       "      <th>Model Name</th>\n",
       "      <th>F1 Scores</th>\n",
       "      <th>Range of F1 Scores</th>\n",
       "      <th>Std Deviation of F1 Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Binomial Naive Bayes Classifier</td>\n",
       "      <td>[0.92, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.032094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>[0.95, 0.99, 0.99, 1.0, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.020736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Decission Tree</td>\n",
       "      <td>[0.92, 0.98, 0.98, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.031305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>[0.93, 0.97, 0.98, 0.99, 1.0]</td>\n",
       "      <td>0.93-1.0</td>\n",
       "      <td>0.027019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>XG Boost</td>\n",
       "      <td>[0.95, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.019235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Stochastic Gradient Descent</td>\n",
       "      <td>[0.95, 0.99, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.019494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Gausian Process</td>\n",
       "      <td>[0.94, 0.99, 0.99, 1.0, 1.0]</td>\n",
       "      <td>0.94-1.0</td>\n",
       "      <td>0.025100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>K Nearst Neighbour</td>\n",
       "      <td>[0.92, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.032094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>[0.98, 0.99, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.98-1.0</td>\n",
       "      <td>0.007071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>[0.95, 0.97, 0.98, 0.99, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.019235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Binomial Naive Bayes Classifier</td>\n",
       "      <td>[0.92, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.032094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>[0.95, 0.97, 1.0, 1.0, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.023022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Decission Tree</td>\n",
       "      <td>[0.93, 0.95, 0.98, 0.98, 1.0]</td>\n",
       "      <td>0.93-1.0</td>\n",
       "      <td>0.027749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Process                       Model Name  \\\n",
       "0  Bag Of Words with NLTK Stemming  Binomial Naive Bayes Classifier   \n",
       "1  Bag Of Words with NLTK Stemming              Logistic Regression   \n",
       "2  Bag Of Words with NLTK Stemming                   Decission Tree   \n",
       "3  Bag Of Words with NLTK Stemming                    Random Forest   \n",
       "4  Bag Of Words with NLTK Stemming                         XG Boost   \n",
       "5  Bag Of Words with NLTK Stemming      Stochastic Gradient Descent   \n",
       "6  Bag Of Words with NLTK Stemming                  Gausian Process   \n",
       "7  Bag Of Words with NLTK Stemming               K Nearst Neighbour   \n",
       "8  Bag Of Words with NLTK Stemming     Linear Discriminant Analysis   \n",
       "9  Bag Of Words with NLTK Stemming           Support Vector Machine   \n",
       "0         TFIDF with NLTK Stemming  Binomial Naive Bayes Classifier   \n",
       "0         TFIDF with NLTK Stemming              Logistic Regression   \n",
       "0         TFIDF with NLTK Stemming                   Decission Tree   \n",
       "\n",
       "                       F1 Scores Range of F1 Scores  \\\n",
       "0  [0.92, 0.98, 0.99, 0.99, 1.0]           0.92-1.0   \n",
       "1   [0.95, 0.99, 0.99, 1.0, 1.0]           0.95-1.0   \n",
       "2  [0.92, 0.98, 0.98, 0.99, 1.0]           0.92-1.0   \n",
       "3  [0.93, 0.97, 0.98, 0.99, 1.0]           0.93-1.0   \n",
       "4  [0.95, 0.98, 0.99, 0.99, 1.0]           0.95-1.0   \n",
       "5  [0.95, 0.99, 0.99, 0.99, 1.0]           0.95-1.0   \n",
       "6   [0.94, 0.99, 0.99, 1.0, 1.0]           0.94-1.0   \n",
       "7  [0.92, 0.97, 0.99, 0.99, 1.0]           0.92-1.0   \n",
       "8  [0.98, 0.99, 0.99, 0.99, 1.0]           0.98-1.0   \n",
       "9  [0.95, 0.97, 0.98, 0.99, 1.0]           0.95-1.0   \n",
       "0  [0.92, 0.98, 0.99, 0.99, 1.0]           0.92-1.0   \n",
       "0    [0.95, 0.97, 1.0, 1.0, 1.0]           0.95-1.0   \n",
       "0  [0.93, 0.95, 0.98, 0.98, 1.0]           0.93-1.0   \n",
       "\n",
       "   Std Deviation of F1 Scores  \n",
       "0                    0.032094  \n",
       "1                    0.020736  \n",
       "2                    0.031305  \n",
       "3                    0.027019  \n",
       "4                    0.019235  \n",
       "5                    0.019494  \n",
       "6                    0.025100  \n",
       "7                    0.032094  \n",
       "8                    0.007071  \n",
       "9                    0.019235  \n",
       "0                    0.032094  \n",
       "0                    0.023022  \n",
       "0                    0.027749  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_obj = decision_tree\n",
    "model_name = \"Decission Tree\"\n",
    "process = \"TFIDF with NLTK Stemming\"\n",
    "n_splits = 5\n",
    "X = train_ds_features.toarray()\n",
    "y = train_ds.sentiment\n",
    "stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n",
    "df_model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "random_forest = RandomForestClassifier(n_estimators=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest.fit(train_X.toarray(), train_y)\n",
    "test_ds_predicted = random_forest.predict( test_X.toarray() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       873\n",
      "           1       0.99      0.99      0.99      1203\n",
      "\n",
      "    accuracy                           0.99      2076\n",
      "   macro avg       0.99      0.99      0.99      2076\n",
      "weighted avg       0.99      0.99      0.99      2076\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print( metrics.classification_report( test_y, test_ds_predicted ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Process</th>\n",
       "      <th>Model Name</th>\n",
       "      <th>F1 Scores</th>\n",
       "      <th>Range of F1 Scores</th>\n",
       "      <th>Std Deviation of F1 Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Binomial Naive Bayes Classifier</td>\n",
       "      <td>[0.92, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.032094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>[0.95, 0.99, 0.99, 1.0, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.020736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Decission Tree</td>\n",
       "      <td>[0.92, 0.98, 0.98, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.031305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>[0.93, 0.97, 0.98, 0.99, 1.0]</td>\n",
       "      <td>0.93-1.0</td>\n",
       "      <td>0.027019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>XG Boost</td>\n",
       "      <td>[0.95, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.019235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Stochastic Gradient Descent</td>\n",
       "      <td>[0.95, 0.99, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.019494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Gausian Process</td>\n",
       "      <td>[0.94, 0.99, 0.99, 1.0, 1.0]</td>\n",
       "      <td>0.94-1.0</td>\n",
       "      <td>0.025100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>K Nearst Neighbour</td>\n",
       "      <td>[0.92, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.032094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>[0.98, 0.99, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.98-1.0</td>\n",
       "      <td>0.007071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>[0.95, 0.97, 0.98, 0.99, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.019235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Binomial Naive Bayes Classifier</td>\n",
       "      <td>[0.92, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.032094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>[0.95, 0.97, 1.0, 1.0, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.023022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Decission Tree</td>\n",
       "      <td>[0.93, 0.95, 0.98, 0.98, 1.0]</td>\n",
       "      <td>0.93-1.0</td>\n",
       "      <td>0.027749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>[0.94, 0.95, 0.97, 0.98, 0.99]</td>\n",
       "      <td>0.94-0.99</td>\n",
       "      <td>0.020736</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Process                       Model Name  \\\n",
       "0  Bag Of Words with NLTK Stemming  Binomial Naive Bayes Classifier   \n",
       "1  Bag Of Words with NLTK Stemming              Logistic Regression   \n",
       "2  Bag Of Words with NLTK Stemming                   Decission Tree   \n",
       "3  Bag Of Words with NLTK Stemming                    Random Forest   \n",
       "4  Bag Of Words with NLTK Stemming                         XG Boost   \n",
       "5  Bag Of Words with NLTK Stemming      Stochastic Gradient Descent   \n",
       "6  Bag Of Words with NLTK Stemming                  Gausian Process   \n",
       "7  Bag Of Words with NLTK Stemming               K Nearst Neighbour   \n",
       "8  Bag Of Words with NLTK Stemming     Linear Discriminant Analysis   \n",
       "9  Bag Of Words with NLTK Stemming           Support Vector Machine   \n",
       "0         TFIDF with NLTK Stemming  Binomial Naive Bayes Classifier   \n",
       "0         TFIDF with NLTK Stemming              Logistic Regression   \n",
       "0         TFIDF with NLTK Stemming                   Decission Tree   \n",
       "0         TFIDF with NLTK Stemming                    Random Forest   \n",
       "\n",
       "                        F1 Scores Range of F1 Scores  \\\n",
       "0   [0.92, 0.98, 0.99, 0.99, 1.0]           0.92-1.0   \n",
       "1    [0.95, 0.99, 0.99, 1.0, 1.0]           0.95-1.0   \n",
       "2   [0.92, 0.98, 0.98, 0.99, 1.0]           0.92-1.0   \n",
       "3   [0.93, 0.97, 0.98, 0.99, 1.0]           0.93-1.0   \n",
       "4   [0.95, 0.98, 0.99, 0.99, 1.0]           0.95-1.0   \n",
       "5   [0.95, 0.99, 0.99, 0.99, 1.0]           0.95-1.0   \n",
       "6    [0.94, 0.99, 0.99, 1.0, 1.0]           0.94-1.0   \n",
       "7   [0.92, 0.97, 0.99, 0.99, 1.0]           0.92-1.0   \n",
       "8   [0.98, 0.99, 0.99, 0.99, 1.0]           0.98-1.0   \n",
       "9   [0.95, 0.97, 0.98, 0.99, 1.0]           0.95-1.0   \n",
       "0   [0.92, 0.98, 0.99, 0.99, 1.0]           0.92-1.0   \n",
       "0     [0.95, 0.97, 1.0, 1.0, 1.0]           0.95-1.0   \n",
       "0   [0.93, 0.95, 0.98, 0.98, 1.0]           0.93-1.0   \n",
       "0  [0.94, 0.95, 0.97, 0.98, 0.99]          0.94-0.99   \n",
       "\n",
       "   Std Deviation of F1 Scores  \n",
       "0                    0.032094  \n",
       "1                    0.020736  \n",
       "2                    0.031305  \n",
       "3                    0.027019  \n",
       "4                    0.019235  \n",
       "5                    0.019494  \n",
       "6                    0.025100  \n",
       "7                    0.032094  \n",
       "8                    0.007071  \n",
       "9                    0.019235  \n",
       "0                    0.032094  \n",
       "0                    0.023022  \n",
       "0                    0.027749  \n",
       "0                    0.020736  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_obj = random_forest\n",
    "model_name = \"Random Forest\"\n",
    "process = \"TFIDF with NLTK Stemming\"\n",
    "n_splits = 5\n",
    "X = train_ds_features.toarray()\n",
    "y = train_ds.sentiment\n",
    "stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n",
    "df_model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgboost = XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost.fit(train_X.toarray(), train_y)\n",
    "test_ds_predicted = xgboost.predict( test_X.toarray() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       873\n",
      "           1       0.99      0.99      0.99      1203\n",
      "\n",
      "    accuracy                           0.99      2076\n",
      "   macro avg       0.99      0.99      0.99      2076\n",
      "weighted avg       0.99      0.99      0.99      2076\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print( metrics.classification_report( test_y, test_ds_predicted ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Process</th>\n",
       "      <th>Model Name</th>\n",
       "      <th>F1 Scores</th>\n",
       "      <th>Range of F1 Scores</th>\n",
       "      <th>Std Deviation of F1 Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Binomial Naive Bayes Classifier</td>\n",
       "      <td>[0.92, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.032094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>[0.95, 0.99, 0.99, 1.0, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.020736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Decission Tree</td>\n",
       "      <td>[0.92, 0.98, 0.98, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.031305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>[0.93, 0.97, 0.98, 0.99, 1.0]</td>\n",
       "      <td>0.93-1.0</td>\n",
       "      <td>0.027019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>XG Boost</td>\n",
       "      <td>[0.95, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.019235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Stochastic Gradient Descent</td>\n",
       "      <td>[0.95, 0.99, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.019494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Gausian Process</td>\n",
       "      <td>[0.94, 0.99, 0.99, 1.0, 1.0]</td>\n",
       "      <td>0.94-1.0</td>\n",
       "      <td>0.025100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>K Nearst Neighbour</td>\n",
       "      <td>[0.92, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.032094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>[0.98, 0.99, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.98-1.0</td>\n",
       "      <td>0.007071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>[0.95, 0.97, 0.98, 0.99, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.019235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Binomial Naive Bayes Classifier</td>\n",
       "      <td>[0.92, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.032094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>[0.95, 0.97, 1.0, 1.0, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.023022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Decission Tree</td>\n",
       "      <td>[0.93, 0.95, 0.98, 0.98, 1.0]</td>\n",
       "      <td>0.93-1.0</td>\n",
       "      <td>0.027749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>[0.94, 0.95, 0.97, 0.98, 0.99]</td>\n",
       "      <td>0.94-0.99</td>\n",
       "      <td>0.020736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>XG Boost</td>\n",
       "      <td>[0.96, 0.97, 0.97, 0.99, 1.0]</td>\n",
       "      <td>0.96-1.0</td>\n",
       "      <td>0.016432</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Process                       Model Name  \\\n",
       "0  Bag Of Words with NLTK Stemming  Binomial Naive Bayes Classifier   \n",
       "1  Bag Of Words with NLTK Stemming              Logistic Regression   \n",
       "2  Bag Of Words with NLTK Stemming                   Decission Tree   \n",
       "3  Bag Of Words with NLTK Stemming                    Random Forest   \n",
       "4  Bag Of Words with NLTK Stemming                         XG Boost   \n",
       "5  Bag Of Words with NLTK Stemming      Stochastic Gradient Descent   \n",
       "6  Bag Of Words with NLTK Stemming                  Gausian Process   \n",
       "7  Bag Of Words with NLTK Stemming               K Nearst Neighbour   \n",
       "8  Bag Of Words with NLTK Stemming     Linear Discriminant Analysis   \n",
       "9  Bag Of Words with NLTK Stemming           Support Vector Machine   \n",
       "0         TFIDF with NLTK Stemming  Binomial Naive Bayes Classifier   \n",
       "0         TFIDF with NLTK Stemming              Logistic Regression   \n",
       "0         TFIDF with NLTK Stemming                   Decission Tree   \n",
       "0         TFIDF with NLTK Stemming                    Random Forest   \n",
       "0         TFIDF with NLTK Stemming                         XG Boost   \n",
       "\n",
       "                        F1 Scores Range of F1 Scores  \\\n",
       "0   [0.92, 0.98, 0.99, 0.99, 1.0]           0.92-1.0   \n",
       "1    [0.95, 0.99, 0.99, 1.0, 1.0]           0.95-1.0   \n",
       "2   [0.92, 0.98, 0.98, 0.99, 1.0]           0.92-1.0   \n",
       "3   [0.93, 0.97, 0.98, 0.99, 1.0]           0.93-1.0   \n",
       "4   [0.95, 0.98, 0.99, 0.99, 1.0]           0.95-1.0   \n",
       "5   [0.95, 0.99, 0.99, 0.99, 1.0]           0.95-1.0   \n",
       "6    [0.94, 0.99, 0.99, 1.0, 1.0]           0.94-1.0   \n",
       "7   [0.92, 0.97, 0.99, 0.99, 1.0]           0.92-1.0   \n",
       "8   [0.98, 0.99, 0.99, 0.99, 1.0]           0.98-1.0   \n",
       "9   [0.95, 0.97, 0.98, 0.99, 1.0]           0.95-1.0   \n",
       "0   [0.92, 0.98, 0.99, 0.99, 1.0]           0.92-1.0   \n",
       "0     [0.95, 0.97, 1.0, 1.0, 1.0]           0.95-1.0   \n",
       "0   [0.93, 0.95, 0.98, 0.98, 1.0]           0.93-1.0   \n",
       "0  [0.94, 0.95, 0.97, 0.98, 0.99]          0.94-0.99   \n",
       "0   [0.96, 0.97, 0.97, 0.99, 1.0]           0.96-1.0   \n",
       "\n",
       "   Std Deviation of F1 Scores  \n",
       "0                    0.032094  \n",
       "1                    0.020736  \n",
       "2                    0.031305  \n",
       "3                    0.027019  \n",
       "4                    0.019235  \n",
       "5                    0.019494  \n",
       "6                    0.025100  \n",
       "7                    0.032094  \n",
       "8                    0.007071  \n",
       "9                    0.019235  \n",
       "0                    0.032094  \n",
       "0                    0.023022  \n",
       "0                    0.027749  \n",
       "0                    0.020736  \n",
       "0                    0.016432  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_obj = xgboost\n",
    "model_name = \"XG Boost\"\n",
    "process = \"TFIDF with NLTK Stemming\"\n",
    "n_splits = 5\n",
    "X = train_ds_features.toarray()\n",
    "y = train_ds.sentiment\n",
    "stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n",
    "df_model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "sgd = OneVsRestClassifier(SGDClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd.fit(train_X.toarray(), train_y)\n",
    "test_ds_predicted = sgd.predict( test_X.toarray() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99       873\n",
      "           1       0.99      1.00      0.99      1203\n",
      "\n",
      "    accuracy                           0.99      2076\n",
      "   macro avg       0.99      0.99      0.99      2076\n",
      "weighted avg       0.99      0.99      0.99      2076\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print( metrics.classification_report( test_y, test_ds_predicted ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Process</th>\n",
       "      <th>Model Name</th>\n",
       "      <th>F1 Scores</th>\n",
       "      <th>Range of F1 Scores</th>\n",
       "      <th>Std Deviation of F1 Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Binomial Naive Bayes Classifier</td>\n",
       "      <td>[0.92, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.032094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>[0.95, 0.99, 0.99, 1.0, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.020736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Decission Tree</td>\n",
       "      <td>[0.92, 0.98, 0.98, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.031305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>[0.93, 0.97, 0.98, 0.99, 1.0]</td>\n",
       "      <td>0.93-1.0</td>\n",
       "      <td>0.027019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>XG Boost</td>\n",
       "      <td>[0.95, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.019235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Stochastic Gradient Descent</td>\n",
       "      <td>[0.95, 0.99, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.019494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Gausian Process</td>\n",
       "      <td>[0.94, 0.99, 0.99, 1.0, 1.0]</td>\n",
       "      <td>0.94-1.0</td>\n",
       "      <td>0.025100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>K Nearst Neighbour</td>\n",
       "      <td>[0.92, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.032094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>[0.98, 0.99, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.98-1.0</td>\n",
       "      <td>0.007071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>[0.95, 0.97, 0.98, 0.99, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.019235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Binomial Naive Bayes Classifier</td>\n",
       "      <td>[0.92, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.032094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>[0.95, 0.97, 1.0, 1.0, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.023022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Decission Tree</td>\n",
       "      <td>[0.93, 0.95, 0.98, 0.98, 1.0]</td>\n",
       "      <td>0.93-1.0</td>\n",
       "      <td>0.027749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>[0.94, 0.95, 0.97, 0.98, 0.99]</td>\n",
       "      <td>0.94-0.99</td>\n",
       "      <td>0.020736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>XG Boost</td>\n",
       "      <td>[0.96, 0.97, 0.97, 0.99, 1.0]</td>\n",
       "      <td>0.96-1.0</td>\n",
       "      <td>0.016432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Stochastic Gradient Descent</td>\n",
       "      <td>[0.97, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.97-1.0</td>\n",
       "      <td>0.011402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Process                       Model Name  \\\n",
       "0  Bag Of Words with NLTK Stemming  Binomial Naive Bayes Classifier   \n",
       "1  Bag Of Words with NLTK Stemming              Logistic Regression   \n",
       "2  Bag Of Words with NLTK Stemming                   Decission Tree   \n",
       "3  Bag Of Words with NLTK Stemming                    Random Forest   \n",
       "4  Bag Of Words with NLTK Stemming                         XG Boost   \n",
       "5  Bag Of Words with NLTK Stemming      Stochastic Gradient Descent   \n",
       "6  Bag Of Words with NLTK Stemming                  Gausian Process   \n",
       "7  Bag Of Words with NLTK Stemming               K Nearst Neighbour   \n",
       "8  Bag Of Words with NLTK Stemming     Linear Discriminant Analysis   \n",
       "9  Bag Of Words with NLTK Stemming           Support Vector Machine   \n",
       "0         TFIDF with NLTK Stemming  Binomial Naive Bayes Classifier   \n",
       "0         TFIDF with NLTK Stemming              Logistic Regression   \n",
       "0         TFIDF with NLTK Stemming                   Decission Tree   \n",
       "0         TFIDF with NLTK Stemming                    Random Forest   \n",
       "0         TFIDF with NLTK Stemming                         XG Boost   \n",
       "0         TFIDF with NLTK Stemming      Stochastic Gradient Descent   \n",
       "\n",
       "                        F1 Scores Range of F1 Scores  \\\n",
       "0   [0.92, 0.98, 0.99, 0.99, 1.0]           0.92-1.0   \n",
       "1    [0.95, 0.99, 0.99, 1.0, 1.0]           0.95-1.0   \n",
       "2   [0.92, 0.98, 0.98, 0.99, 1.0]           0.92-1.0   \n",
       "3   [0.93, 0.97, 0.98, 0.99, 1.0]           0.93-1.0   \n",
       "4   [0.95, 0.98, 0.99, 0.99, 1.0]           0.95-1.0   \n",
       "5   [0.95, 0.99, 0.99, 0.99, 1.0]           0.95-1.0   \n",
       "6    [0.94, 0.99, 0.99, 1.0, 1.0]           0.94-1.0   \n",
       "7   [0.92, 0.97, 0.99, 0.99, 1.0]           0.92-1.0   \n",
       "8   [0.98, 0.99, 0.99, 0.99, 1.0]           0.98-1.0   \n",
       "9   [0.95, 0.97, 0.98, 0.99, 1.0]           0.95-1.0   \n",
       "0   [0.92, 0.98, 0.99, 0.99, 1.0]           0.92-1.0   \n",
       "0     [0.95, 0.97, 1.0, 1.0, 1.0]           0.95-1.0   \n",
       "0   [0.93, 0.95, 0.98, 0.98, 1.0]           0.93-1.0   \n",
       "0  [0.94, 0.95, 0.97, 0.98, 0.99]          0.94-0.99   \n",
       "0   [0.96, 0.97, 0.97, 0.99, 1.0]           0.96-1.0   \n",
       "0   [0.97, 0.98, 0.99, 0.99, 1.0]           0.97-1.0   \n",
       "\n",
       "   Std Deviation of F1 Scores  \n",
       "0                    0.032094  \n",
       "1                    0.020736  \n",
       "2                    0.031305  \n",
       "3                    0.027019  \n",
       "4                    0.019235  \n",
       "5                    0.019494  \n",
       "6                    0.025100  \n",
       "7                    0.032094  \n",
       "8                    0.007071  \n",
       "9                    0.019235  \n",
       "0                    0.032094  \n",
       "0                    0.023022  \n",
       "0                    0.027749  \n",
       "0                    0.020736  \n",
       "0                    0.016432  \n",
       "0                    0.011402  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_obj = sgd\n",
    "model_name = \"Stochastic Gradient Descent\"\n",
    "process = \"TFIDF with NLTK Stemming\"\n",
    "n_splits = 5\n",
    "X = train_ds_features.toarray()\n",
    "y = train_ds.sentiment\n",
    "stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n",
    "df_model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Process Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "gausian_process = GaussianProcessClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "gausian_process.fit(train_X.toarray(), train_y)\n",
    "test_ds_predicted = gausian_process.predict( test_X.toarray() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98       873\n",
      "           1       0.98      0.99      0.99      1203\n",
      "\n",
      "    accuracy                           0.98      2076\n",
      "   macro avg       0.98      0.98      0.98      2076\n",
      "weighted avg       0.98      0.98      0.98      2076\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print( metrics.classification_report( test_y, test_ds_predicted ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-1affdd926fd6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_ds_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentiment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mstratified_K_fold_validation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mdf_model_selection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-96a8a085cfe4>\u001b[0m in \u001b[0;36mstratified_K_fold_validation\u001b[1;34m(model_obj, model_name, process, n_splits, X, y)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mval_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mval_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mmodel_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m##### HERE ###\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mtest_ds_predicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;31m##### HERE ####\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;31m#print( metrics.classification_report( y_test, test_ds_predicted ) )\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\gpc.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    632\u001b[0m                                  % self.multi_class)\n\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 634\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    635\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_classes_\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\gpc.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    233\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_marginal_likelihood_value_\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_marginal_likelihood\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtheta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m         \u001b[1;31m# Precompute quantities required for predictions which are independent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\gpc.py\u001b[0m in \u001b[0;36mlog_marginal_likelihood\u001b[1;34m(self, theta, eval_gradient)\u001b[0m\n\u001b[0;32m    341\u001b[0m             \u001b[0mK\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mK_gradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX_train_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_gradient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 343\u001b[1;33m             \u001b[0mK\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX_train_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[1;31m# Compute log-marginal-likelihood Z and also store some temporaries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, X, Y, eval_gradient)\u001b[0m\n\u001b[0;32m    762\u001b[0m                                        K2_gradient * K1[:, :, np.newaxis]))\n\u001b[0;32m    763\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 764\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mk1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mk2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    765\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdiag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, X, Y, eval_gradient)\u001b[0m\n\u001b[0;32m   1213\u001b[0m         \u001b[0mlength_scale\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_length_scale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlength_scale\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1214\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mY\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1215\u001b[1;33m             \u001b[0mdists\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpdist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlength_scale\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sqeuclidean'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1216\u001b[0m             \u001b[0mK\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m.5\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdists\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1217\u001b[0m             \u001b[1;31m# convert from upper-triangular matrix to square matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py\u001b[0m in \u001b[0;36mpdist\u001b[1;34m(X, metric, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2064\u001b[0m             pdist_fn = getattr(_distance_wrap,\n\u001b[0;32m   2065\u001b[0m                                \"pdist_%s_%s_wrap\" % (metric_name, typ))\n\u001b[1;32m-> 2066\u001b[1;33m             \u001b[0mpdist_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2067\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mdm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2068\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_obj = gausian_process\n",
    "model_name = \"Gausian Process\"\n",
    "process = \"TFIDF with NLTK Stemming\"\n",
    "n_splits = 5\n",
    "X = train_ds_features.toarray()\n",
    "y = train_ds.sentiment\n",
    "stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n",
    "df_model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(train_X.toarray(), train_y)\n",
    "test_ds_predicted = knn.predict( test_X.toarray() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97       873\n",
      "           1       0.97      0.98      0.98      1203\n",
      "\n",
      "    accuracy                           0.97      2076\n",
      "   macro avg       0.97      0.97      0.97      2076\n",
      "weighted avg       0.97      0.97      0.97      2076\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print( metrics.classification_report( test_y, test_ds_predicted ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Process</th>\n",
       "      <th>Model Name</th>\n",
       "      <th>F1 Scores</th>\n",
       "      <th>Range of F1 Scores</th>\n",
       "      <th>Std Deviation of F1 Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Binomial Naive Bayes Classifier</td>\n",
       "      <td>[0.92, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.032094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>[0.95, 0.99, 0.99, 1.0, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.020736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Decission Tree</td>\n",
       "      <td>[0.92, 0.98, 0.98, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.031305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>[0.93, 0.97, 0.98, 0.99, 1.0]</td>\n",
       "      <td>0.93-1.0</td>\n",
       "      <td>0.027019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>XG Boost</td>\n",
       "      <td>[0.95, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.019235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Stochastic Gradient Descent</td>\n",
       "      <td>[0.95, 0.99, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.019494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Gausian Process</td>\n",
       "      <td>[0.94, 0.99, 0.99, 1.0, 1.0]</td>\n",
       "      <td>0.94-1.0</td>\n",
       "      <td>0.025100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>K Nearst Neighbour</td>\n",
       "      <td>[0.92, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.032094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>[0.98, 0.99, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.98-1.0</td>\n",
       "      <td>0.007071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>[0.95, 0.97, 0.98, 0.99, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.019235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Binomial Naive Bayes Classifier</td>\n",
       "      <td>[0.92, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.032094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>[0.95, 0.97, 1.0, 1.0, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.023022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Decission Tree</td>\n",
       "      <td>[0.93, 0.95, 0.98, 0.98, 1.0]</td>\n",
       "      <td>0.93-1.0</td>\n",
       "      <td>0.027749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>[0.94, 0.95, 0.97, 0.98, 0.99]</td>\n",
       "      <td>0.94-0.99</td>\n",
       "      <td>0.020736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>XG Boost</td>\n",
       "      <td>[0.96, 0.97, 0.97, 0.99, 1.0]</td>\n",
       "      <td>0.96-1.0</td>\n",
       "      <td>0.016432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Stochastic Gradient Descent</td>\n",
       "      <td>[0.97, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.97-1.0</td>\n",
       "      <td>0.011402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>K Nearst Neighbour</td>\n",
       "      <td>[0.94, 0.96, 0.96, 0.97, 1.0]</td>\n",
       "      <td>0.94-1.0</td>\n",
       "      <td>0.021909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Process                       Model Name  \\\n",
       "0  Bag Of Words with NLTK Stemming  Binomial Naive Bayes Classifier   \n",
       "1  Bag Of Words with NLTK Stemming              Logistic Regression   \n",
       "2  Bag Of Words with NLTK Stemming                   Decission Tree   \n",
       "3  Bag Of Words with NLTK Stemming                    Random Forest   \n",
       "4  Bag Of Words with NLTK Stemming                         XG Boost   \n",
       "5  Bag Of Words with NLTK Stemming      Stochastic Gradient Descent   \n",
       "6  Bag Of Words with NLTK Stemming                  Gausian Process   \n",
       "7  Bag Of Words with NLTK Stemming               K Nearst Neighbour   \n",
       "8  Bag Of Words with NLTK Stemming     Linear Discriminant Analysis   \n",
       "9  Bag Of Words with NLTK Stemming           Support Vector Machine   \n",
       "0         TFIDF with NLTK Stemming  Binomial Naive Bayes Classifier   \n",
       "0         TFIDF with NLTK Stemming              Logistic Regression   \n",
       "0         TFIDF with NLTK Stemming                   Decission Tree   \n",
       "0         TFIDF with NLTK Stemming                    Random Forest   \n",
       "0         TFIDF with NLTK Stemming                         XG Boost   \n",
       "0         TFIDF with NLTK Stemming      Stochastic Gradient Descent   \n",
       "0         TFIDF with NLTK Stemming               K Nearst Neighbour   \n",
       "\n",
       "                        F1 Scores Range of F1 Scores  \\\n",
       "0   [0.92, 0.98, 0.99, 0.99, 1.0]           0.92-1.0   \n",
       "1    [0.95, 0.99, 0.99, 1.0, 1.0]           0.95-1.0   \n",
       "2   [0.92, 0.98, 0.98, 0.99, 1.0]           0.92-1.0   \n",
       "3   [0.93, 0.97, 0.98, 0.99, 1.0]           0.93-1.0   \n",
       "4   [0.95, 0.98, 0.99, 0.99, 1.0]           0.95-1.0   \n",
       "5   [0.95, 0.99, 0.99, 0.99, 1.0]           0.95-1.0   \n",
       "6    [0.94, 0.99, 0.99, 1.0, 1.0]           0.94-1.0   \n",
       "7   [0.92, 0.97, 0.99, 0.99, 1.0]           0.92-1.0   \n",
       "8   [0.98, 0.99, 0.99, 0.99, 1.0]           0.98-1.0   \n",
       "9   [0.95, 0.97, 0.98, 0.99, 1.0]           0.95-1.0   \n",
       "0   [0.92, 0.98, 0.99, 0.99, 1.0]           0.92-1.0   \n",
       "0     [0.95, 0.97, 1.0, 1.0, 1.0]           0.95-1.0   \n",
       "0   [0.93, 0.95, 0.98, 0.98, 1.0]           0.93-1.0   \n",
       "0  [0.94, 0.95, 0.97, 0.98, 0.99]          0.94-0.99   \n",
       "0   [0.96, 0.97, 0.97, 0.99, 1.0]           0.96-1.0   \n",
       "0   [0.97, 0.98, 0.99, 0.99, 1.0]           0.97-1.0   \n",
       "0   [0.94, 0.96, 0.96, 0.97, 1.0]           0.94-1.0   \n",
       "\n",
       "   Std Deviation of F1 Scores  \n",
       "0                    0.032094  \n",
       "1                    0.020736  \n",
       "2                    0.031305  \n",
       "3                    0.027019  \n",
       "4                    0.019235  \n",
       "5                    0.019494  \n",
       "6                    0.025100  \n",
       "7                    0.032094  \n",
       "8                    0.007071  \n",
       "9                    0.019235  \n",
       "0                    0.032094  \n",
       "0                    0.023022  \n",
       "0                    0.027749  \n",
       "0                    0.020736  \n",
       "0                    0.016432  \n",
       "0                    0.011402  \n",
       "0                    0.021909  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_obj = knn\n",
    "model_name = \"K Nearst Neighbour\"\n",
    "process = \"TFIDF with NLTK Stemming\"\n",
    "n_splits = 5\n",
    "X = train_ds_features.toarray()\n",
    "y = train_ds.sentiment\n",
    "stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n",
    "df_model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda = LinearDiscriminantAnalysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.fit(train_X.toarray(), train_y)\n",
    "test_ds_predicted = lda.predict( test_X.toarray() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.95       873\n",
      "           1       0.96      0.97      0.97      1203\n",
      "\n",
      "    accuracy                           0.96      2076\n",
      "   macro avg       0.96      0.96      0.96      2076\n",
      "weighted avg       0.96      0.96      0.96      2076\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print( metrics.classification_report( test_y, test_ds_predicted ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Process</th>\n",
       "      <th>Model Name</th>\n",
       "      <th>F1 Scores</th>\n",
       "      <th>Range of F1 Scores</th>\n",
       "      <th>Std Deviation of F1 Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Binomial Naive Bayes Classifier</td>\n",
       "      <td>[0.92, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.032094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>[0.95, 0.99, 0.99, 1.0, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.020736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Decission Tree</td>\n",
       "      <td>[0.92, 0.98, 0.98, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.031305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>[0.93, 0.97, 0.98, 0.99, 1.0]</td>\n",
       "      <td>0.93-1.0</td>\n",
       "      <td>0.027019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>XG Boost</td>\n",
       "      <td>[0.95, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.019235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Stochastic Gradient Descent</td>\n",
       "      <td>[0.95, 0.99, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.019494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Gausian Process</td>\n",
       "      <td>[0.94, 0.99, 0.99, 1.0, 1.0]</td>\n",
       "      <td>0.94-1.0</td>\n",
       "      <td>0.025100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>K Nearst Neighbour</td>\n",
       "      <td>[0.92, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.032094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>[0.98, 0.99, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.98-1.0</td>\n",
       "      <td>0.007071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>[0.95, 0.97, 0.98, 0.99, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.019235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Binomial Naive Bayes Classifier</td>\n",
       "      <td>[0.92, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.032094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>[0.95, 0.97, 1.0, 1.0, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.023022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Decission Tree</td>\n",
       "      <td>[0.93, 0.95, 0.98, 0.98, 1.0]</td>\n",
       "      <td>0.93-1.0</td>\n",
       "      <td>0.027749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>[0.94, 0.95, 0.97, 0.98, 0.99]</td>\n",
       "      <td>0.94-0.99</td>\n",
       "      <td>0.020736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>XG Boost</td>\n",
       "      <td>[0.96, 0.97, 0.97, 0.99, 1.0]</td>\n",
       "      <td>0.96-1.0</td>\n",
       "      <td>0.016432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Stochastic Gradient Descent</td>\n",
       "      <td>[0.97, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.97-1.0</td>\n",
       "      <td>0.011402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>K Nearst Neighbour</td>\n",
       "      <td>[0.94, 0.96, 0.96, 0.97, 1.0]</td>\n",
       "      <td>0.94-1.0</td>\n",
       "      <td>0.021909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>[0.93, 0.94, 0.96, 0.96, 1.0]</td>\n",
       "      <td>0.93-1.0</td>\n",
       "      <td>0.026833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Process                       Model Name  \\\n",
       "0  Bag Of Words with NLTK Stemming  Binomial Naive Bayes Classifier   \n",
       "1  Bag Of Words with NLTK Stemming              Logistic Regression   \n",
       "2  Bag Of Words with NLTK Stemming                   Decission Tree   \n",
       "3  Bag Of Words with NLTK Stemming                    Random Forest   \n",
       "4  Bag Of Words with NLTK Stemming                         XG Boost   \n",
       "5  Bag Of Words with NLTK Stemming      Stochastic Gradient Descent   \n",
       "6  Bag Of Words with NLTK Stemming                  Gausian Process   \n",
       "7  Bag Of Words with NLTK Stemming               K Nearst Neighbour   \n",
       "8  Bag Of Words with NLTK Stemming     Linear Discriminant Analysis   \n",
       "9  Bag Of Words with NLTK Stemming           Support Vector Machine   \n",
       "0         TFIDF with NLTK Stemming  Binomial Naive Bayes Classifier   \n",
       "0         TFIDF with NLTK Stemming              Logistic Regression   \n",
       "0         TFIDF with NLTK Stemming                   Decission Tree   \n",
       "0         TFIDF with NLTK Stemming                    Random Forest   \n",
       "0         TFIDF with NLTK Stemming                         XG Boost   \n",
       "0         TFIDF with NLTK Stemming      Stochastic Gradient Descent   \n",
       "0         TFIDF with NLTK Stemming               K Nearst Neighbour   \n",
       "0         TFIDF with NLTK Stemming     Linear Discriminant Analysis   \n",
       "\n",
       "                        F1 Scores Range of F1 Scores  \\\n",
       "0   [0.92, 0.98, 0.99, 0.99, 1.0]           0.92-1.0   \n",
       "1    [0.95, 0.99, 0.99, 1.0, 1.0]           0.95-1.0   \n",
       "2   [0.92, 0.98, 0.98, 0.99, 1.0]           0.92-1.0   \n",
       "3   [0.93, 0.97, 0.98, 0.99, 1.0]           0.93-1.0   \n",
       "4   [0.95, 0.98, 0.99, 0.99, 1.0]           0.95-1.0   \n",
       "5   [0.95, 0.99, 0.99, 0.99, 1.0]           0.95-1.0   \n",
       "6    [0.94, 0.99, 0.99, 1.0, 1.0]           0.94-1.0   \n",
       "7   [0.92, 0.97, 0.99, 0.99, 1.0]           0.92-1.0   \n",
       "8   [0.98, 0.99, 0.99, 0.99, 1.0]           0.98-1.0   \n",
       "9   [0.95, 0.97, 0.98, 0.99, 1.0]           0.95-1.0   \n",
       "0   [0.92, 0.98, 0.99, 0.99, 1.0]           0.92-1.0   \n",
       "0     [0.95, 0.97, 1.0, 1.0, 1.0]           0.95-1.0   \n",
       "0   [0.93, 0.95, 0.98, 0.98, 1.0]           0.93-1.0   \n",
       "0  [0.94, 0.95, 0.97, 0.98, 0.99]          0.94-0.99   \n",
       "0   [0.96, 0.97, 0.97, 0.99, 1.0]           0.96-1.0   \n",
       "0   [0.97, 0.98, 0.99, 0.99, 1.0]           0.97-1.0   \n",
       "0   [0.94, 0.96, 0.96, 0.97, 1.0]           0.94-1.0   \n",
       "0   [0.93, 0.94, 0.96, 0.96, 1.0]           0.93-1.0   \n",
       "\n",
       "   Std Deviation of F1 Scores  \n",
       "0                    0.032094  \n",
       "1                    0.020736  \n",
       "2                    0.031305  \n",
       "3                    0.027019  \n",
       "4                    0.019235  \n",
       "5                    0.019494  \n",
       "6                    0.025100  \n",
       "7                    0.032094  \n",
       "8                    0.007071  \n",
       "9                    0.019235  \n",
       "0                    0.032094  \n",
       "0                    0.023022  \n",
       "0                    0.027749  \n",
       "0                    0.020736  \n",
       "0                    0.016432  \n",
       "0                    0.011402  \n",
       "0                    0.021909  \n",
       "0                    0.026833  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_obj = lda\n",
    "model_name = \"Linear Discriminant Analysis\"\n",
    "process = \"TFIDF with NLTK Stemming\"\n",
    "n_splits = 5\n",
    "X = train_ds_features.toarray()\n",
    "y = train_ds.sentiment\n",
    "stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n",
    "df_model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.fit(train_X.toarray(), train_y)\n",
    "test_ds_predicted = svm.predict( test_X.toarray() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.12      0.22       873\n",
      "           1       0.61      1.00      0.76      1203\n",
      "\n",
      "    accuracy                           0.63      2076\n",
      "   macro avg       0.81      0.56      0.49      2076\n",
      "weighted avg       0.77      0.63      0.53      2076\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print( metrics.classification_report( test_y, test_ds_predicted ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_obj = svm\n",
    "model_name = \"Support Vector Machine\"\n",
    "process = \"TFIDF with NLTK Stemming\"\n",
    "n_splits = 5\n",
    "X = train_ds_features.toarray()\n",
    "y = train_ds.sentiment\n",
    "stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n",
    "df_model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_selection.to_csv(\"Model_statistics.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
