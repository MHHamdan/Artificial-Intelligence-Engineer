{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The Da Vinci Code book is just awesome.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>this was the first clive cussler i've ever rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>i liked the Da Vinci Code a lot.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>i liked the Da Vinci Code a lot.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>I liked the Da Vinci Code but it ultimatly did...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text\n",
       "0          1            The Da Vinci Code book is just awesome.\n",
       "1          1  this was the first clive cussler i've ever rea...\n",
       "2          1                   i liked the Da Vinci Code a lot.\n",
       "3          1                   i liked the Da Vinci Code a lot.\n",
       "4          1  I liked the Da Vinci Code but it ultimatly did..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "train_ds = pd.read_csv( \"data_for_sentiment_analysis\", delimiter=\"\\t\" )\n",
    "train_ds.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMN_NAMES = [\"Process\",\"Model Name\", \"F1 Scores\",\"Range of F1 Scores\",\"Std Deviation of F1 Scores\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_selection = pd.read_csv(\"Model_statistics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import metrics\n",
    "def stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y):\n",
    "    global df_model_selection\n",
    "    skf = StratifiedKFold(n_splits=5, random_state=29)\n",
    "    weighted_f1_score = []\n",
    "    for train_index, val_index in skf.split(X,y):\n",
    "        X_train, X_test = X[train_index], X[val_index] \n",
    "        y_train, y_test = y[train_index], y[val_index]\n",
    "        model_obj.fit(X_train, y_train)##### HERE ###\n",
    "        test_ds_predicted = model_obj.predict( X_test ) ##### HERE ####   \n",
    "        #print( metrics.classification_report( y_test, test_ds_predicted ) )    \n",
    "        weighted_f1_score.append(round(f1_score(y_test, test_ds_predicted , average='weighted'),2))\n",
    "        \n",
    "    sd_weighted_f1_score = np.std(weighted_f1_score, ddof=1)\n",
    "    range_of_f1_scores = \"{}-{}\".format(min(weighted_f1_score),max(weighted_f1_score))    \n",
    "    df_model_selection = pd.concat([df_model_selection,pd.DataFrame([[process,model_name,sorted(weighted_f1_score),range_of_f1_scores,sd_weighted_f1_score]], columns =COLUMN_NAMES) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few stop words:  ['an', 'out', 'although', 'anything', 'onto', 'call', 'ten', 'noone', 'anywhere', 'six']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "my_stop_words = text.ENGLISH_STOP_WORDS\n",
    "#Printing first few stop words\n",
    "print(\"Few stop words: \", list(my_stop_words)[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding custom words to the list of stop words\n",
    "my_stop_words = text.ENGLISH_STOP_WORDS.union( ['harry', 'potter', 'code', 'vinci', 'da',\n",
    "'harri', 'mountain', 'movie', 'movies'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting stop words list\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vectorizer = CountVectorizer( stop_words = my_stop_words,\n",
    "max_features = 1000 )\n",
    "feature_vector = count_vectorizer.fit( train_ds.text )\n",
    "train_ds_features = count_vectorizer.transform( train_ds.text )\n",
    "features = feature_vector.get_feature_names()\n",
    "features_counts = np.sum( train_ds_features.toarray(), axis = 0 )\n",
    "feature_counts = pd.DataFrame( dict( features = features,\n",
    "counts = features_counts ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning - Stemming or Lemmatization\n",
    "\n",
    "### To get words into root form and hence in a motivation of decreasing few more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. PorterStemmer\n",
    "#2. LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "\n",
    "\n",
    "#Custom function for stemming and stop word removal\n",
    "def stemmed_words(doc):\n",
    "    ### Stemming of words\n",
    "    stemmed_words = (stemmer.stem(w) for w in analyzer(doc))\n",
    "    ### Remove the words in stop words list\n",
    "    non_stop_words = [ word for word in list(set(stemmed_words) - set(my_stop_words)) ]\n",
    "    return non_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>brokeback</td>\n",
       "      <td>1930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>love</td>\n",
       "      <td>1837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>suck</td>\n",
       "      <td>1378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>922</th>\n",
       "      <td>wa</td>\n",
       "      <td>1142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>awesom</td>\n",
       "      <td>1116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>mission</td>\n",
       "      <td>1090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>imposs</td>\n",
       "      <td>1090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>movi</td>\n",
       "      <td>1052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>like</td>\n",
       "      <td>823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>hate</td>\n",
       "      <td>636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>becaus</td>\n",
       "      <td>524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>realli</td>\n",
       "      <td>370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794</th>\n",
       "      <td>stupid</td>\n",
       "      <td>364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>know</td>\n",
       "      <td>354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>read</td>\n",
       "      <td>284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      features  counts\n",
       "80   brokeback    1930\n",
       "406       love    1837\n",
       "801       suck    1378\n",
       "922         wa    1142\n",
       "43      awesom    1116\n",
       "432    mission    1090\n",
       "344     imposs    1090\n",
       "438       movi    1052\n",
       "392       like     823\n",
       "298       hate     636\n",
       "54      becaus     524\n",
       "602     realli     370\n",
       "794     stupid     364\n",
       "378       know     354\n",
       "597       read     284"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer( analyzer=stemmed_words,\n",
    "max_features = 1000)\n",
    "feature_vector = count_vectorizer.fit( train_ds.text )\n",
    "train_ds_features = count_vectorizer.transform( train_ds.text )\n",
    "features = feature_vector.get_feature_names()\n",
    "features_counts = np.sum( train_ds_features.toarray(), axis = 0 )\n",
    "feature_counts = pd.DataFrame( dict( features = features,\n",
    "counts = features_counts ) )\n",
    "feature_counts.sort_values( \"counts\", ascending = False )[0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Z003RJMK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "# library for regular expressions\n",
    "import re\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stemmed_tokens( doc ):\n",
    "    # Tokenize the documents to words\n",
    "    all_tokens = [word for word in nltk.word_tokenize(doc)]\n",
    "    clean_tokens = []\n",
    "    # remove the all characters other than alphabets. It takes a regex for matching.\n",
    "    for each_token in all_tokens:\n",
    "        if re.search('[a-zA-Z]', each_token):\n",
    "            clean_tokens.append(each_token)\n",
    "    \n",
    "    \n",
    "    # Stem the words\n",
    "    stemmed_tokens = [stemmer.stem(t) for t in clean_tokens]\n",
    "    return stemmed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=500,stop_words='english',tokenizer=get_stemmed_tokens,ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vector = tfidf_vectorizer.fit( train_ds.text )\n",
    "train_ds_features = tfidf_vectorizer.transform( train_ds.text )\n",
    "features = feature_vector.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'m\",\n",
       " \"'re\",\n",
       " \"'re gon\",\n",
       " \"'s\",\n",
       " \"'s great\",\n",
       " \"'s like\",\n",
       " \"'s mom\",\n",
       " \"'s onli\",\n",
       " \"'s retart\",\n",
       " \"'s right\",\n",
       " \"'s stupid\",\n",
       " \"'yeah\",\n",
       " \"'yeah got\",\n",
       " 'absolut',\n",
       " 'absolut awesom',\n",
       " 'accept',\n",
       " 'ach',\n",
       " 'ach cock',\n",
       " 'acn',\n",
       " 'acn love',\n",
       " 'alway',\n",
       " 'alway know',\n",
       " 'anyon',\n",
       " 'anyon say',\n",
       " 'ass',\n",
       " 'award',\n",
       " 'award remind',\n",
       " 'awesom',\n",
       " 'awesom book',\n",
       " 'awesom ca',\n",
       " 'awesom movi',\n",
       " \"awesom n't\",\n",
       " 'awesom stori',\n",
       " 'awesome..',\n",
       " 'b',\n",
       " 'b suck',\n",
       " 'beauti',\n",
       " 'becaus',\n",
       " 'becaus awesom',\n",
       " 'becaus hate',\n",
       " 'becaus know',\n",
       " 'becaus like',\n",
       " 'becaus love',\n",
       " 'becaus outshin',\n",
       " 'becaus type',\n",
       " 'becom',\n",
       " 'becom accept',\n",
       " 'begin',\n",
       " 'better',\n",
       " 'better read',\n",
       " 'big',\n",
       " 'big time',\n",
       " 'bitch',\n",
       " 'black',\n",
       " 'black guy',\n",
       " 'blond',\n",
       " 'blond rock-hard',\n",
       " 'bobbypin',\n",
       " 'bobbypin insan',\n",
       " 'bonker',\n",
       " 'book',\n",
       " 'book catcher',\n",
       " 'bore',\n",
       " 'brokeback',\n",
       " 'brokeback mountain',\n",
       " 'bye..',\n",
       " 'ca',\n",
       " \"ca n't\",\n",
       " 'care',\n",
       " 'care anyon',\n",
       " 'catcher',\n",
       " 'catcher tye',\n",
       " 'charact',\n",
       " 'charact die',\n",
       " 'clean',\n",
       " 'clean tabl',\n",
       " 'cock',\n",
       " 'code',\n",
       " 'code awesom',\n",
       " 'code awesome..',\n",
       " 'code film',\n",
       " 'code left',\n",
       " 'code suck',\n",
       " 'code sucked..',\n",
       " 'code wa',\n",
       " 'combin',\n",
       " 'combin opinion',\n",
       " 'commun',\n",
       " 'commun like',\n",
       " 'cool',\n",
       " 'cool hat',\n",
       " 'count',\n",
       " 'count book',\n",
       " 'cowboy',\n",
       " 'cowboy jokes..',\n",
       " 'coz',\n",
       " 'coz wa',\n",
       " 'crazi',\n",
       " 'crazi hate',\n",
       " 'cruis',\n",
       " 'da',\n",
       " 'da vinci',\n",
       " 'dad',\n",
       " \"dad 's\",\n",
       " 'daniel',\n",
       " 'daniel wotshisfac',\n",
       " 'dash',\n",
       " 'dash like',\n",
       " 'deep',\n",
       " 'deep profound',\n",
       " 'depress',\n",
       " 'depress movi',\n",
       " 'desper',\n",
       " \"desper love'th\",\n",
       " 'despis',\n",
       " 'despis movi',\n",
       " 'did',\n",
       " 'die',\n",
       " 'differ',\n",
       " 'doe',\n",
       " 'doe harri',\n",
       " 'draco',\n",
       " 'draco malfoy',\n",
       " 'drag',\n",
       " 'drag draco',\n",
       " 'dudee',\n",
       " 'dudee love',\n",
       " 'enjoy',\n",
       " 'escapad',\n",
       " 'escapad mission',\n",
       " 'evil',\n",
       " 'excel',\n",
       " 'eyr',\n",
       " 'eyr virgin',\n",
       " 'felicia',\n",
       " \"felicia 's\",\n",
       " 'felicia grab',\n",
       " 'film',\n",
       " 'freakin',\n",
       " 'freakin mission',\n",
       " 'friday',\n",
       " 'friday hung',\n",
       " 'friend',\n",
       " 'friend like',\n",
       " 'fuck',\n",
       " 'fuck horrible..',\n",
       " 'fuck slap',\n",
       " 'fun',\n",
       " 'gari',\n",
       " 'gari gin',\n",
       " 'gay',\n",
       " 'gay stupid',\n",
       " 'gin',\n",
       " 'gin zen',\n",
       " 'goin',\n",
       " 'goin mission',\n",
       " 'gon',\n",
       " 'gon na',\n",
       " 'good',\n",
       " 'good start',\n",
       " 'got',\n",
       " 'got acn',\n",
       " 'grab',\n",
       " 'grab key',\n",
       " 'great',\n",
       " 'great homosexu',\n",
       " 'groan',\n",
       " 'groan blond',\n",
       " 'guy',\n",
       " 'guy crazi',\n",
       " 'harri',\n",
       " 'harri potter',\n",
       " 'harri potter..',\n",
       " 'hat',\n",
       " 'hat head',\n",
       " 'hate',\n",
       " 'hate brokeback',\n",
       " 'hate da',\n",
       " 'hate harri',\n",
       " 'head',\n",
       " 'head laugh',\n",
       " 'heard',\n",
       " 'heard da',\n",
       " 'hella',\n",
       " 'hella like',\n",
       " 'help',\n",
       " 'help bobbypin',\n",
       " 'hi',\n",
       " 'hi hip',\n",
       " 'hi throat',\n",
       " 'hill',\n",
       " 'hill turn',\n",
       " 'hip',\n",
       " 'hip suck',\n",
       " 'homosexu',\n",
       " 'homosexu becom',\n",
       " 'hoot',\n",
       " 'horribl',\n",
       " 'horribl movi',\n",
       " 'horrible..',\n",
       " 'hung',\n",
       " 'hung kelsi',\n",
       " 'iii',\n",
       " 'imposs',\n",
       " 'imposs awesom',\n",
       " 'imposs bitch',\n",
       " 'imposs hoot',\n",
       " 'imposs iii',\n",
       " 'imposs movi',\n",
       " 'imposs rock',\n",
       " 'imposs station',\n",
       " 'imposs suck',\n",
       " 'imposs tom',\n",
       " 'imposs wa',\n",
       " 'insan',\n",
       " 'insan cool',\n",
       " 'jane',\n",
       " 'jane eyr',\n",
       " 'join',\n",
       " 'join commun',\n",
       " 'jokes..',\n",
       " 'just',\n",
       " 'just doe',\n",
       " 'just let',\n",
       " 'just plain',\n",
       " 'kate',\n",
       " 'kate escapad',\n",
       " 'kelsi',\n",
       " 'kelsi went',\n",
       " 'key',\n",
       " 'key dash',\n",
       " 'kirsten',\n",
       " 'kirsten leah',\n",
       " 'know',\n",
       " \"know 's\",\n",
       " 'know love',\n",
       " 'know want',\n",
       " 'laugh',\n",
       " 'laugh stupid',\n",
       " 'leah',\n",
       " 'leah kate',\n",
       " 'left',\n",
       " 'left right',\n",
       " 'let',\n",
       " 'let know',\n",
       " 'like',\n",
       " \"like 'yeah\",\n",
       " 'like freakin',\n",
       " 'like goin',\n",
       " 'like harri',\n",
       " 'like main',\n",
       " 'like make',\n",
       " 'like mission',\n",
       " 'like realli',\n",
       " 'like thi',\n",
       " 'like thing',\n",
       " 'like watch',\n",
       " 'lot',\n",
       " 'love',\n",
       " 'love brokeback',\n",
       " 'love da',\n",
       " 'love harri',\n",
       " 'love kirsten',\n",
       " 'love luv',\n",
       " 'love mission',\n",
       " 'love sentri',\n",
       " \"love'th\",\n",
       " \"love'th da\",\n",
       " 'lubb',\n",
       " 'lubb da',\n",
       " 'luv',\n",
       " 'luv lubb',\n",
       " 'main',\n",
       " 'main charact',\n",
       " 'make',\n",
       " 'make friend',\n",
       " 'make whimper',\n",
       " 'malfoy',\n",
       " 'malfoy s',\n",
       " 'man',\n",
       " 'man love',\n",
       " 'materi',\n",
       " 'materi movi',\n",
       " 'mission',\n",
       " 'mission imposs',\n",
       " 'mom',\n",
       " 'mom clean',\n",
       " 'mountain',\n",
       " 'mountain beauti',\n",
       " 'mountain cowboy',\n",
       " 'mountain depress',\n",
       " 'mountain fuck',\n",
       " 'mountain horribl',\n",
       " 'mountain suck',\n",
       " 'mountain terribl',\n",
       " 'mountain think',\n",
       " 'mountain wa',\n",
       " 'movi',\n",
       " 'movi award',\n",
       " 'movi becaus',\n",
       " 'movi brokeback',\n",
       " 'movi just',\n",
       " 'movi realli',\n",
       " 'movi seen..',\n",
       " 'mtv',\n",
       " 'mtv movi',\n",
       " \"n't\",\n",
       " \"n't care\",\n",
       " \"n't wait\",\n",
       " 'na',\n",
       " 'na like',\n",
       " 'need',\n",
       " 'need fuck',\n",
       " 'nois',\n",
       " 'nois pant',\n",
       " 'oh',\n",
       " 'oh brokeback',\n",
       " 'ok',\n",
       " 'ok brokeback',\n",
       " 'ok bye..',\n",
       " 'onli',\n",
       " 'onli black',\n",
       " 'onli star',\n",
       " 'opinion',\n",
       " 'opinion review',\n",
       " 'outshin',\n",
       " 'outshin better',\n",
       " 'pant',\n",
       " 'pant groan',\n",
       " 'past',\n",
       " 'past hi',\n",
       " 'peopl',\n",
       " 'peopl seriou',\n",
       " 'peopl worth',\n",
       " 'person',\n",
       " 'person like',\n",
       " 'place',\n",
       " 'place peopl',\n",
       " 'plain',\n",
       " 'plain stupid',\n",
       " 'potter',\n",
       " \"potter 's\",\n",
       " 'potter awesom',\n",
       " 'potter becaus',\n",
       " 'potter daniel',\n",
       " 'potter deep',\n",
       " 'potter drag',\n",
       " 'potter movi',\n",
       " 'potter realli',\n",
       " 'potter seri',\n",
       " 'potter suck',\n",
       " 'potter thought',\n",
       " 'potter..',\n",
       " 'potter.. hate',\n",
       " 'pretti',\n",
       " 'profound',\n",
       " 'profound love',\n",
       " 'quiz',\n",
       " 'quiz suck',\n",
       " 'read',\n",
       " 'read harri',\n",
       " 'read materi',\n",
       " 'realiti',\n",
       " 'realiti coz',\n",
       " 'realli',\n",
       " 'realli depress',\n",
       " 'realli like',\n",
       " 'realli suck',\n",
       " 'realli want',\n",
       " 'remind',\n",
       " 'remind despis',\n",
       " 'retart',\n",
       " 'retart gay',\n",
       " 'review',\n",
       " 'review gari',\n",
       " 'right',\n",
       " 'right b',\n",
       " 'right left',\n",
       " 'rock',\n",
       " 'rock-hard',\n",
       " 'rock-hard ach',\n",
       " 's',\n",
       " 's trouser',\n",
       " 'said',\n",
       " 'said silent',\n",
       " 'saw',\n",
       " 'saw da',\n",
       " 'say',\n",
       " 'say differ',\n",
       " 'seen..',\n",
       " 'sentri',\n",
       " 'sentri mission',\n",
       " 'seri',\n",
       " 'seri becaus',\n",
       " 'seri count',\n",
       " 'seriou',\n",
       " 'seriou fun',\n",
       " 'silent',\n",
       " 'silent hill',\n",
       " 'sit',\n",
       " 'sit watch',\n",
       " 'slap',\n",
       " 'snuck',\n",
       " 'snuck brokeback',\n",
       " 'soo',\n",
       " 'soo onli',\n",
       " 'stand',\n",
       " 'stand mission',\n",
       " 'star',\n",
       " 'start',\n",
       " 'start read',\n",
       " 'station',\n",
       " 'station bonker',\n",
       " 'stori',\n",
       " 'stori harri',\n",
       " 'stupid',\n",
       " \"stupid 's\",\n",
       " 'stupid begin',\n",
       " 'stupid brokeback',\n",
       " 'suck',\n",
       " 'suck ass',\n",
       " 'suck big',\n",
       " 'suck harri',\n",
       " 'suck hi',\n",
       " 'suck just',\n",
       " 'suck ok',\n",
       " 'suck soo',\n",
       " 'sucked..',\n",
       " 'suicid',\n",
       " 'suicid yeah',\n",
       " 'tabl',\n",
       " 'tabl felicia',\n",
       " 'talk',\n",
       " 'terribl',\n",
       " 'terribl movi',\n",
       " 'thi',\n",
       " 'thi good',\n",
       " 'thi quiz',\n",
       " 'thing',\n",
       " 'thing like',\n",
       " 'think',\n",
       " \"think 's\",\n",
       " 'think hate',\n",
       " 'thought',\n",
       " 'thought join',\n",
       " 'throat',\n",
       " 'throat vigor',\n",
       " 'time',\n",
       " 'tom',\n",
       " 'tom cruis',\n",
       " 'trouser',\n",
       " 'trouser past',\n",
       " 'turn',\n",
       " 'turn realiti',\n",
       " 'tye',\n",
       " 'tye jane',\n",
       " 'type',\n",
       " 'type person',\n",
       " 'veri',\n",
       " 'vigor',\n",
       " 'vigor make',\n",
       " 'vinci',\n",
       " 'vinci code',\n",
       " 'virgin',\n",
       " 'virgin suicid',\n",
       " 'wa',\n",
       " 'wa absolut',\n",
       " 'wa awesom',\n",
       " 'wa beauti',\n",
       " 'wa bore',\n",
       " 'wa excel',\n",
       " 'wa hella',\n",
       " 'wa realli',\n",
       " 'wa terribl',\n",
       " 'wait',\n",
       " 'wait read',\n",
       " 'want',\n",
       " 'want becaus',\n",
       " 'want desper',\n",
       " 'want guy',\n",
       " 'want place',\n",
       " 'watch',\n",
       " 'watch mission',\n",
       " 'watch mtv',\n",
       " 'way',\n",
       " 'way da',\n",
       " 'went',\n",
       " 'went saw',\n",
       " 'whi',\n",
       " 'whi love',\n",
       " 'whi said',\n",
       " 'whimper',\n",
       " 'whimper nois',\n",
       " 'worth',\n",
       " 'worth know',\n",
       " 'wotshisfac',\n",
       " 'wotshisfac need',\n",
       " 'yeah',\n",
       " 'zen',\n",
       " 'zen da']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>'m</th>\n",
       "      <th>'re</th>\n",
       "      <th>'re gon</th>\n",
       "      <th>'s</th>\n",
       "      <th>'s great</th>\n",
       "      <th>'s like</th>\n",
       "      <th>'s mom</th>\n",
       "      <th>'s onli</th>\n",
       "      <th>'s retart</th>\n",
       "      <th>'s right</th>\n",
       "      <th>...</th>\n",
       "      <th>whimper</th>\n",
       "      <th>whimper nois</th>\n",
       "      <th>worth</th>\n",
       "      <th>worth know</th>\n",
       "      <th>wotshisfac</th>\n",
       "      <th>wotshisfac need</th>\n",
       "      <th>yeah</th>\n",
       "      <th>zen</th>\n",
       "      <th>zen da</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.33902</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6913</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6914</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6915</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6916</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6917</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6918 rows Ã— 501 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       'm  're  're gon       's  's great  's like  's mom  's onli  \\\n",
       "0     0.0  0.0      0.0  0.00000       0.0      0.0     0.0      0.0   \n",
       "1     0.0  0.0      0.0  0.00000       0.0      0.0     0.0      0.0   \n",
       "2     0.0  0.0      0.0  0.00000       0.0      0.0     0.0      0.0   \n",
       "3     0.0  0.0      0.0  0.00000       0.0      0.0     0.0      0.0   \n",
       "4     0.0  0.0      0.0  0.33902       0.0      0.0     0.0      0.0   \n",
       "...   ...  ...      ...      ...       ...      ...     ...      ...   \n",
       "6913  0.0  0.0      0.0  0.00000       0.0      0.0     0.0      0.0   \n",
       "6914  0.0  0.0      0.0  0.00000       0.0      0.0     0.0      0.0   \n",
       "6915  0.0  0.0      0.0  0.00000       0.0      0.0     0.0      0.0   \n",
       "6916  0.0  0.0      0.0  0.00000       0.0      0.0     0.0      0.0   \n",
       "6917  0.0  0.0      0.0  0.00000       0.0      0.0     0.0      0.0   \n",
       "\n",
       "      's retart  's right  ...  whimper  whimper nois  worth  worth know  \\\n",
       "0           0.0       0.0  ...      0.0           0.0    0.0         0.0   \n",
       "1           0.0       0.0  ...      0.0           0.0    0.0         0.0   \n",
       "2           0.0       0.0  ...      0.0           0.0    0.0         0.0   \n",
       "3           0.0       0.0  ...      0.0           0.0    0.0         0.0   \n",
       "4           0.0       0.0  ...      0.0           0.0    0.0         0.0   \n",
       "...         ...       ...  ...      ...           ...    ...         ...   \n",
       "6913        0.0       0.0  ...      0.0           0.0    0.0         0.0   \n",
       "6914        0.0       0.0  ...      0.0           0.0    0.0         0.0   \n",
       "6915        0.0       0.0  ...      0.0           0.0    0.0         0.0   \n",
       "6916        0.0       0.0  ...      0.0           0.0    0.0         0.0   \n",
       "6917        0.0       0.0  ...      0.0           0.0    0.0         0.0   \n",
       "\n",
       "      wotshisfac  wotshisfac need  yeah  zen  zen da  sentiment  \n",
       "0            0.0              0.0   0.0  0.0     0.0          1  \n",
       "1            0.0              0.0   0.0  0.0     0.0          1  \n",
       "2            0.0              0.0   0.0  0.0     0.0          1  \n",
       "3            0.0              0.0   0.0  0.0     0.0          1  \n",
       "4            0.0              0.0   0.0  0.0     0.0          1  \n",
       "...          ...              ...   ...  ...     ...        ...  \n",
       "6913         0.0              0.0   0.0  0.0     0.0          0  \n",
       "6914         0.0              0.0   0.0  0.0     0.0          0  \n",
       "6915         0.0              0.0   0.0  0.0     0.0          0  \n",
       "6916         0.0              0.0   0.0  0.0     0.0          0  \n",
       "6917         0.0              0.0   0.0  0.0     0.0          0  \n",
       "\n",
       "[6918 rows x 501 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the document vector matrix into dataframe\n",
    "train_ds_df = pd.DataFrame(train_ds_features.todense())\n",
    "# Assign the features names to the column\n",
    "train_ds_df.columns = features\n",
    "# Assign the sentiment labels to the train_ds\n",
    "train_ds_df['sentiment'] = train_ds.sentiment\n",
    "train_ds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_X, test_X, train_y, test_y = train_test_split( train_ds_features,train_ds.sentiment,test_size = 0.3,random_state = 42 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Model for Sentiment Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "nb_clf = BernoulliNB()\n",
    "nb_clf.fit( train_X.toarray(), train_y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds_predicted = nb_clf.predict( test_X.toarray() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97       873\n",
      "           1       0.96      1.00      0.98      1203\n",
      "\n",
      "    accuracy                           0.97      2076\n",
      "   macro avg       0.98      0.97      0.97      2076\n",
      "weighted avg       0.97      0.97      0.97      2076\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print( metrics.classification_report( test_y, test_ds_predicted ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Process</th>\n",
       "      <th>Model Name</th>\n",
       "      <th>F1 Scores</th>\n",
       "      <th>Range of F1 Scores</th>\n",
       "      <th>Std Deviation of F1 Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>[0.98, 0.99, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.98-1.0</td>\n",
       "      <td>0.007071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Stochastic Gradient Descent</td>\n",
       "      <td>[0.96, 0.98, 0.99, 1.0, 1.0]</td>\n",
       "      <td>0.96-1.0</td>\n",
       "      <td>0.016733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>XG Boost</td>\n",
       "      <td>[0.95, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>[0.95, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>[0.95, 0.98, 0.99, 1.0, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.020736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Binomial Naive Bayes Classifier</td>\n",
       "      <td>[0.92, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.032094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>[0.95, 0.97, 1.0, 1.0, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.023022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Decission Tree</td>\n",
       "      <td>[0.93, 0.95, 0.98, 0.98, 1.0]</td>\n",
       "      <td>0.93-1.0</td>\n",
       "      <td>0.027749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>[0.95, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.019235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>XG Boost</td>\n",
       "      <td>[0.96, 0.97, 0.97, 0.99, 1.0]</td>\n",
       "      <td>0.96-1.0</td>\n",
       "      <td>0.016432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Stochastic Gradient Descent</td>\n",
       "      <td>[0.97, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.97-1.0</td>\n",
       "      <td>0.013416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Gausian Process</td>\n",
       "      <td>[0.94, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.94-1.0</td>\n",
       "      <td>0.023875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>K Nearst Neighbour</td>\n",
       "      <td>[0.94, 0.96, 0.96, 0.97, 1.0]</td>\n",
       "      <td>0.94-1.0</td>\n",
       "      <td>0.021909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>[0.93, 0.94, 0.96, 0.96, 1.0]</td>\n",
       "      <td>0.93-1.0</td>\n",
       "      <td>0.026833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>[0.41, 0.51, 0.58, 0.64, 0.66]</td>\n",
       "      <td>0.41-0.66</td>\n",
       "      <td>0.102225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ngram with NLTK Stemming</td>\n",
       "      <td>Binomial Naive Bayes Classifier</td>\n",
       "      <td>[0.93, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.93-1.0</td>\n",
       "      <td>0.027928</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Process                       Model Name  \\\n",
       "0   Bag Of Words with NLTK Stemming     Linear Discriminant Analysis   \n",
       "1   Bag Of Words with NLTK Stemming      Stochastic Gradient Descent   \n",
       "2   Bag Of Words with NLTK Stemming                         XG Boost   \n",
       "3   Bag Of Words with NLTK Stemming           Support Vector Machine   \n",
       "4   Bag Of Words with NLTK Stemming              Logistic Regression   \n",
       "5          TFIDF with NLTK Stemming  Binomial Naive Bayes Classifier   \n",
       "6          TFIDF with NLTK Stemming              Logistic Regression   \n",
       "7          TFIDF with NLTK Stemming                   Decission Tree   \n",
       "8          TFIDF with NLTK Stemming                    Random Forest   \n",
       "9          TFIDF with NLTK Stemming                         XG Boost   \n",
       "10         TFIDF with NLTK Stemming      Stochastic Gradient Descent   \n",
       "11         TFIDF with NLTK Stemming                  Gausian Process   \n",
       "12         TFIDF with NLTK Stemming               K Nearst Neighbour   \n",
       "13         TFIDF with NLTK Stemming     Linear Discriminant Analysis   \n",
       "14         TFIDF with NLTK Stemming           Support Vector Machine   \n",
       "0          Ngram with NLTK Stemming  Binomial Naive Bayes Classifier   \n",
       "\n",
       "                         F1 Scores Range of F1 Scores  \\\n",
       "0    [0.98, 0.99, 0.99, 0.99, 1.0]           0.98-1.0   \n",
       "1     [0.96, 0.98, 0.99, 1.0, 1.0]           0.96-1.0   \n",
       "2    [0.95, 0.97, 0.99, 0.99, 1.0]           0.95-1.0   \n",
       "3    [0.95, 0.97, 0.99, 0.99, 1.0]           0.95-1.0   \n",
       "4     [0.95, 0.98, 0.99, 1.0, 1.0]           0.95-1.0   \n",
       "5    [0.92, 0.98, 0.99, 0.99, 1.0]           0.92-1.0   \n",
       "6      [0.95, 0.97, 1.0, 1.0, 1.0]           0.95-1.0   \n",
       "7    [0.93, 0.95, 0.98, 0.98, 1.0]           0.93-1.0   \n",
       "8    [0.95, 0.98, 0.99, 0.99, 1.0]           0.95-1.0   \n",
       "9    [0.96, 0.97, 0.97, 0.99, 1.0]           0.96-1.0   \n",
       "10   [0.97, 0.97, 0.99, 0.99, 1.0]           0.97-1.0   \n",
       "11   [0.94, 0.97, 0.99, 0.99, 1.0]           0.94-1.0   \n",
       "12   [0.94, 0.96, 0.96, 0.97, 1.0]           0.94-1.0   \n",
       "13   [0.93, 0.94, 0.96, 0.96, 1.0]           0.93-1.0   \n",
       "14  [0.41, 0.51, 0.58, 0.64, 0.66]          0.41-0.66   \n",
       "0    [0.93, 0.97, 0.99, 0.99, 1.0]           0.93-1.0   \n",
       "\n",
       "    Std Deviation of F1 Scores  \n",
       "0                     0.007071  \n",
       "1                     0.016733  \n",
       "2                     0.020000  \n",
       "3                     0.020000  \n",
       "4                     0.020736  \n",
       "5                     0.032094  \n",
       "6                     0.023022  \n",
       "7                     0.027749  \n",
       "8                     0.019235  \n",
       "9                     0.016432  \n",
       "10                    0.013416  \n",
       "11                    0.023875  \n",
       "12                    0.021909  \n",
       "13                    0.026833  \n",
       "14                    0.102225  \n",
       "0                     0.027928  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_obj = nb_clf\n",
    "model_name = \"Binomial Naive Bayes Classifier\"\n",
    "process = \"Ngram with NLTK Stemming\"\n",
    "n_splits = 5\n",
    "X = train_ds_features.toarray()\n",
    "y = train_ds.sentiment\n",
    "stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n",
    "df_model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(train_X.toarray(), train_y)\n",
    "test_ds_predicted = logreg.predict( test_X.toarray() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99       873\n",
      "           1       0.99      0.99      0.99      1203\n",
      "\n",
      "    accuracy                           0.99      2076\n",
      "   macro avg       0.99      0.99      0.99      2076\n",
      "weighted avg       0.99      0.99      0.99      2076\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print( metrics.classification_report( test_y, test_ds_predicted ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Process</th>\n",
       "      <th>Model Name</th>\n",
       "      <th>F1 Scores</th>\n",
       "      <th>Range of F1 Scores</th>\n",
       "      <th>Std Deviation of F1 Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>[0.98, 0.99, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.98-1.0</td>\n",
       "      <td>0.007071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Stochastic Gradient Descent</td>\n",
       "      <td>[0.96, 0.98, 0.99, 1.0, 1.0]</td>\n",
       "      <td>0.96-1.0</td>\n",
       "      <td>0.016733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>XG Boost</td>\n",
       "      <td>[0.95, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>[0.95, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>[0.95, 0.98, 0.99, 1.0, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.020736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Binomial Naive Bayes Classifier</td>\n",
       "      <td>[0.92, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.032094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>[0.95, 0.97, 1.0, 1.0, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.023022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Decission Tree</td>\n",
       "      <td>[0.93, 0.95, 0.98, 0.98, 1.0]</td>\n",
       "      <td>0.93-1.0</td>\n",
       "      <td>0.027749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>[0.95, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.019235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>XG Boost</td>\n",
       "      <td>[0.96, 0.97, 0.97, 0.99, 1.0]</td>\n",
       "      <td>0.96-1.0</td>\n",
       "      <td>0.016432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Stochastic Gradient Descent</td>\n",
       "      <td>[0.97, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.97-1.0</td>\n",
       "      <td>0.013416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Gausian Process</td>\n",
       "      <td>[0.94, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.94-1.0</td>\n",
       "      <td>0.023875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>K Nearst Neighbour</td>\n",
       "      <td>[0.94, 0.96, 0.96, 0.97, 1.0]</td>\n",
       "      <td>0.94-1.0</td>\n",
       "      <td>0.021909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>[0.93, 0.94, 0.96, 0.96, 1.0]</td>\n",
       "      <td>0.93-1.0</td>\n",
       "      <td>0.026833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>[0.41, 0.51, 0.58, 0.64, 0.66]</td>\n",
       "      <td>0.41-0.66</td>\n",
       "      <td>0.102225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ngram with NLTK Stemming</td>\n",
       "      <td>Binomial Naive Bayes Classifier</td>\n",
       "      <td>[0.93, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.93-1.0</td>\n",
       "      <td>0.027928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ngram with NLTK Stemming</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>[0.93, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.93-1.0</td>\n",
       "      <td>0.027749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Process                       Model Name  \\\n",
       "0   Bag Of Words with NLTK Stemming     Linear Discriminant Analysis   \n",
       "1   Bag Of Words with NLTK Stemming      Stochastic Gradient Descent   \n",
       "2   Bag Of Words with NLTK Stemming                         XG Boost   \n",
       "3   Bag Of Words with NLTK Stemming           Support Vector Machine   \n",
       "4   Bag Of Words with NLTK Stemming              Logistic Regression   \n",
       "5          TFIDF with NLTK Stemming  Binomial Naive Bayes Classifier   \n",
       "6          TFIDF with NLTK Stemming              Logistic Regression   \n",
       "7          TFIDF with NLTK Stemming                   Decission Tree   \n",
       "8          TFIDF with NLTK Stemming                    Random Forest   \n",
       "9          TFIDF with NLTK Stemming                         XG Boost   \n",
       "10         TFIDF with NLTK Stemming      Stochastic Gradient Descent   \n",
       "11         TFIDF with NLTK Stemming                  Gausian Process   \n",
       "12         TFIDF with NLTK Stemming               K Nearst Neighbour   \n",
       "13         TFIDF with NLTK Stemming     Linear Discriminant Analysis   \n",
       "14         TFIDF with NLTK Stemming           Support Vector Machine   \n",
       "0          Ngram with NLTK Stemming  Binomial Naive Bayes Classifier   \n",
       "0          Ngram with NLTK Stemming              Logistic Regression   \n",
       "\n",
       "                         F1 Scores Range of F1 Scores  \\\n",
       "0    [0.98, 0.99, 0.99, 0.99, 1.0]           0.98-1.0   \n",
       "1     [0.96, 0.98, 0.99, 1.0, 1.0]           0.96-1.0   \n",
       "2    [0.95, 0.97, 0.99, 0.99, 1.0]           0.95-1.0   \n",
       "3    [0.95, 0.97, 0.99, 0.99, 1.0]           0.95-1.0   \n",
       "4     [0.95, 0.98, 0.99, 1.0, 1.0]           0.95-1.0   \n",
       "5    [0.92, 0.98, 0.99, 0.99, 1.0]           0.92-1.0   \n",
       "6      [0.95, 0.97, 1.0, 1.0, 1.0]           0.95-1.0   \n",
       "7    [0.93, 0.95, 0.98, 0.98, 1.0]           0.93-1.0   \n",
       "8    [0.95, 0.98, 0.99, 0.99, 1.0]           0.95-1.0   \n",
       "9    [0.96, 0.97, 0.97, 0.99, 1.0]           0.96-1.0   \n",
       "10   [0.97, 0.97, 0.99, 0.99, 1.0]           0.97-1.0   \n",
       "11   [0.94, 0.97, 0.99, 0.99, 1.0]           0.94-1.0   \n",
       "12   [0.94, 0.96, 0.96, 0.97, 1.0]           0.94-1.0   \n",
       "13   [0.93, 0.94, 0.96, 0.96, 1.0]           0.93-1.0   \n",
       "14  [0.41, 0.51, 0.58, 0.64, 0.66]          0.41-0.66   \n",
       "0    [0.93, 0.97, 0.99, 0.99, 1.0]           0.93-1.0   \n",
       "0    [0.93, 0.98, 0.99, 0.99, 1.0]           0.93-1.0   \n",
       "\n",
       "    Std Deviation of F1 Scores  \n",
       "0                     0.007071  \n",
       "1                     0.016733  \n",
       "2                     0.020000  \n",
       "3                     0.020000  \n",
       "4                     0.020736  \n",
       "5                     0.032094  \n",
       "6                     0.023022  \n",
       "7                     0.027749  \n",
       "8                     0.019235  \n",
       "9                     0.016432  \n",
       "10                    0.013416  \n",
       "11                    0.023875  \n",
       "12                    0.021909  \n",
       "13                    0.026833  \n",
       "14                    0.102225  \n",
       "0                     0.027928  \n",
       "0                     0.027749  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_obj = logreg\n",
    "model_name = \"Logistic Regression\"\n",
    "process = \"Ngram with NLTK Stemming\"\n",
    "n_splits = 5\n",
    "X = train_ds_features.toarray()\n",
    "y = train_ds.sentiment\n",
    "stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n",
    "df_model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "decision_tree = DecisionTreeClassifier(criterion='entropy')\n",
    "\n",
    "decision_tree.fit(train_X.toarray(), train_y)\n",
    "test_ds_predicted = decision_tree.predict( test_X.toarray() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98       873\n",
      "           1       0.99      0.98      0.98      1203\n",
      "\n",
      "    accuracy                           0.98      2076\n",
      "   macro avg       0.98      0.98      0.98      2076\n",
      "weighted avg       0.98      0.98      0.98      2076\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print( metrics.classification_report( test_y, test_ds_predicted ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Process</th>\n",
       "      <th>Model Name</th>\n",
       "      <th>F1 Scores</th>\n",
       "      <th>Range of F1 Scores</th>\n",
       "      <th>Std Deviation of F1 Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>[0.98, 0.99, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.98-1.0</td>\n",
       "      <td>0.007071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Stochastic Gradient Descent</td>\n",
       "      <td>[0.96, 0.98, 0.99, 1.0, 1.0]</td>\n",
       "      <td>0.96-1.0</td>\n",
       "      <td>0.016733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>XG Boost</td>\n",
       "      <td>[0.95, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>[0.95, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>[0.95, 0.98, 0.99, 1.0, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.020736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Binomial Naive Bayes Classifier</td>\n",
       "      <td>[0.92, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.032094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>[0.95, 0.97, 1.0, 1.0, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.023022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Decission Tree</td>\n",
       "      <td>[0.93, 0.95, 0.98, 0.98, 1.0]</td>\n",
       "      <td>0.93-1.0</td>\n",
       "      <td>0.027749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>[0.95, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.019235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>XG Boost</td>\n",
       "      <td>[0.96, 0.97, 0.97, 0.99, 1.0]</td>\n",
       "      <td>0.96-1.0</td>\n",
       "      <td>0.016432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Stochastic Gradient Descent</td>\n",
       "      <td>[0.97, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.97-1.0</td>\n",
       "      <td>0.013416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Gausian Process</td>\n",
       "      <td>[0.94, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.94-1.0</td>\n",
       "      <td>0.023875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>K Nearst Neighbour</td>\n",
       "      <td>[0.94, 0.96, 0.96, 0.97, 1.0]</td>\n",
       "      <td>0.94-1.0</td>\n",
       "      <td>0.021909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>[0.93, 0.94, 0.96, 0.96, 1.0]</td>\n",
       "      <td>0.93-1.0</td>\n",
       "      <td>0.026833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>[0.41, 0.51, 0.58, 0.64, 0.66]</td>\n",
       "      <td>0.41-0.66</td>\n",
       "      <td>0.102225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ngram with NLTK Stemming</td>\n",
       "      <td>Binomial Naive Bayes Classifier</td>\n",
       "      <td>[0.93, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.93-1.0</td>\n",
       "      <td>0.027928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ngram with NLTK Stemming</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>[0.93, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.93-1.0</td>\n",
       "      <td>0.027749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ngram with NLTK Stemming</td>\n",
       "      <td>Decission Tree</td>\n",
       "      <td>[0.92, 0.92, 0.95, 0.98, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.035777</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Process                       Model Name  \\\n",
       "0   Bag Of Words with NLTK Stemming     Linear Discriminant Analysis   \n",
       "1   Bag Of Words with NLTK Stemming      Stochastic Gradient Descent   \n",
       "2   Bag Of Words with NLTK Stemming                         XG Boost   \n",
       "3   Bag Of Words with NLTK Stemming           Support Vector Machine   \n",
       "4   Bag Of Words with NLTK Stemming              Logistic Regression   \n",
       "5          TFIDF with NLTK Stemming  Binomial Naive Bayes Classifier   \n",
       "6          TFIDF with NLTK Stemming              Logistic Regression   \n",
       "7          TFIDF with NLTK Stemming                   Decission Tree   \n",
       "8          TFIDF with NLTK Stemming                    Random Forest   \n",
       "9          TFIDF with NLTK Stemming                         XG Boost   \n",
       "10         TFIDF with NLTK Stemming      Stochastic Gradient Descent   \n",
       "11         TFIDF with NLTK Stemming                  Gausian Process   \n",
       "12         TFIDF with NLTK Stemming               K Nearst Neighbour   \n",
       "13         TFIDF with NLTK Stemming     Linear Discriminant Analysis   \n",
       "14         TFIDF with NLTK Stemming           Support Vector Machine   \n",
       "0          Ngram with NLTK Stemming  Binomial Naive Bayes Classifier   \n",
       "0          Ngram with NLTK Stemming              Logistic Regression   \n",
       "0          Ngram with NLTK Stemming                   Decission Tree   \n",
       "\n",
       "                         F1 Scores Range of F1 Scores  \\\n",
       "0    [0.98, 0.99, 0.99, 0.99, 1.0]           0.98-1.0   \n",
       "1     [0.96, 0.98, 0.99, 1.0, 1.0]           0.96-1.0   \n",
       "2    [0.95, 0.97, 0.99, 0.99, 1.0]           0.95-1.0   \n",
       "3    [0.95, 0.97, 0.99, 0.99, 1.0]           0.95-1.0   \n",
       "4     [0.95, 0.98, 0.99, 1.0, 1.0]           0.95-1.0   \n",
       "5    [0.92, 0.98, 0.99, 0.99, 1.0]           0.92-1.0   \n",
       "6      [0.95, 0.97, 1.0, 1.0, 1.0]           0.95-1.0   \n",
       "7    [0.93, 0.95, 0.98, 0.98, 1.0]           0.93-1.0   \n",
       "8    [0.95, 0.98, 0.99, 0.99, 1.0]           0.95-1.0   \n",
       "9    [0.96, 0.97, 0.97, 0.99, 1.0]           0.96-1.0   \n",
       "10   [0.97, 0.97, 0.99, 0.99, 1.0]           0.97-1.0   \n",
       "11   [0.94, 0.97, 0.99, 0.99, 1.0]           0.94-1.0   \n",
       "12   [0.94, 0.96, 0.96, 0.97, 1.0]           0.94-1.0   \n",
       "13   [0.93, 0.94, 0.96, 0.96, 1.0]           0.93-1.0   \n",
       "14  [0.41, 0.51, 0.58, 0.64, 0.66]          0.41-0.66   \n",
       "0    [0.93, 0.97, 0.99, 0.99, 1.0]           0.93-1.0   \n",
       "0    [0.93, 0.98, 0.99, 0.99, 1.0]           0.93-1.0   \n",
       "0    [0.92, 0.92, 0.95, 0.98, 1.0]           0.92-1.0   \n",
       "\n",
       "    Std Deviation of F1 Scores  \n",
       "0                     0.007071  \n",
       "1                     0.016733  \n",
       "2                     0.020000  \n",
       "3                     0.020000  \n",
       "4                     0.020736  \n",
       "5                     0.032094  \n",
       "6                     0.023022  \n",
       "7                     0.027749  \n",
       "8                     0.019235  \n",
       "9                     0.016432  \n",
       "10                    0.013416  \n",
       "11                    0.023875  \n",
       "12                    0.021909  \n",
       "13                    0.026833  \n",
       "14                    0.102225  \n",
       "0                     0.027928  \n",
       "0                     0.027749  \n",
       "0                     0.035777  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_obj = decision_tree\n",
    "model_name = \"Decission Tree\"\n",
    "process = \"Ngram with NLTK Stemming\"\n",
    "n_splits = 5\n",
    "X = train_ds_features.toarray()\n",
    "y = train_ds.sentiment\n",
    "stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n",
    "df_model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "random_forest = RandomForestClassifier(n_estimators=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest.fit(train_X.toarray(), train_y)\n",
    "test_ds_predicted = random_forest.predict( test_X.toarray() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98       873\n",
      "           1       0.98      0.98      0.98      1203\n",
      "\n",
      "    accuracy                           0.98      2076\n",
      "   macro avg       0.98      0.98      0.98      2076\n",
      "weighted avg       0.98      0.98      0.98      2076\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print( metrics.classification_report( test_y, test_ds_predicted ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Process</th>\n",
       "      <th>Model Name</th>\n",
       "      <th>F1 Scores</th>\n",
       "      <th>Range of F1 Scores</th>\n",
       "      <th>Std Deviation of F1 Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>[0.98, 0.99, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.98-1.0</td>\n",
       "      <td>0.007071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Stochastic Gradient Descent</td>\n",
       "      <td>[0.96, 0.98, 0.99, 1.0, 1.0]</td>\n",
       "      <td>0.96-1.0</td>\n",
       "      <td>0.016733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>XG Boost</td>\n",
       "      <td>[0.95, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>[0.95, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>[0.95, 0.98, 0.99, 1.0, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.020736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Binomial Naive Bayes Classifier</td>\n",
       "      <td>[0.92, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.032094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>[0.95, 0.97, 1.0, 1.0, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.023022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Decission Tree</td>\n",
       "      <td>[0.93, 0.95, 0.98, 0.98, 1.0]</td>\n",
       "      <td>0.93-1.0</td>\n",
       "      <td>0.027749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>[0.95, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.019235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>XG Boost</td>\n",
       "      <td>[0.96, 0.97, 0.97, 0.99, 1.0]</td>\n",
       "      <td>0.96-1.0</td>\n",
       "      <td>0.016432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Stochastic Gradient Descent</td>\n",
       "      <td>[0.97, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.97-1.0</td>\n",
       "      <td>0.013416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Gausian Process</td>\n",
       "      <td>[0.94, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.94-1.0</td>\n",
       "      <td>0.023875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>K Nearst Neighbour</td>\n",
       "      <td>[0.94, 0.96, 0.96, 0.97, 1.0]</td>\n",
       "      <td>0.94-1.0</td>\n",
       "      <td>0.021909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>[0.93, 0.94, 0.96, 0.96, 1.0]</td>\n",
       "      <td>0.93-1.0</td>\n",
       "      <td>0.026833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>[0.41, 0.51, 0.58, 0.64, 0.66]</td>\n",
       "      <td>0.41-0.66</td>\n",
       "      <td>0.102225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ngram with NLTK Stemming</td>\n",
       "      <td>Binomial Naive Bayes Classifier</td>\n",
       "      <td>[0.93, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.93-1.0</td>\n",
       "      <td>0.027928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ngram with NLTK Stemming</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>[0.93, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.93-1.0</td>\n",
       "      <td>0.027749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ngram with NLTK Stemming</td>\n",
       "      <td>Decission Tree</td>\n",
       "      <td>[0.92, 0.92, 0.95, 0.98, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.035777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ngram with NLTK Stemming</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>[0.9, 0.93, 0.93, 0.95, 1.0]</td>\n",
       "      <td>0.9-1.0</td>\n",
       "      <td>0.037014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Process                       Model Name  \\\n",
       "0   Bag Of Words with NLTK Stemming     Linear Discriminant Analysis   \n",
       "1   Bag Of Words with NLTK Stemming      Stochastic Gradient Descent   \n",
       "2   Bag Of Words with NLTK Stemming                         XG Boost   \n",
       "3   Bag Of Words with NLTK Stemming           Support Vector Machine   \n",
       "4   Bag Of Words with NLTK Stemming              Logistic Regression   \n",
       "5          TFIDF with NLTK Stemming  Binomial Naive Bayes Classifier   \n",
       "6          TFIDF with NLTK Stemming              Logistic Regression   \n",
       "7          TFIDF with NLTK Stemming                   Decission Tree   \n",
       "8          TFIDF with NLTK Stemming                    Random Forest   \n",
       "9          TFIDF with NLTK Stemming                         XG Boost   \n",
       "10         TFIDF with NLTK Stemming      Stochastic Gradient Descent   \n",
       "11         TFIDF with NLTK Stemming                  Gausian Process   \n",
       "12         TFIDF with NLTK Stemming               K Nearst Neighbour   \n",
       "13         TFIDF with NLTK Stemming     Linear Discriminant Analysis   \n",
       "14         TFIDF with NLTK Stemming           Support Vector Machine   \n",
       "0          Ngram with NLTK Stemming  Binomial Naive Bayes Classifier   \n",
       "0          Ngram with NLTK Stemming              Logistic Regression   \n",
       "0          Ngram with NLTK Stemming                   Decission Tree   \n",
       "0          Ngram with NLTK Stemming                    Random Forest   \n",
       "\n",
       "                         F1 Scores Range of F1 Scores  \\\n",
       "0    [0.98, 0.99, 0.99, 0.99, 1.0]           0.98-1.0   \n",
       "1     [0.96, 0.98, 0.99, 1.0, 1.0]           0.96-1.0   \n",
       "2    [0.95, 0.97, 0.99, 0.99, 1.0]           0.95-1.0   \n",
       "3    [0.95, 0.97, 0.99, 0.99, 1.0]           0.95-1.0   \n",
       "4     [0.95, 0.98, 0.99, 1.0, 1.0]           0.95-1.0   \n",
       "5    [0.92, 0.98, 0.99, 0.99, 1.0]           0.92-1.0   \n",
       "6      [0.95, 0.97, 1.0, 1.0, 1.0]           0.95-1.0   \n",
       "7    [0.93, 0.95, 0.98, 0.98, 1.0]           0.93-1.0   \n",
       "8    [0.95, 0.98, 0.99, 0.99, 1.0]           0.95-1.0   \n",
       "9    [0.96, 0.97, 0.97, 0.99, 1.0]           0.96-1.0   \n",
       "10   [0.97, 0.97, 0.99, 0.99, 1.0]           0.97-1.0   \n",
       "11   [0.94, 0.97, 0.99, 0.99, 1.0]           0.94-1.0   \n",
       "12   [0.94, 0.96, 0.96, 0.97, 1.0]           0.94-1.0   \n",
       "13   [0.93, 0.94, 0.96, 0.96, 1.0]           0.93-1.0   \n",
       "14  [0.41, 0.51, 0.58, 0.64, 0.66]          0.41-0.66   \n",
       "0    [0.93, 0.97, 0.99, 0.99, 1.0]           0.93-1.0   \n",
       "0    [0.93, 0.98, 0.99, 0.99, 1.0]           0.93-1.0   \n",
       "0    [0.92, 0.92, 0.95, 0.98, 1.0]           0.92-1.0   \n",
       "0     [0.9, 0.93, 0.93, 0.95, 1.0]            0.9-1.0   \n",
       "\n",
       "    Std Deviation of F1 Scores  \n",
       "0                     0.007071  \n",
       "1                     0.016733  \n",
       "2                     0.020000  \n",
       "3                     0.020000  \n",
       "4                     0.020736  \n",
       "5                     0.032094  \n",
       "6                     0.023022  \n",
       "7                     0.027749  \n",
       "8                     0.019235  \n",
       "9                     0.016432  \n",
       "10                    0.013416  \n",
       "11                    0.023875  \n",
       "12                    0.021909  \n",
       "13                    0.026833  \n",
       "14                    0.102225  \n",
       "0                     0.027928  \n",
       "0                     0.027749  \n",
       "0                     0.035777  \n",
       "0                     0.037014  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_obj = random_forest\n",
    "model_name = \"Random Forest\"\n",
    "process = \"Ngram with NLTK Stemming\"\n",
    "n_splits = 5\n",
    "X = train_ds_features.toarray()\n",
    "y = train_ds.sentiment\n",
    "stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n",
    "df_model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgboost = XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost.fit(train_X.toarray(), train_y)\n",
    "test_ds_predicted = xgboost.predict( test_X.toarray() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       873\n",
      "           1       0.99      0.99      0.99      1203\n",
      "\n",
      "    accuracy                           0.99      2076\n",
      "   macro avg       0.99      0.99      0.99      2076\n",
      "weighted avg       0.99      0.99      0.99      2076\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print( metrics.classification_report( test_y, test_ds_predicted ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Process</th>\n",
       "      <th>Model Name</th>\n",
       "      <th>F1 Scores</th>\n",
       "      <th>Range of F1 Scores</th>\n",
       "      <th>Std Deviation of F1 Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>[0.98, 0.99, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.98-1.0</td>\n",
       "      <td>0.007071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Stochastic Gradient Descent</td>\n",
       "      <td>[0.96, 0.98, 0.99, 1.0, 1.0]</td>\n",
       "      <td>0.96-1.0</td>\n",
       "      <td>0.016733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>XG Boost</td>\n",
       "      <td>[0.95, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>[0.95, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>[0.95, 0.98, 0.99, 1.0, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.020736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Binomial Naive Bayes Classifier</td>\n",
       "      <td>[0.92, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.032094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>[0.95, 0.97, 1.0, 1.0, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.023022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Decission Tree</td>\n",
       "      <td>[0.93, 0.95, 0.98, 0.98, 1.0]</td>\n",
       "      <td>0.93-1.0</td>\n",
       "      <td>0.027749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>[0.95, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.019235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>XG Boost</td>\n",
       "      <td>[0.96, 0.97, 0.97, 0.99, 1.0]</td>\n",
       "      <td>0.96-1.0</td>\n",
       "      <td>0.016432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Stochastic Gradient Descent</td>\n",
       "      <td>[0.97, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.97-1.0</td>\n",
       "      <td>0.013416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Gausian Process</td>\n",
       "      <td>[0.94, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.94-1.0</td>\n",
       "      <td>0.023875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>K Nearst Neighbour</td>\n",
       "      <td>[0.94, 0.96, 0.96, 0.97, 1.0]</td>\n",
       "      <td>0.94-1.0</td>\n",
       "      <td>0.021909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>[0.93, 0.94, 0.96, 0.96, 1.0]</td>\n",
       "      <td>0.93-1.0</td>\n",
       "      <td>0.026833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>[0.41, 0.51, 0.58, 0.64, 0.66]</td>\n",
       "      <td>0.41-0.66</td>\n",
       "      <td>0.102225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ngram with NLTK Stemming</td>\n",
       "      <td>Binomial Naive Bayes Classifier</td>\n",
       "      <td>[0.93, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.93-1.0</td>\n",
       "      <td>0.027928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ngram with NLTK Stemming</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>[0.93, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.93-1.0</td>\n",
       "      <td>0.027749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ngram with NLTK Stemming</td>\n",
       "      <td>Decission Tree</td>\n",
       "      <td>[0.92, 0.92, 0.95, 0.98, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.035777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ngram with NLTK Stemming</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>[0.9, 0.93, 0.93, 0.95, 1.0]</td>\n",
       "      <td>0.9-1.0</td>\n",
       "      <td>0.037014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ngram with NLTK Stemming</td>\n",
       "      <td>XG Boost</td>\n",
       "      <td>[0.94, 0.95, 0.96, 0.98, 1.0]</td>\n",
       "      <td>0.94-1.0</td>\n",
       "      <td>0.024083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Process                       Model Name  \\\n",
       "0   Bag Of Words with NLTK Stemming     Linear Discriminant Analysis   \n",
       "1   Bag Of Words with NLTK Stemming      Stochastic Gradient Descent   \n",
       "2   Bag Of Words with NLTK Stemming                         XG Boost   \n",
       "3   Bag Of Words with NLTK Stemming           Support Vector Machine   \n",
       "4   Bag Of Words with NLTK Stemming              Logistic Regression   \n",
       "5          TFIDF with NLTK Stemming  Binomial Naive Bayes Classifier   \n",
       "6          TFIDF with NLTK Stemming              Logistic Regression   \n",
       "7          TFIDF with NLTK Stemming                   Decission Tree   \n",
       "8          TFIDF with NLTK Stemming                    Random Forest   \n",
       "9          TFIDF with NLTK Stemming                         XG Boost   \n",
       "10         TFIDF with NLTK Stemming      Stochastic Gradient Descent   \n",
       "11         TFIDF with NLTK Stemming                  Gausian Process   \n",
       "12         TFIDF with NLTK Stemming               K Nearst Neighbour   \n",
       "13         TFIDF with NLTK Stemming     Linear Discriminant Analysis   \n",
       "14         TFIDF with NLTK Stemming           Support Vector Machine   \n",
       "0          Ngram with NLTK Stemming  Binomial Naive Bayes Classifier   \n",
       "0          Ngram with NLTK Stemming              Logistic Regression   \n",
       "0          Ngram with NLTK Stemming                   Decission Tree   \n",
       "0          Ngram with NLTK Stemming                    Random Forest   \n",
       "0          Ngram with NLTK Stemming                         XG Boost   \n",
       "\n",
       "                         F1 Scores Range of F1 Scores  \\\n",
       "0    [0.98, 0.99, 0.99, 0.99, 1.0]           0.98-1.0   \n",
       "1     [0.96, 0.98, 0.99, 1.0, 1.0]           0.96-1.0   \n",
       "2    [0.95, 0.97, 0.99, 0.99, 1.0]           0.95-1.0   \n",
       "3    [0.95, 0.97, 0.99, 0.99, 1.0]           0.95-1.0   \n",
       "4     [0.95, 0.98, 0.99, 1.0, 1.0]           0.95-1.0   \n",
       "5    [0.92, 0.98, 0.99, 0.99, 1.0]           0.92-1.0   \n",
       "6      [0.95, 0.97, 1.0, 1.0, 1.0]           0.95-1.0   \n",
       "7    [0.93, 0.95, 0.98, 0.98, 1.0]           0.93-1.0   \n",
       "8    [0.95, 0.98, 0.99, 0.99, 1.0]           0.95-1.0   \n",
       "9    [0.96, 0.97, 0.97, 0.99, 1.0]           0.96-1.0   \n",
       "10   [0.97, 0.97, 0.99, 0.99, 1.0]           0.97-1.0   \n",
       "11   [0.94, 0.97, 0.99, 0.99, 1.0]           0.94-1.0   \n",
       "12   [0.94, 0.96, 0.96, 0.97, 1.0]           0.94-1.0   \n",
       "13   [0.93, 0.94, 0.96, 0.96, 1.0]           0.93-1.0   \n",
       "14  [0.41, 0.51, 0.58, 0.64, 0.66]          0.41-0.66   \n",
       "0    [0.93, 0.97, 0.99, 0.99, 1.0]           0.93-1.0   \n",
       "0    [0.93, 0.98, 0.99, 0.99, 1.0]           0.93-1.0   \n",
       "0    [0.92, 0.92, 0.95, 0.98, 1.0]           0.92-1.0   \n",
       "0     [0.9, 0.93, 0.93, 0.95, 1.0]            0.9-1.0   \n",
       "0    [0.94, 0.95, 0.96, 0.98, 1.0]           0.94-1.0   \n",
       "\n",
       "    Std Deviation of F1 Scores  \n",
       "0                     0.007071  \n",
       "1                     0.016733  \n",
       "2                     0.020000  \n",
       "3                     0.020000  \n",
       "4                     0.020736  \n",
       "5                     0.032094  \n",
       "6                     0.023022  \n",
       "7                     0.027749  \n",
       "8                     0.019235  \n",
       "9                     0.016432  \n",
       "10                    0.013416  \n",
       "11                    0.023875  \n",
       "12                    0.021909  \n",
       "13                    0.026833  \n",
       "14                    0.102225  \n",
       "0                     0.027928  \n",
       "0                     0.027749  \n",
       "0                     0.035777  \n",
       "0                     0.037014  \n",
       "0                     0.024083  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_obj = xgboost\n",
    "model_name = \"XG Boost\"\n",
    "process = \"Ngram with NLTK Stemming\"\n",
    "n_splits = 5\n",
    "X = train_ds_features.toarray()\n",
    "y = train_ds.sentiment\n",
    "stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n",
    "df_model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "sgd = OneVsRestClassifier(SGDClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd.fit(train_X.toarray(), train_y)\n",
    "test_ds_predicted = sgd.predict( test_X.toarray() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       873\n",
      "           1       0.99      0.99      0.99      1203\n",
      "\n",
      "    accuracy                           0.99      2076\n",
      "   macro avg       0.99      0.99      0.99      2076\n",
      "weighted avg       0.99      0.99      0.99      2076\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print( metrics.classification_report( test_y, test_ds_predicted ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Process</th>\n",
       "      <th>Model Name</th>\n",
       "      <th>F1 Scores</th>\n",
       "      <th>Range of F1 Scores</th>\n",
       "      <th>Std Deviation of F1 Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>[0.98, 0.99, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.98-1.0</td>\n",
       "      <td>0.007071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Stochastic Gradient Descent</td>\n",
       "      <td>[0.96, 0.98, 0.99, 1.0, 1.0]</td>\n",
       "      <td>0.96-1.0</td>\n",
       "      <td>0.016733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>XG Boost</td>\n",
       "      <td>[0.95, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>[0.95, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>[0.95, 0.98, 0.99, 1.0, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.020736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Binomial Naive Bayes Classifier</td>\n",
       "      <td>[0.92, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.032094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>[0.95, 0.97, 1.0, 1.0, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.023022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Decission Tree</td>\n",
       "      <td>[0.93, 0.95, 0.98, 0.98, 1.0]</td>\n",
       "      <td>0.93-1.0</td>\n",
       "      <td>0.027749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>[0.95, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.019235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>XG Boost</td>\n",
       "      <td>[0.96, 0.97, 0.97, 0.99, 1.0]</td>\n",
       "      <td>0.96-1.0</td>\n",
       "      <td>0.016432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Stochastic Gradient Descent</td>\n",
       "      <td>[0.97, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.97-1.0</td>\n",
       "      <td>0.013416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Gausian Process</td>\n",
       "      <td>[0.94, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.94-1.0</td>\n",
       "      <td>0.023875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>K Nearst Neighbour</td>\n",
       "      <td>[0.94, 0.96, 0.96, 0.97, 1.0]</td>\n",
       "      <td>0.94-1.0</td>\n",
       "      <td>0.021909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>[0.93, 0.94, 0.96, 0.96, 1.0]</td>\n",
       "      <td>0.93-1.0</td>\n",
       "      <td>0.026833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>[0.41, 0.51, 0.58, 0.64, 0.66]</td>\n",
       "      <td>0.41-0.66</td>\n",
       "      <td>0.102225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ngram with NLTK Stemming</td>\n",
       "      <td>Binomial Naive Bayes Classifier</td>\n",
       "      <td>[0.93, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.93-1.0</td>\n",
       "      <td>0.027928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ngram with NLTK Stemming</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>[0.93, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.93-1.0</td>\n",
       "      <td>0.027749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ngram with NLTK Stemming</td>\n",
       "      <td>Decission Tree</td>\n",
       "      <td>[0.92, 0.92, 0.95, 0.98, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.035777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ngram with NLTK Stemming</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>[0.9, 0.93, 0.93, 0.95, 1.0]</td>\n",
       "      <td>0.9-1.0</td>\n",
       "      <td>0.037014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ngram with NLTK Stemming</td>\n",
       "      <td>XG Boost</td>\n",
       "      <td>[0.94, 0.95, 0.96, 0.98, 1.0]</td>\n",
       "      <td>0.94-1.0</td>\n",
       "      <td>0.024083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ngram with NLTK Stemming</td>\n",
       "      <td>Stochastic Gradient Descent</td>\n",
       "      <td>[0.96, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.96-1.0</td>\n",
       "      <td>0.016432</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Process                       Model Name  \\\n",
       "0   Bag Of Words with NLTK Stemming     Linear Discriminant Analysis   \n",
       "1   Bag Of Words with NLTK Stemming      Stochastic Gradient Descent   \n",
       "2   Bag Of Words with NLTK Stemming                         XG Boost   \n",
       "3   Bag Of Words with NLTK Stemming           Support Vector Machine   \n",
       "4   Bag Of Words with NLTK Stemming              Logistic Regression   \n",
       "5          TFIDF with NLTK Stemming  Binomial Naive Bayes Classifier   \n",
       "6          TFIDF with NLTK Stemming              Logistic Regression   \n",
       "7          TFIDF with NLTK Stemming                   Decission Tree   \n",
       "8          TFIDF with NLTK Stemming                    Random Forest   \n",
       "9          TFIDF with NLTK Stemming                         XG Boost   \n",
       "10         TFIDF with NLTK Stemming      Stochastic Gradient Descent   \n",
       "11         TFIDF with NLTK Stemming                  Gausian Process   \n",
       "12         TFIDF with NLTK Stemming               K Nearst Neighbour   \n",
       "13         TFIDF with NLTK Stemming     Linear Discriminant Analysis   \n",
       "14         TFIDF with NLTK Stemming           Support Vector Machine   \n",
       "0          Ngram with NLTK Stemming  Binomial Naive Bayes Classifier   \n",
       "0          Ngram with NLTK Stemming              Logistic Regression   \n",
       "0          Ngram with NLTK Stemming                   Decission Tree   \n",
       "0          Ngram with NLTK Stemming                    Random Forest   \n",
       "0          Ngram with NLTK Stemming                         XG Boost   \n",
       "0          Ngram with NLTK Stemming      Stochastic Gradient Descent   \n",
       "\n",
       "                         F1 Scores Range of F1 Scores  \\\n",
       "0    [0.98, 0.99, 0.99, 0.99, 1.0]           0.98-1.0   \n",
       "1     [0.96, 0.98, 0.99, 1.0, 1.0]           0.96-1.0   \n",
       "2    [0.95, 0.97, 0.99, 0.99, 1.0]           0.95-1.0   \n",
       "3    [0.95, 0.97, 0.99, 0.99, 1.0]           0.95-1.0   \n",
       "4     [0.95, 0.98, 0.99, 1.0, 1.0]           0.95-1.0   \n",
       "5    [0.92, 0.98, 0.99, 0.99, 1.0]           0.92-1.0   \n",
       "6      [0.95, 0.97, 1.0, 1.0, 1.0]           0.95-1.0   \n",
       "7    [0.93, 0.95, 0.98, 0.98, 1.0]           0.93-1.0   \n",
       "8    [0.95, 0.98, 0.99, 0.99, 1.0]           0.95-1.0   \n",
       "9    [0.96, 0.97, 0.97, 0.99, 1.0]           0.96-1.0   \n",
       "10   [0.97, 0.97, 0.99, 0.99, 1.0]           0.97-1.0   \n",
       "11   [0.94, 0.97, 0.99, 0.99, 1.0]           0.94-1.0   \n",
       "12   [0.94, 0.96, 0.96, 0.97, 1.0]           0.94-1.0   \n",
       "13   [0.93, 0.94, 0.96, 0.96, 1.0]           0.93-1.0   \n",
       "14  [0.41, 0.51, 0.58, 0.64, 0.66]          0.41-0.66   \n",
       "0    [0.93, 0.97, 0.99, 0.99, 1.0]           0.93-1.0   \n",
       "0    [0.93, 0.98, 0.99, 0.99, 1.0]           0.93-1.0   \n",
       "0    [0.92, 0.92, 0.95, 0.98, 1.0]           0.92-1.0   \n",
       "0     [0.9, 0.93, 0.93, 0.95, 1.0]            0.9-1.0   \n",
       "0    [0.94, 0.95, 0.96, 0.98, 1.0]           0.94-1.0   \n",
       "0    [0.96, 0.97, 0.99, 0.99, 1.0]           0.96-1.0   \n",
       "\n",
       "    Std Deviation of F1 Scores  \n",
       "0                     0.007071  \n",
       "1                     0.016733  \n",
       "2                     0.020000  \n",
       "3                     0.020000  \n",
       "4                     0.020736  \n",
       "5                     0.032094  \n",
       "6                     0.023022  \n",
       "7                     0.027749  \n",
       "8                     0.019235  \n",
       "9                     0.016432  \n",
       "10                    0.013416  \n",
       "11                    0.023875  \n",
       "12                    0.021909  \n",
       "13                    0.026833  \n",
       "14                    0.102225  \n",
       "0                     0.027928  \n",
       "0                     0.027749  \n",
       "0                     0.035777  \n",
       "0                     0.037014  \n",
       "0                     0.024083  \n",
       "0                     0.016432  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_obj = sgd\n",
    "model_name = \"Stochastic Gradient Descent\"\n",
    "process = \"Ngram with NLTK Stemming\"\n",
    "n_splits = 5\n",
    "X = train_ds_features.toarray()\n",
    "y = train_ds.sentiment\n",
    "stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n",
    "df_model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Process Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "gausian_process = GaussianProcessClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "gausian_process.fit(train_X.toarray(), train_y)\n",
    "test_ds_predicted = gausian_process.predict( test_X.toarray() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98       873\n",
      "           1       0.98      0.99      0.99      1203\n",
      "\n",
      "    accuracy                           0.99      2076\n",
      "   macro avg       0.99      0.99      0.99      2076\n",
      "weighted avg       0.99      0.99      0.99      2076\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print( metrics.classification_report( test_y, test_ds_predicted ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Process</th>\n",
       "      <th>Model Name</th>\n",
       "      <th>F1 Scores</th>\n",
       "      <th>Range of F1 Scores</th>\n",
       "      <th>Std Deviation of F1 Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>[0.98, 0.99, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.98-1.0</td>\n",
       "      <td>0.007071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Stochastic Gradient Descent</td>\n",
       "      <td>[0.96, 0.98, 0.99, 1.0, 1.0]</td>\n",
       "      <td>0.96-1.0</td>\n",
       "      <td>0.016733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>XG Boost</td>\n",
       "      <td>[0.95, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>[0.95, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>[0.95, 0.98, 0.99, 1.0, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.020736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Binomial Naive Bayes Classifier</td>\n",
       "      <td>[0.92, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.032094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>[0.95, 0.97, 1.0, 1.0, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.023022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Decission Tree</td>\n",
       "      <td>[0.93, 0.95, 0.98, 0.98, 1.0]</td>\n",
       "      <td>0.93-1.0</td>\n",
       "      <td>0.027749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>[0.95, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.019235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>XG Boost</td>\n",
       "      <td>[0.96, 0.97, 0.97, 0.99, 1.0]</td>\n",
       "      <td>0.96-1.0</td>\n",
       "      <td>0.016432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Stochastic Gradient Descent</td>\n",
       "      <td>[0.97, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.97-1.0</td>\n",
       "      <td>0.013416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Gausian Process</td>\n",
       "      <td>[0.94, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.94-1.0</td>\n",
       "      <td>0.023875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>K Nearst Neighbour</td>\n",
       "      <td>[0.94, 0.96, 0.96, 0.97, 1.0]</td>\n",
       "      <td>0.94-1.0</td>\n",
       "      <td>0.021909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>[0.93, 0.94, 0.96, 0.96, 1.0]</td>\n",
       "      <td>0.93-1.0</td>\n",
       "      <td>0.026833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>[0.41, 0.51, 0.58, 0.64, 0.66]</td>\n",
       "      <td>0.41-0.66</td>\n",
       "      <td>0.102225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ngram with NLTK Stemming</td>\n",
       "      <td>Binomial Naive Bayes Classifier</td>\n",
       "      <td>[0.93, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.93-1.0</td>\n",
       "      <td>0.027928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ngram with NLTK Stemming</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>[0.93, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.93-1.0</td>\n",
       "      <td>0.027749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ngram with NLTK Stemming</td>\n",
       "      <td>Decission Tree</td>\n",
       "      <td>[0.92, 0.92, 0.95, 0.98, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.035777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ngram with NLTK Stemming</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>[0.9, 0.93, 0.93, 0.95, 1.0]</td>\n",
       "      <td>0.9-1.0</td>\n",
       "      <td>0.037014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ngram with NLTK Stemming</td>\n",
       "      <td>XG Boost</td>\n",
       "      <td>[0.94, 0.95, 0.96, 0.98, 1.0]</td>\n",
       "      <td>0.94-1.0</td>\n",
       "      <td>0.024083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ngram with NLTK Stemming</td>\n",
       "      <td>Stochastic Gradient Descent</td>\n",
       "      <td>[0.96, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.96-1.0</td>\n",
       "      <td>0.016432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ngram with NLTK Stemming</td>\n",
       "      <td>Gausian Process</td>\n",
       "      <td>[0.93, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.93-1.0</td>\n",
       "      <td>0.027749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Process                       Model Name  \\\n",
       "0   Bag Of Words with NLTK Stemming     Linear Discriminant Analysis   \n",
       "1   Bag Of Words with NLTK Stemming      Stochastic Gradient Descent   \n",
       "2   Bag Of Words with NLTK Stemming                         XG Boost   \n",
       "3   Bag Of Words with NLTK Stemming           Support Vector Machine   \n",
       "4   Bag Of Words with NLTK Stemming              Logistic Regression   \n",
       "5          TFIDF with NLTK Stemming  Binomial Naive Bayes Classifier   \n",
       "6          TFIDF with NLTK Stemming              Logistic Regression   \n",
       "7          TFIDF with NLTK Stemming                   Decission Tree   \n",
       "8          TFIDF with NLTK Stemming                    Random Forest   \n",
       "9          TFIDF with NLTK Stemming                         XG Boost   \n",
       "10         TFIDF with NLTK Stemming      Stochastic Gradient Descent   \n",
       "11         TFIDF with NLTK Stemming                  Gausian Process   \n",
       "12         TFIDF with NLTK Stemming               K Nearst Neighbour   \n",
       "13         TFIDF with NLTK Stemming     Linear Discriminant Analysis   \n",
       "14         TFIDF with NLTK Stemming           Support Vector Machine   \n",
       "0          Ngram with NLTK Stemming  Binomial Naive Bayes Classifier   \n",
       "0          Ngram with NLTK Stemming              Logistic Regression   \n",
       "0          Ngram with NLTK Stemming                   Decission Tree   \n",
       "0          Ngram with NLTK Stemming                    Random Forest   \n",
       "0          Ngram with NLTK Stemming                         XG Boost   \n",
       "0          Ngram with NLTK Stemming      Stochastic Gradient Descent   \n",
       "0          Ngram with NLTK Stemming                  Gausian Process   \n",
       "\n",
       "                         F1 Scores Range of F1 Scores  \\\n",
       "0    [0.98, 0.99, 0.99, 0.99, 1.0]           0.98-1.0   \n",
       "1     [0.96, 0.98, 0.99, 1.0, 1.0]           0.96-1.0   \n",
       "2    [0.95, 0.97, 0.99, 0.99, 1.0]           0.95-1.0   \n",
       "3    [0.95, 0.97, 0.99, 0.99, 1.0]           0.95-1.0   \n",
       "4     [0.95, 0.98, 0.99, 1.0, 1.0]           0.95-1.0   \n",
       "5    [0.92, 0.98, 0.99, 0.99, 1.0]           0.92-1.0   \n",
       "6      [0.95, 0.97, 1.0, 1.0, 1.0]           0.95-1.0   \n",
       "7    [0.93, 0.95, 0.98, 0.98, 1.0]           0.93-1.0   \n",
       "8    [0.95, 0.98, 0.99, 0.99, 1.0]           0.95-1.0   \n",
       "9    [0.96, 0.97, 0.97, 0.99, 1.0]           0.96-1.0   \n",
       "10   [0.97, 0.97, 0.99, 0.99, 1.0]           0.97-1.0   \n",
       "11   [0.94, 0.97, 0.99, 0.99, 1.0]           0.94-1.0   \n",
       "12   [0.94, 0.96, 0.96, 0.97, 1.0]           0.94-1.0   \n",
       "13   [0.93, 0.94, 0.96, 0.96, 1.0]           0.93-1.0   \n",
       "14  [0.41, 0.51, 0.58, 0.64, 0.66]          0.41-0.66   \n",
       "0    [0.93, 0.97, 0.99, 0.99, 1.0]           0.93-1.0   \n",
       "0    [0.93, 0.98, 0.99, 0.99, 1.0]           0.93-1.0   \n",
       "0    [0.92, 0.92, 0.95, 0.98, 1.0]           0.92-1.0   \n",
       "0     [0.9, 0.93, 0.93, 0.95, 1.0]            0.9-1.0   \n",
       "0    [0.94, 0.95, 0.96, 0.98, 1.0]           0.94-1.0   \n",
       "0    [0.96, 0.97, 0.99, 0.99, 1.0]           0.96-1.0   \n",
       "0    [0.93, 0.98, 0.99, 0.99, 1.0]           0.93-1.0   \n",
       "\n",
       "    Std Deviation of F1 Scores  \n",
       "0                     0.007071  \n",
       "1                     0.016733  \n",
       "2                     0.020000  \n",
       "3                     0.020000  \n",
       "4                     0.020736  \n",
       "5                     0.032094  \n",
       "6                     0.023022  \n",
       "7                     0.027749  \n",
       "8                     0.019235  \n",
       "9                     0.016432  \n",
       "10                    0.013416  \n",
       "11                    0.023875  \n",
       "12                    0.021909  \n",
       "13                    0.026833  \n",
       "14                    0.102225  \n",
       "0                     0.027928  \n",
       "0                     0.027749  \n",
       "0                     0.035777  \n",
       "0                     0.037014  \n",
       "0                     0.024083  \n",
       "0                     0.016432  \n",
       "0                     0.027749  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_obj = gausian_process\n",
    "model_name = \"Gausian Process\"\n",
    "process = \"Ngram with NLTK Stemming\"\n",
    "n_splits = 5\n",
    "X = train_ds_features.toarray()\n",
    "y = train_ds.sentiment\n",
    "stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n",
    "df_model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(train_X.toarray(), train_y)\n",
    "test_ds_predicted = knn.predict( test_X.toarray() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       873\n",
      "           1       0.99      0.98      0.98      1203\n",
      "\n",
      "    accuracy                           0.98      2076\n",
      "   macro avg       0.98      0.98      0.98      2076\n",
      "weighted avg       0.98      0.98      0.98      2076\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print( metrics.classification_report( test_y, test_ds_predicted ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Process</th>\n",
       "      <th>Model Name</th>\n",
       "      <th>F1 Scores</th>\n",
       "      <th>Range of F1 Scores</th>\n",
       "      <th>Std Deviation of F1 Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>[0.98, 0.99, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.98-1.0</td>\n",
       "      <td>0.007071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Stochastic Gradient Descent</td>\n",
       "      <td>[0.96, 0.98, 0.99, 1.0, 1.0]</td>\n",
       "      <td>0.96-1.0</td>\n",
       "      <td>0.016733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>XG Boost</td>\n",
       "      <td>[0.95, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>[0.95, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>[0.95, 0.98, 0.99, 1.0, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.020736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Binomial Naive Bayes Classifier</td>\n",
       "      <td>[0.92, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.032094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>[0.95, 0.97, 1.0, 1.0, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.023022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Decission Tree</td>\n",
       "      <td>[0.93, 0.95, 0.98, 0.98, 1.0]</td>\n",
       "      <td>0.93-1.0</td>\n",
       "      <td>0.027749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>[0.95, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.019235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>XG Boost</td>\n",
       "      <td>[0.96, 0.97, 0.97, 0.99, 1.0]</td>\n",
       "      <td>0.96-1.0</td>\n",
       "      <td>0.016432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Stochastic Gradient Descent</td>\n",
       "      <td>[0.97, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.97-1.0</td>\n",
       "      <td>0.013416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Gausian Process</td>\n",
       "      <td>[0.94, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.94-1.0</td>\n",
       "      <td>0.023875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>K Nearst Neighbour</td>\n",
       "      <td>[0.94, 0.96, 0.96, 0.97, 1.0]</td>\n",
       "      <td>0.94-1.0</td>\n",
       "      <td>0.021909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>[0.93, 0.94, 0.96, 0.96, 1.0]</td>\n",
       "      <td>0.93-1.0</td>\n",
       "      <td>0.026833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>[0.41, 0.51, 0.58, 0.64, 0.66]</td>\n",
       "      <td>0.41-0.66</td>\n",
       "      <td>0.102225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ngram with NLTK Stemming</td>\n",
       "      <td>Binomial Naive Bayes Classifier</td>\n",
       "      <td>[0.93, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.93-1.0</td>\n",
       "      <td>0.027928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ngram with NLTK Stemming</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>[0.93, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.93-1.0</td>\n",
       "      <td>0.027749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ngram with NLTK Stemming</td>\n",
       "      <td>Decission Tree</td>\n",
       "      <td>[0.92, 0.92, 0.95, 0.98, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.035777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ngram with NLTK Stemming</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>[0.9, 0.93, 0.93, 0.95, 1.0]</td>\n",
       "      <td>0.9-1.0</td>\n",
       "      <td>0.037014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ngram with NLTK Stemming</td>\n",
       "      <td>XG Boost</td>\n",
       "      <td>[0.94, 0.95, 0.96, 0.98, 1.0]</td>\n",
       "      <td>0.94-1.0</td>\n",
       "      <td>0.024083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ngram with NLTK Stemming</td>\n",
       "      <td>Stochastic Gradient Descent</td>\n",
       "      <td>[0.96, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.96-1.0</td>\n",
       "      <td>0.016432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ngram with NLTK Stemming</td>\n",
       "      <td>Gausian Process</td>\n",
       "      <td>[0.93, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.93-1.0</td>\n",
       "      <td>0.027749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ngram with NLTK Stemming</td>\n",
       "      <td>K Nearst Neighbour</td>\n",
       "      <td>[0.93, 0.93, 0.95, 0.99, 1.0]</td>\n",
       "      <td>0.93-1.0</td>\n",
       "      <td>0.033166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Process                       Model Name  \\\n",
       "0   Bag Of Words with NLTK Stemming     Linear Discriminant Analysis   \n",
       "1   Bag Of Words with NLTK Stemming      Stochastic Gradient Descent   \n",
       "2   Bag Of Words with NLTK Stemming                         XG Boost   \n",
       "3   Bag Of Words with NLTK Stemming           Support Vector Machine   \n",
       "4   Bag Of Words with NLTK Stemming              Logistic Regression   \n",
       "5          TFIDF with NLTK Stemming  Binomial Naive Bayes Classifier   \n",
       "6          TFIDF with NLTK Stemming              Logistic Regression   \n",
       "7          TFIDF with NLTK Stemming                   Decission Tree   \n",
       "8          TFIDF with NLTK Stemming                    Random Forest   \n",
       "9          TFIDF with NLTK Stemming                         XG Boost   \n",
       "10         TFIDF with NLTK Stemming      Stochastic Gradient Descent   \n",
       "11         TFIDF with NLTK Stemming                  Gausian Process   \n",
       "12         TFIDF with NLTK Stemming               K Nearst Neighbour   \n",
       "13         TFIDF with NLTK Stemming     Linear Discriminant Analysis   \n",
       "14         TFIDF with NLTK Stemming           Support Vector Machine   \n",
       "0          Ngram with NLTK Stemming  Binomial Naive Bayes Classifier   \n",
       "0          Ngram with NLTK Stemming              Logistic Regression   \n",
       "0          Ngram with NLTK Stemming                   Decission Tree   \n",
       "0          Ngram with NLTK Stemming                    Random Forest   \n",
       "0          Ngram with NLTK Stemming                         XG Boost   \n",
       "0          Ngram with NLTK Stemming      Stochastic Gradient Descent   \n",
       "0          Ngram with NLTK Stemming                  Gausian Process   \n",
       "0          Ngram with NLTK Stemming               K Nearst Neighbour   \n",
       "\n",
       "                         F1 Scores Range of F1 Scores  \\\n",
       "0    [0.98, 0.99, 0.99, 0.99, 1.0]           0.98-1.0   \n",
       "1     [0.96, 0.98, 0.99, 1.0, 1.0]           0.96-1.0   \n",
       "2    [0.95, 0.97, 0.99, 0.99, 1.0]           0.95-1.0   \n",
       "3    [0.95, 0.97, 0.99, 0.99, 1.0]           0.95-1.0   \n",
       "4     [0.95, 0.98, 0.99, 1.0, 1.0]           0.95-1.0   \n",
       "5    [0.92, 0.98, 0.99, 0.99, 1.0]           0.92-1.0   \n",
       "6      [0.95, 0.97, 1.0, 1.0, 1.0]           0.95-1.0   \n",
       "7    [0.93, 0.95, 0.98, 0.98, 1.0]           0.93-1.0   \n",
       "8    [0.95, 0.98, 0.99, 0.99, 1.0]           0.95-1.0   \n",
       "9    [0.96, 0.97, 0.97, 0.99, 1.0]           0.96-1.0   \n",
       "10   [0.97, 0.97, 0.99, 0.99, 1.0]           0.97-1.0   \n",
       "11   [0.94, 0.97, 0.99, 0.99, 1.0]           0.94-1.0   \n",
       "12   [0.94, 0.96, 0.96, 0.97, 1.0]           0.94-1.0   \n",
       "13   [0.93, 0.94, 0.96, 0.96, 1.0]           0.93-1.0   \n",
       "14  [0.41, 0.51, 0.58, 0.64, 0.66]          0.41-0.66   \n",
       "0    [0.93, 0.97, 0.99, 0.99, 1.0]           0.93-1.0   \n",
       "0    [0.93, 0.98, 0.99, 0.99, 1.0]           0.93-1.0   \n",
       "0    [0.92, 0.92, 0.95, 0.98, 1.0]           0.92-1.0   \n",
       "0     [0.9, 0.93, 0.93, 0.95, 1.0]            0.9-1.0   \n",
       "0    [0.94, 0.95, 0.96, 0.98, 1.0]           0.94-1.0   \n",
       "0    [0.96, 0.97, 0.99, 0.99, 1.0]           0.96-1.0   \n",
       "0    [0.93, 0.98, 0.99, 0.99, 1.0]           0.93-1.0   \n",
       "0    [0.93, 0.93, 0.95, 0.99, 1.0]           0.93-1.0   \n",
       "\n",
       "    Std Deviation of F1 Scores  \n",
       "0                     0.007071  \n",
       "1                     0.016733  \n",
       "2                     0.020000  \n",
       "3                     0.020000  \n",
       "4                     0.020736  \n",
       "5                     0.032094  \n",
       "6                     0.023022  \n",
       "7                     0.027749  \n",
       "8                     0.019235  \n",
       "9                     0.016432  \n",
       "10                    0.013416  \n",
       "11                    0.023875  \n",
       "12                    0.021909  \n",
       "13                    0.026833  \n",
       "14                    0.102225  \n",
       "0                     0.027928  \n",
       "0                     0.027749  \n",
       "0                     0.035777  \n",
       "0                     0.037014  \n",
       "0                     0.024083  \n",
       "0                     0.016432  \n",
       "0                     0.027749  \n",
       "0                     0.033166  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_obj = knn\n",
    "model_name = \"K Nearst Neighbour\"\n",
    "process = \"Ngram with NLTK Stemming\"\n",
    "n_splits = 5\n",
    "X = train_ds_features.toarray()\n",
    "y = train_ds.sentiment\n",
    "stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n",
    "df_model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda = LinearDiscriminantAnalysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.fit(train_X.toarray(), train_y)\n",
    "test_ds_predicted = lda.predict( test_X.toarray() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       873\n",
      "           1       0.99      0.99      0.99      1203\n",
      "\n",
      "    accuracy                           0.99      2076\n",
      "   macro avg       0.99      0.99      0.99      2076\n",
      "weighted avg       0.99      0.99      0.99      2076\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print( metrics.classification_report( test_y, test_ds_predicted ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Process</th>\n",
       "      <th>Model Name</th>\n",
       "      <th>F1 Scores</th>\n",
       "      <th>Range of F1 Scores</th>\n",
       "      <th>Std Deviation of F1 Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>[0.98, 0.99, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.98-1.0</td>\n",
       "      <td>0.007071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Stochastic Gradient Descent</td>\n",
       "      <td>[0.96, 0.98, 0.99, 1.0, 1.0]</td>\n",
       "      <td>0.96-1.0</td>\n",
       "      <td>0.016733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>XG Boost</td>\n",
       "      <td>[0.95, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>[0.95, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>[0.95, 0.98, 0.99, 1.0, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.020736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Binomial Naive Bayes Classifier</td>\n",
       "      <td>[0.92, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.032094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>[0.95, 0.97, 1.0, 1.0, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.023022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Decission Tree</td>\n",
       "      <td>[0.93, 0.95, 0.98, 0.98, 1.0]</td>\n",
       "      <td>0.93-1.0</td>\n",
       "      <td>0.027749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>[0.95, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.019235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>XG Boost</td>\n",
       "      <td>[0.96, 0.97, 0.97, 0.99, 1.0]</td>\n",
       "      <td>0.96-1.0</td>\n",
       "      <td>0.016432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Stochastic Gradient Descent</td>\n",
       "      <td>[0.97, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.97-1.0</td>\n",
       "      <td>0.013416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Gausian Process</td>\n",
       "      <td>[0.94, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.94-1.0</td>\n",
       "      <td>0.023875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>K Nearst Neighbour</td>\n",
       "      <td>[0.94, 0.96, 0.96, 0.97, 1.0]</td>\n",
       "      <td>0.94-1.0</td>\n",
       "      <td>0.021909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>[0.93, 0.94, 0.96, 0.96, 1.0]</td>\n",
       "      <td>0.93-1.0</td>\n",
       "      <td>0.026833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>[0.41, 0.51, 0.58, 0.64, 0.66]</td>\n",
       "      <td>0.41-0.66</td>\n",
       "      <td>0.102225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ngram with NLTK Stemming</td>\n",
       "      <td>Binomial Naive Bayes Classifier</td>\n",
       "      <td>[0.93, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.93-1.0</td>\n",
       "      <td>0.027928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ngram with NLTK Stemming</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>[0.93, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.93-1.0</td>\n",
       "      <td>0.027749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ngram with NLTK Stemming</td>\n",
       "      <td>Decission Tree</td>\n",
       "      <td>[0.92, 0.92, 0.95, 0.98, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.035777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ngram with NLTK Stemming</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>[0.9, 0.93, 0.93, 0.95, 1.0]</td>\n",
       "      <td>0.9-1.0</td>\n",
       "      <td>0.037014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ngram with NLTK Stemming</td>\n",
       "      <td>XG Boost</td>\n",
       "      <td>[0.94, 0.95, 0.96, 0.98, 1.0]</td>\n",
       "      <td>0.94-1.0</td>\n",
       "      <td>0.024083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ngram with NLTK Stemming</td>\n",
       "      <td>Stochastic Gradient Descent</td>\n",
       "      <td>[0.96, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.96-1.0</td>\n",
       "      <td>0.016432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ngram with NLTK Stemming</td>\n",
       "      <td>Gausian Process</td>\n",
       "      <td>[0.93, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.93-1.0</td>\n",
       "      <td>0.027749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ngram with NLTK Stemming</td>\n",
       "      <td>K Nearst Neighbour</td>\n",
       "      <td>[0.93, 0.93, 0.95, 0.99, 1.0]</td>\n",
       "      <td>0.93-1.0</td>\n",
       "      <td>0.033166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ngram with NLTK Stemming</td>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>[0.83, 0.96, 0.97, 0.98, 0.98]</td>\n",
       "      <td>0.83-0.98</td>\n",
       "      <td>0.064265</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Process                       Model Name  \\\n",
       "0   Bag Of Words with NLTK Stemming     Linear Discriminant Analysis   \n",
       "1   Bag Of Words with NLTK Stemming      Stochastic Gradient Descent   \n",
       "2   Bag Of Words with NLTK Stemming                         XG Boost   \n",
       "3   Bag Of Words with NLTK Stemming           Support Vector Machine   \n",
       "4   Bag Of Words with NLTK Stemming              Logistic Regression   \n",
       "5          TFIDF with NLTK Stemming  Binomial Naive Bayes Classifier   \n",
       "6          TFIDF with NLTK Stemming              Logistic Regression   \n",
       "7          TFIDF with NLTK Stemming                   Decission Tree   \n",
       "8          TFIDF with NLTK Stemming                    Random Forest   \n",
       "9          TFIDF with NLTK Stemming                         XG Boost   \n",
       "10         TFIDF with NLTK Stemming      Stochastic Gradient Descent   \n",
       "11         TFIDF with NLTK Stemming                  Gausian Process   \n",
       "12         TFIDF with NLTK Stemming               K Nearst Neighbour   \n",
       "13         TFIDF with NLTK Stemming     Linear Discriminant Analysis   \n",
       "14         TFIDF with NLTK Stemming           Support Vector Machine   \n",
       "0          Ngram with NLTK Stemming  Binomial Naive Bayes Classifier   \n",
       "0          Ngram with NLTK Stemming              Logistic Regression   \n",
       "0          Ngram with NLTK Stemming                   Decission Tree   \n",
       "0          Ngram with NLTK Stemming                    Random Forest   \n",
       "0          Ngram with NLTK Stemming                         XG Boost   \n",
       "0          Ngram with NLTK Stemming      Stochastic Gradient Descent   \n",
       "0          Ngram with NLTK Stemming                  Gausian Process   \n",
       "0          Ngram with NLTK Stemming               K Nearst Neighbour   \n",
       "0          Ngram with NLTK Stemming     Linear Discriminant Analysis   \n",
       "\n",
       "                         F1 Scores Range of F1 Scores  \\\n",
       "0    [0.98, 0.99, 0.99, 0.99, 1.0]           0.98-1.0   \n",
       "1     [0.96, 0.98, 0.99, 1.0, 1.0]           0.96-1.0   \n",
       "2    [0.95, 0.97, 0.99, 0.99, 1.0]           0.95-1.0   \n",
       "3    [0.95, 0.97, 0.99, 0.99, 1.0]           0.95-1.0   \n",
       "4     [0.95, 0.98, 0.99, 1.0, 1.0]           0.95-1.0   \n",
       "5    [0.92, 0.98, 0.99, 0.99, 1.0]           0.92-1.0   \n",
       "6      [0.95, 0.97, 1.0, 1.0, 1.0]           0.95-1.0   \n",
       "7    [0.93, 0.95, 0.98, 0.98, 1.0]           0.93-1.0   \n",
       "8    [0.95, 0.98, 0.99, 0.99, 1.0]           0.95-1.0   \n",
       "9    [0.96, 0.97, 0.97, 0.99, 1.0]           0.96-1.0   \n",
       "10   [0.97, 0.97, 0.99, 0.99, 1.0]           0.97-1.0   \n",
       "11   [0.94, 0.97, 0.99, 0.99, 1.0]           0.94-1.0   \n",
       "12   [0.94, 0.96, 0.96, 0.97, 1.0]           0.94-1.0   \n",
       "13   [0.93, 0.94, 0.96, 0.96, 1.0]           0.93-1.0   \n",
       "14  [0.41, 0.51, 0.58, 0.64, 0.66]          0.41-0.66   \n",
       "0    [0.93, 0.97, 0.99, 0.99, 1.0]           0.93-1.0   \n",
       "0    [0.93, 0.98, 0.99, 0.99, 1.0]           0.93-1.0   \n",
       "0    [0.92, 0.92, 0.95, 0.98, 1.0]           0.92-1.0   \n",
       "0     [0.9, 0.93, 0.93, 0.95, 1.0]            0.9-1.0   \n",
       "0    [0.94, 0.95, 0.96, 0.98, 1.0]           0.94-1.0   \n",
       "0    [0.96, 0.97, 0.99, 0.99, 1.0]           0.96-1.0   \n",
       "0    [0.93, 0.98, 0.99, 0.99, 1.0]           0.93-1.0   \n",
       "0    [0.93, 0.93, 0.95, 0.99, 1.0]           0.93-1.0   \n",
       "0   [0.83, 0.96, 0.97, 0.98, 0.98]          0.83-0.98   \n",
       "\n",
       "    Std Deviation of F1 Scores  \n",
       "0                     0.007071  \n",
       "1                     0.016733  \n",
       "2                     0.020000  \n",
       "3                     0.020000  \n",
       "4                     0.020736  \n",
       "5                     0.032094  \n",
       "6                     0.023022  \n",
       "7                     0.027749  \n",
       "8                     0.019235  \n",
       "9                     0.016432  \n",
       "10                    0.013416  \n",
       "11                    0.023875  \n",
       "12                    0.021909  \n",
       "13                    0.026833  \n",
       "14                    0.102225  \n",
       "0                     0.027928  \n",
       "0                     0.027749  \n",
       "0                     0.035777  \n",
       "0                     0.037014  \n",
       "0                     0.024083  \n",
       "0                     0.016432  \n",
       "0                     0.027749  \n",
       "0                     0.033166  \n",
       "0                     0.064265  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_obj = lda\n",
    "model_name = \"Linear Discriminant Analysis\"\n",
    "process = \"Ngram with NLTK Stemming\"\n",
    "n_splits = 5\n",
    "X = train_ds_features.toarray()\n",
    "y = train_ds.sentiment\n",
    "stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n",
    "df_model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.fit(train_X.toarray(), train_y)\n",
    "test_ds_predicted = svm.predict( test_X.toarray() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.09      0.17       873\n",
      "           1       0.60      1.00      0.75      1203\n",
      "\n",
      "    accuracy                           0.62      2076\n",
      "   macro avg       0.80      0.55      0.46      2076\n",
      "weighted avg       0.77      0.62      0.51      2076\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print( metrics.classification_report( test_y, test_ds_predicted ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Process</th>\n",
       "      <th>Model Name</th>\n",
       "      <th>F1 Scores</th>\n",
       "      <th>Range of F1 Scores</th>\n",
       "      <th>Std Deviation of F1 Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>[0.98, 0.99, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.98-1.0</td>\n",
       "      <td>0.007071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Stochastic Gradient Descent</td>\n",
       "      <td>[0.96, 0.98, 0.99, 1.0, 1.0]</td>\n",
       "      <td>0.96-1.0</td>\n",
       "      <td>0.016733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>XG Boost</td>\n",
       "      <td>[0.95, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>[0.95, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>[0.95, 0.98, 0.99, 1.0, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.020736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Binomial Naive Bayes Classifier</td>\n",
       "      <td>[0.92, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.032094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>[0.95, 0.97, 1.0, 1.0, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.023022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Decission Tree</td>\n",
       "      <td>[0.93, 0.95, 0.98, 0.98, 1.0]</td>\n",
       "      <td>0.93-1.0</td>\n",
       "      <td>0.027749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>[0.95, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.019235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>XG Boost</td>\n",
       "      <td>[0.96, 0.97, 0.97, 0.99, 1.0]</td>\n",
       "      <td>0.96-1.0</td>\n",
       "      <td>0.016432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Stochastic Gradient Descent</td>\n",
       "      <td>[0.97, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.97-1.0</td>\n",
       "      <td>0.013416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Gausian Process</td>\n",
       "      <td>[0.94, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.94-1.0</td>\n",
       "      <td>0.023875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>K Nearst Neighbour</td>\n",
       "      <td>[0.94, 0.96, 0.96, 0.97, 1.0]</td>\n",
       "      <td>0.94-1.0</td>\n",
       "      <td>0.021909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>[0.93, 0.94, 0.96, 0.96, 1.0]</td>\n",
       "      <td>0.93-1.0</td>\n",
       "      <td>0.026833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>TFIDF with NLTK Stemming</td>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>[0.41, 0.51, 0.58, 0.64, 0.66]</td>\n",
       "      <td>0.41-0.66</td>\n",
       "      <td>0.102225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ngram with NLTK Stemming</td>\n",
       "      <td>Binomial Naive Bayes Classifier</td>\n",
       "      <td>[0.93, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.93-1.0</td>\n",
       "      <td>0.027928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ngram with NLTK Stemming</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>[0.93, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.93-1.0</td>\n",
       "      <td>0.027749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ngram with NLTK Stemming</td>\n",
       "      <td>Decission Tree</td>\n",
       "      <td>[0.92, 0.92, 0.95, 0.98, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.035777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ngram with NLTK Stemming</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>[0.9, 0.93, 0.93, 0.95, 1.0]</td>\n",
       "      <td>0.9-1.0</td>\n",
       "      <td>0.037014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ngram with NLTK Stemming</td>\n",
       "      <td>XG Boost</td>\n",
       "      <td>[0.94, 0.95, 0.96, 0.98, 1.0]</td>\n",
       "      <td>0.94-1.0</td>\n",
       "      <td>0.024083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ngram with NLTK Stemming</td>\n",
       "      <td>Stochastic Gradient Descent</td>\n",
       "      <td>[0.96, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.96-1.0</td>\n",
       "      <td>0.016432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ngram with NLTK Stemming</td>\n",
       "      <td>Gausian Process</td>\n",
       "      <td>[0.93, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.93-1.0</td>\n",
       "      <td>0.027749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ngram with NLTK Stemming</td>\n",
       "      <td>K Nearst Neighbour</td>\n",
       "      <td>[0.93, 0.93, 0.95, 0.99, 1.0]</td>\n",
       "      <td>0.93-1.0</td>\n",
       "      <td>0.033166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ngram with NLTK Stemming</td>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>[0.83, 0.96, 0.97, 0.98, 0.98]</td>\n",
       "      <td>0.83-0.98</td>\n",
       "      <td>0.064265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ngram with NLTK Stemming</td>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>[0.41, 0.41, 0.54, 0.59, 0.76]</td>\n",
       "      <td>0.41-0.76</td>\n",
       "      <td>0.145499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Process                       Model Name  \\\n",
       "0   Bag Of Words with NLTK Stemming     Linear Discriminant Analysis   \n",
       "1   Bag Of Words with NLTK Stemming      Stochastic Gradient Descent   \n",
       "2   Bag Of Words with NLTK Stemming                         XG Boost   \n",
       "3   Bag Of Words with NLTK Stemming           Support Vector Machine   \n",
       "4   Bag Of Words with NLTK Stemming              Logistic Regression   \n",
       "5          TFIDF with NLTK Stemming  Binomial Naive Bayes Classifier   \n",
       "6          TFIDF with NLTK Stemming              Logistic Regression   \n",
       "7          TFIDF with NLTK Stemming                   Decission Tree   \n",
       "8          TFIDF with NLTK Stemming                    Random Forest   \n",
       "9          TFIDF with NLTK Stemming                         XG Boost   \n",
       "10         TFIDF with NLTK Stemming      Stochastic Gradient Descent   \n",
       "11         TFIDF with NLTK Stemming                  Gausian Process   \n",
       "12         TFIDF with NLTK Stemming               K Nearst Neighbour   \n",
       "13         TFIDF with NLTK Stemming     Linear Discriminant Analysis   \n",
       "14         TFIDF with NLTK Stemming           Support Vector Machine   \n",
       "0          Ngram with NLTK Stemming  Binomial Naive Bayes Classifier   \n",
       "0          Ngram with NLTK Stemming              Logistic Regression   \n",
       "0          Ngram with NLTK Stemming                   Decission Tree   \n",
       "0          Ngram with NLTK Stemming                    Random Forest   \n",
       "0          Ngram with NLTK Stemming                         XG Boost   \n",
       "0          Ngram with NLTK Stemming      Stochastic Gradient Descent   \n",
       "0          Ngram with NLTK Stemming                  Gausian Process   \n",
       "0          Ngram with NLTK Stemming               K Nearst Neighbour   \n",
       "0          Ngram with NLTK Stemming     Linear Discriminant Analysis   \n",
       "0          Ngram with NLTK Stemming           Support Vector Machine   \n",
       "\n",
       "                         F1 Scores Range of F1 Scores  \\\n",
       "0    [0.98, 0.99, 0.99, 0.99, 1.0]           0.98-1.0   \n",
       "1     [0.96, 0.98, 0.99, 1.0, 1.0]           0.96-1.0   \n",
       "2    [0.95, 0.97, 0.99, 0.99, 1.0]           0.95-1.0   \n",
       "3    [0.95, 0.97, 0.99, 0.99, 1.0]           0.95-1.0   \n",
       "4     [0.95, 0.98, 0.99, 1.0, 1.0]           0.95-1.0   \n",
       "5    [0.92, 0.98, 0.99, 0.99, 1.0]           0.92-1.0   \n",
       "6      [0.95, 0.97, 1.0, 1.0, 1.0]           0.95-1.0   \n",
       "7    [0.93, 0.95, 0.98, 0.98, 1.0]           0.93-1.0   \n",
       "8    [0.95, 0.98, 0.99, 0.99, 1.0]           0.95-1.0   \n",
       "9    [0.96, 0.97, 0.97, 0.99, 1.0]           0.96-1.0   \n",
       "10   [0.97, 0.97, 0.99, 0.99, 1.0]           0.97-1.0   \n",
       "11   [0.94, 0.97, 0.99, 0.99, 1.0]           0.94-1.0   \n",
       "12   [0.94, 0.96, 0.96, 0.97, 1.0]           0.94-1.0   \n",
       "13   [0.93, 0.94, 0.96, 0.96, 1.0]           0.93-1.0   \n",
       "14  [0.41, 0.51, 0.58, 0.64, 0.66]          0.41-0.66   \n",
       "0    [0.93, 0.97, 0.99, 0.99, 1.0]           0.93-1.0   \n",
       "0    [0.93, 0.98, 0.99, 0.99, 1.0]           0.93-1.0   \n",
       "0    [0.92, 0.92, 0.95, 0.98, 1.0]           0.92-1.0   \n",
       "0     [0.9, 0.93, 0.93, 0.95, 1.0]            0.9-1.0   \n",
       "0    [0.94, 0.95, 0.96, 0.98, 1.0]           0.94-1.0   \n",
       "0    [0.96, 0.97, 0.99, 0.99, 1.0]           0.96-1.0   \n",
       "0    [0.93, 0.98, 0.99, 0.99, 1.0]           0.93-1.0   \n",
       "0    [0.93, 0.93, 0.95, 0.99, 1.0]           0.93-1.0   \n",
       "0   [0.83, 0.96, 0.97, 0.98, 0.98]          0.83-0.98   \n",
       "0   [0.41, 0.41, 0.54, 0.59, 0.76]          0.41-0.76   \n",
       "\n",
       "    Std Deviation of F1 Scores  \n",
       "0                     0.007071  \n",
       "1                     0.016733  \n",
       "2                     0.020000  \n",
       "3                     0.020000  \n",
       "4                     0.020736  \n",
       "5                     0.032094  \n",
       "6                     0.023022  \n",
       "7                     0.027749  \n",
       "8                     0.019235  \n",
       "9                     0.016432  \n",
       "10                    0.013416  \n",
       "11                    0.023875  \n",
       "12                    0.021909  \n",
       "13                    0.026833  \n",
       "14                    0.102225  \n",
       "0                     0.027928  \n",
       "0                     0.027749  \n",
       "0                     0.035777  \n",
       "0                     0.037014  \n",
       "0                     0.024083  \n",
       "0                     0.016432  \n",
       "0                     0.027749  \n",
       "0                     0.033166  \n",
       "0                     0.064265  \n",
       "0                     0.145499  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_obj = svm\n",
    "model_name = \"Support Vector Machine\"\n",
    "process = \"Ngram with NLTK Stemming\"\n",
    "n_splits = 5\n",
    "X = train_ds_features.toarray()\n",
    "y = train_ds.sentiment\n",
    "stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n",
    "df_model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_selection.to_csv(\"Model_statistics.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
