{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Pre_Trained_Word_Embeddings.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"cells":[{"cell_type":"markdown","metadata":{"id":"z89NDA-ToPnT"},"source":["In this notebook, let us see how we can represent text using pre-trained word embedding models. \n","\n","# 1. Using a pre-trained word2vec model\n","\n","Let us take an example of a pre-trained word2vec model, and how we can use it to look for most similar words. We will use the Google News vectors embeddings.\n","https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM\n","\n","A few other pre-trained word embedding models, and details on the means to access them through gensim can be found in:\n","https://github.com/RaRe-Technologies/gensim-data"]},{"cell_type":"code","metadata":{"id":"FTpzLd6dvB6Q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605962413137,"user_tz":-330,"elapsed":98557,"user":{"displayName":"Raghav Nyapati","photoUrl":"","userId":"13728981372697321629"}},"outputId":"10b5af8c-3759-48f5-9340-88ee2181c0a7"},"source":["!wget -P /tmp/input/ -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""],"execution_count":1,"outputs":[{"output_type":"stream","text":["--2020-11-21 12:38:24--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n","Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.12.86\n","Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.12.86|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1647046227 (1.5G) [application/x-gzip]\n","Saving to: ‘/tmp/input/GoogleNews-vectors-negative300.bin.gz’\n","\n","GoogleNews-vectors- 100%[===================>]   1.53G  16.6MB/s    in 97s     \n","\n","2020-11-21 12:40:02 (16.3 MB/s) - ‘/tmp/input/GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZBsTuJ5FwAFm","executionInfo":{"status":"ok","timestamp":1605962424621,"user_tz":-330,"elapsed":1269,"user":{"displayName":"Raghav Nyapati","photoUrl":"","userId":"13728981372697321629"}}},"source":["import warnings #This module ignores the various types of warnings generated\n","warnings.filterwarnings(\"ignore\") \n","\n","import os #This module provides a way of using operating system dependent functionality\n","\n","import psutil #This module helps in retrieving information on running processes and system resource utilization\n","process = psutil.Process(os.getpid())\n","from psutil import virtual_memory\n","mem = virtual_memory()\n","\n","import time #This module is used to calculate the time  "],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"aodBmqZToPnY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605962539739,"user_tz":-330,"elapsed":112699,"user":{"displayName":"Raghav Nyapati","photoUrl":"","userId":"13728981372697321629"}},"outputId":"68ccd149-70fc-45f3-8651-85ff3a391084"},"source":["from gensim.models import Word2Vec, KeyedVectors\n","pretrainedpath = '/tmp/input/GoogleNews-vectors-negative300.bin.gz'\n","\n","#Load W2V model. This will take some time, but it is a one time effort! \n","pre = process.memory_info().rss\n","print(\"Memory used in GB before Loading the Model: %0.2f\"%float(pre/(10**9))) #Check memory usage before loading the model\n","print('-'*10)\n","\n","start_time = time.time() #Start the timer\n","ttl = mem.total #Toal memory available\n","\n","w2v_model = KeyedVectors.load_word2vec_format(pretrainedpath, binary=True) #load the model\n","print(\"%0.2f seconds taken to load\"%float(time.time() - start_time)) #Calculate the total time elapsed since starting the timer\n","print('-'*10)\n","\n","print('Finished loading Word2Vec')\n","print('-'*10)\n","\n","post = process.memory_info().rss\n","print(\"Memory used in GB after Loading the Model: {:.2f}\".format(float(post/(10**9)))) #Calculate the memory used after loading the model\n","print('-'*10)\n","\n","print(\"Percentage increase in memory usage: {:.2f}% \".format(float((post/pre)*100))) #Percentage increase in memory after loading the model\n","print('-'*10)\n","\n","print(\"Numver of words in vocablulary: \",len(w2v_model.vocab)) #Number of words in the vocabulary. "],"execution_count":3,"outputs":[{"output_type":"stream","text":["Memory used in GB before Loading the Model: 0.16\n","----------\n","109.84 seconds taken to load\n","----------\n","Finished loading Word2Vec\n","----------\n","Memory used in GB after Loading the Model: 5.01\n","----------\n","Percentage increase in memory usage: 3176.33% \n","----------\n","Numver of words in vocablulary:  3000000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZhJ_488PoPnr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605962713093,"user_tz":-330,"elapsed":10375,"user":{"displayName":"Raghav Nyapati","photoUrl":"","userId":"13728981372697321629"}},"outputId":"9ca170b2-0233-48cb-8c8c-9a73f7467b77"},"source":["#Let us examine the model by knowing what the most similar words are, for a given word!\n","w2v_model.most_similar('beautiful')"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('gorgeous', 0.8353004455566406),\n"," ('lovely', 0.810693621635437),\n"," ('stunningly_beautiful', 0.7329413890838623),\n"," ('breathtakingly_beautiful', 0.7231341004371643),\n"," ('wonderful', 0.6854087114334106),\n"," ('fabulous', 0.6700063943862915),\n"," ('loveliest', 0.6612576246261597),\n"," ('prettiest', 0.6595001816749573),\n"," ('beatiful', 0.6593326330184937),\n"," ('magnificent', 0.6591402292251587)]"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"G1Or5oG5oPn1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605962730802,"user_tz":-330,"elapsed":1311,"user":{"displayName":"Raghav Nyapati","photoUrl":"","userId":"13728981372697321629"}},"outputId":"bfc06d5f-ab37-4bfc-d9f7-fd69a5dc27a1"},"source":["#Let us try with another word! \n","w2v_model.most_similar('toronto')"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('montreal', 0.698411226272583),\n"," ('vancouver', 0.6587257385253906),\n"," ('nyc', 0.6248831748962402),\n"," ('alberta', 0.6179691553115845),\n"," ('boston', 0.611499547958374),\n"," ('calgary', 0.61032634973526),\n"," ('edmonton', 0.6100261211395264),\n"," ('canadian', 0.5944076776504517),\n"," ('chicago', 0.5911980271339417),\n"," ('springfield', 0.5888351202011108)]"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"rtQiYOR9oPn_","colab":{"base_uri":"https://localhost:8080/","height":180},"executionInfo":{"status":"error","timestamp":1605977576552,"user_tz":-330,"elapsed":1247,"user":{"displayName":"Raghav Nyapati","photoUrl":"","userId":"13728981372697321629"}},"outputId":"6ad3ca87-a8b9-4161-957b-481e814ba92f"},"source":["#What is the vector representation for a word? \n","w2v_model['computer']"],"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-e56ebda0e12e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#What is the vector representation for a word?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2v_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'computer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'w2v_model' is not defined"]}]},{"cell_type":"code","metadata":{"id":"RoeH_gfroPoJ","colab":{"base_uri":"https://localhost:8080/","height":323},"executionInfo":{"status":"error","timestamp":1605962759275,"user_tz":-330,"elapsed":1593,"user":{"displayName":"Raghav Nyapati","photoUrl":"","userId":"13728981372697321629"}},"outputId":"f8c72caa-302e-43ab-d433-d55b937d24dc"},"source":["#What if I am looking for a word that is not in this vocabulary?\n","w2v_model['practicalnlp']"],"execution_count":7,"outputs":[{"output_type":"error","ename":"KeyError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-354849ef77a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#What if I am looking for a word that is not in this vocabulary?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mw2v_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'practicalnlp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, entities)\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: \"word 'practicalnlp' not in vocabulary\""]}]},{"cell_type":"markdown","metadata":{"id":"douwnKS5oPoS"},"source":["####Two things to note while using pre-trained models: \n","\n","\n","1.   Tokens/Words are always lowercased. If a word is not in the vocabulary,   the model throws an exception.\n","2.   So, it is always a good idea to encapsulate those statements in try/except blocks.\n","\n"," "]},{"cell_type":"markdown","metadata":{"id":"mjWxh9kBoPob"},"source":["# 2. Getting the embedding representation for full text\n","\n","We have seen how to get embedding vectors for single words. How do we use them to get such a representation for a full text? A simple way is to just sum or average the embeddings for individual words. We will see an example of this using Word2Vec in Chapter 4. Let us see a small example using another NLP library Spacy - which we saw earlier in Chapter 2 too.\n"]},{"cell_type":"code","metadata":{"id":"uFLuSb9ZoPoc","outputId":"35fe37df-f1bd-4c64-c8ad-0455980966ee"},"source":["import spacy\n","\n","# Load the spacy model that we already installed in Chapter 2. This takes a few seconds.\n","%time nlp = spacy.load('en_core_web_md')\n","# process a sentence using the model\n","mydoc = nlp(\"Canada is a large country\")\n","#Get a vector for individual words\n","#print(doc[0].vector) #vector for 'Canada', the first word in the text \n","print(mydoc.vector) #Averaged vector for the entire sentence"],"execution_count":null,"outputs":[{"output_type":"stream","text":["CPU times: user 13.3 s, sys: 366 ms, total: 13.6 s\n","Wall time: 13.6 s\n","[-1.12055197e-01  2.26087615e-01 -5.15111461e-02 -1.21812008e-01\n","  4.13958639e-01 -8.56475979e-02 -2.84600933e-03 -2.26096585e-01\n","  6.98113963e-02  2.27946019e+00 -4.49774921e-01 -6.39050007e-02\n"," -1.80326015e-01 -8.79765972e-02  9.93399299e-04 -1.57384202e-01\n"," -1.23817801e-01  1.54990411e+00  2.00794004e-02  1.38399601e-01\n"," -1.48897991e-01 -2.23025799e-01 -1.48171991e-01  4.68924567e-02\n"," -3.17026004e-02  1.19096041e-02 -6.10985979e-02  9.57068056e-02\n","  9.37099904e-02  1.70955807e-01 -9.29740071e-03  7.88536817e-02\n","  1.74508005e-01 -1.04450598e-01  1.04872189e-01 -1.16961405e-01\n","  6.23028055e-02 -2.23016590e-01 -1.44107476e-01 -2.03423887e-01\n","  2.61404991e-01  2.43404001e-01  1.51980996e-01 -1.12484001e-01\n","  1.18055798e-01 -9.51323956e-02  8.66319984e-02 -2.54322797e-01\n","  3.84932049e-02  1.18278004e-01 -3.21602583e-01  3.73764008e-01\n","  1.13018408e-01 -8.05834010e-02  1.84921592e-01  9.38879885e-03\n","  1.22166201e-01 -3.24288011e-02  1.01590000e-01 -1.56877995e-01\n"," -2.57006437e-02  1.63392588e-01  1.06118001e-01  2.25193188e-01\n","  8.06204006e-02 -1.21081993e-01 -1.52107209e-01  8.25726017e-02\n"," -6.09899946e-02  1.44145802e-01  2.01554038e-02  2.54258011e-02\n","  1.06071997e-02  6.37948066e-02  1.10551611e-01 -6.40176088e-02\n"," -6.36451989e-02 -9.99798030e-02 -7.01020136e-02  3.09334368e-01\n","  5.68300001e-02  3.63879651e-03 -1.65255398e-01  2.98442870e-01\n","  4.01660334e-03 -1.73631594e-01  5.15965708e-02  1.61811799e-01\n","  2.20304996e-01 -8.29614028e-02 -2.64678001e-01  2.44114012e-01\n","  3.48895532e-03 -1.57521993e-01  1.67974800e-01  1.05541132e-01\n"," -1.31224409e-01  7.17941970e-02  1.39708191e-01 -1.95359858e-03\n"," -8.55428055e-02  1.20119795e-01 -6.84404075e-02  5.14601183e-04\n"," -2.86250003e-02 -1.10662603e+00  2.02491835e-01 -1.50410801e-01\n","  6.51507173e-03 -3.30360234e-03  1.21523812e-01 -1.61614027e-02\n"," -1.43233404e-01 -9.88139957e-02 -2.17486005e-02  1.81988999e-01\n","  8.85506049e-02  2.72242010e-01 -7.73219988e-02  1.43622067e-02\n"," -1.57062009e-01  4.01146002e-02  3.90305184e-02 -1.42812401e-01\n"," -2.08329991e-01  9.64459926e-02  1.42821997e-01 -1.94155991e-01\n","  5.37982993e-02 -1.00471973e-02  1.94714032e-02 -9.83514041e-02\n"," -4.17162031e-02  1.23069003e-01  1.68428212e-01 -1.17991492e-01\n"," -2.56704390e-01 -1.89464003e-01  9.22677964e-02 -1.72503412e-01\n"," -1.11929202e+00  6.42500073e-03  3.51435989e-01  8.19059983e-02\n","  4.92408946e-02 -1.80243999e-01  1.82863399e-01  8.92240033e-02\n","  2.47399211e-01  2.74492018e-02 -2.49322020e-02  2.35055804e-01\n","  8.12319964e-02 -1.86482631e-02 -1.06439434e-01  5.28851971e-02\n"," -1.02569997e-01  1.35777995e-01 -2.32603997e-01  9.24602076e-02\n","  1.92440599e-01  1.48551196e-01  5.57186007e-02  3.97088043e-02\n"," -6.74048066e-03  9.73687991e-02  2.62231939e-02 -8.26509967e-02\n","  1.30085424e-01 -1.38572007e-01 -4.11808006e-02 -4.13070023e-02\n"," -3.41880023e-02  1.28202796e-01 -6.66912049e-02 -7.41944537e-02\n"," -5.87003939e-02  1.36300415e-01  1.67494014e-01  1.71119809e-01\n","  1.18692197e-01  2.30142009e-02 -2.06086040e-02 -3.85930002e-01\n"," -1.17673976e-02 -7.34595209e-02 -3.43096368e-02 -7.80718103e-02\n"," -2.81003956e-02 -7.30765983e-02 -2.21649408e-01 -1.02057599e-01\n","  5.11020012e-02 -9.07440037e-02 -4.69896048e-02 -2.10200553e-03\n","  1.05816983e-01  4.79107983e-02  1.03080198e-01 -8.96641985e-02\n","  8.85651931e-02 -9.09178331e-02 -5.16167991e-02  1.50742605e-01\n","  3.07500213e-01 -4.05239780e-03  1.04269005e-01  3.55780013e-02\n","  1.16165996e-01 -2.97939777e-03 -1.42966792e-01  5.00957891e-02\n"," -1.08308598e-01  1.68199837e-03  1.36314392e-01  1.48694202e-01\n"," -3.17817986e-01  1.21000603e-01 -1.59556001e-01  7.51644000e-02\n"," -1.03386201e-01  1.10754207e-01  8.20529982e-02 -6.02059904e-03\n","  1.35578603e-01 -4.08943966e-02  6.05328009e-02  1.03734590e-01\n"," -6.22724071e-02  2.30276197e-01  1.30762011e-01  1.51950002e-01\n","  7.40183964e-02 -1.84507206e-01 -1.33174613e-01 -1.49338007e-01\n","  1.19309977e-01 -2.41554022e-01 -1.00904807e-01  1.54562384e-01\n"," -7.63845369e-02  1.66379198e-01  2.20374197e-01  1.58361979e-02\n","  1.80677801e-01 -1.77342609e-01  2.22857997e-01 -2.99477577e-01\n"," -1.53620601e-01 -2.67919600e-01  1.56353399e-01 -1.74718007e-01\n","  1.83644608e-01  1.28259212e-01 -6.30084053e-02  2.68236816e-01\n","  2.10368007e-01 -4.73994762e-02 -1.09680817e-01  1.62620202e-01\n","  8.96113962e-02  1.50361210e-01 -1.55037967e-02  1.50141995e-02\n","  1.76618043e-02 -2.28057191e-01  7.85290003e-02 -4.59632799e-02\n","  1.98103897e-02 -1.71379801e-02 -1.45676598e-01 -3.32076550e-02\n"," -2.09102005e-01 -2.48584002e-01 -8.51256028e-02  4.25900035e-02\n"," -1.33966401e-01  2.89979968e-02  2.10713193e-01 -1.86206046e-02\n","  1.71603993e-01  2.21868396e-01 -2.10479975e-01  1.49794608e-01\n"," -1.10692397e-01 -4.47340589e-03  5.13652042e-02 -7.27116019e-02\n","  6.07413948e-02 -8.13369974e-02 -1.94639400e-01 -5.06809242e-02\n","  6.40980080e-02 -2.20814198e-01  2.96969917e-02  1.53438210e-01\n"," -2.18270030e-02 -1.93358198e-01 -2.26220220e-01  1.84093148e-01]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Q-8wrQlLoPol","outputId":"86297e83-a7a9-41a4-80b7-47eb392a4e66"},"source":["#What happens when I give a sentence with strange words (and stop words), and try to get its word vector in Spacy?\n","temp = nlp('practicalnlp is a newword')\n","temp[0].vector"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"A2PSThugoPos"},"source":["Well, at least, this is better than throwing an exception! :) \n","\n"]},{"cell_type":"code","metadata":{"id":"8fpAdB8Vsct-"},"source":[""],"execution_count":null,"outputs":[]}]}