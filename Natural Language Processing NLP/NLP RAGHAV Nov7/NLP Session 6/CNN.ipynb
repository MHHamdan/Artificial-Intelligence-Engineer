{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.1"},"colab":{"name":"CNN.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3Zyu-D1l8XUS","executionInfo":{"status":"ok","timestamp":1606653186643,"user_tz":-330,"elapsed":13758,"user":{"displayName":"Raghav Nyapati","photoUrl":"","userId":"13728981372697321629"}},"outputId":"dfbe6310-ede3-4868-8f55-2c201222e0c7"},"source":["import os\n","import re\n","import tarfile\n","import tqdm\n","\n","import requests\n","! pip install pugnlp\n","from pugnlp.futil import path_status, find_files\n","import numpy as np  # Keras takes care of most of this but it likes to see Numpy arrays\n","from keras.preprocessing import sequence    # A helper module to handle padding input\n","from keras.models import Sequential         # The base keras Neural Network model\n","from keras.layers import Dense, Dropout, Activation   # The layer objects we will pile into the model\n","from keras.layers import Conv1D, GlobalMaxPooling1D"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting pugnlp\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/20/a9f0b1f45c074c63da716fc1b301916cc3b64c11d5cbf7cb305eafaf158a/pugnlp-0.2.6-py2.py3-none-any.whl (706kB)\n","\r\u001b[K     |▌                               | 10kB 11.7MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 10.2MB/s eta 0:00:01\r\u001b[K     |█▍                              | 30kB 6.1MB/s eta 0:00:01\r\u001b[K     |█▉                              | 40kB 6.1MB/s eta 0:00:01\r\u001b[K     |██▎                             | 51kB 4.4MB/s eta 0:00:01\r\u001b[K     |██▉                             | 61kB 4.6MB/s eta 0:00:01\r\u001b[K     |███▎                            | 71kB 5.0MB/s eta 0:00:01\r\u001b[K     |███▊                            | 81kB 5.1MB/s eta 0:00:01\r\u001b[K     |████▏                           | 92kB 4.9MB/s eta 0:00:01\r\u001b[K     |████▋                           | 102kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████                           | 112kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 122kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████                          | 133kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 143kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████                         | 153kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 163kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 174kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 184kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 194kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 204kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 215kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 225kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 235kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 245kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 256kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████                    | 266kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 276kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 286kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 296kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 307kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 317kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 327kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 337kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 348kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 358kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 368kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 378kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 389kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 399kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 409kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 419kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 430kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 440kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 450kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 460kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 471kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 481kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 491kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 501kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 512kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 522kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 532kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 542kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 552kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 563kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 573kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 583kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 593kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 604kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 614kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 624kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 634kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 645kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 655kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 665kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 675kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 686kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 696kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 706kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 716kB 5.4MB/s \n","\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from pugnlp) (3.2.5)\n","Requirement already satisfied: jupyter in /usr/local/lib/python3.6/dist-packages (from pugnlp) (1.0.0)\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (from pugnlp) (0.11.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from pugnlp) (1.4.1)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pugnlp) (0.16.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pugnlp) (4.41.1)\n","Collecting fuzzywuzzy\n","  Downloading https://files.pythonhosted.org/packages/43/ff/74f23998ad2f93b945c0309f825be92e04e0348e062026998b5eefef4c33/fuzzywuzzy-0.18.0-py2.py3-none-any.whl\n","Requirement already satisfied: wheel in /usr/local/lib/python3.6/dist-packages (from pugnlp) (0.35.1)\n","Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (from pugnlp) (3.6.0)\n","Collecting pypandoc\n","  Downloading https://files.pythonhosted.org/packages/d6/b7/5050dc1769c8a93d3ec7c4bd55be161991c94b8b235f88bf7c764449e708/pypandoc-1.5.tar.gz\n","Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from pugnlp) (4.0.1)\n","Requirement already satisfied: pip in /usr/local/lib/python3.6/dist-packages (from pugnlp) (19.3.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from pugnlp) (0.22.2.post1)\n","Requirement already satisfied: plotly in /usr/local/lib/python3.6/dist-packages (from pugnlp) (4.4.1)\n","Requirement already satisfied: coverage in /usr/local/lib/python3.6/dist-packages (from pugnlp) (3.7.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from pugnlp) (1.1.4)\n","Collecting python-Levenshtein\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/a9/d1785c85ebf9b7dfacd08938dd028209c34a0ea3b1bcdb895208bd40a67d/python-Levenshtein-0.12.0.tar.gz (48kB)\n","\u001b[K     |████████████████████████████████| 51kB 5.5MB/s \n","\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from pugnlp) (3.2.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->pugnlp) (1.15.0)\n","Requirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (from jupyter->pugnlp) (7.5.1)\n","Requirement already satisfied: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter->pugnlp) (4.7.7)\n","Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from jupyter->pugnlp) (5.3.1)\n","Requirement already satisfied: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter->pugnlp) (5.2.0)\n","Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from jupyter->pugnlp) (4.10.1)\n","Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from jupyter->pugnlp) (5.6.1)\n","Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/dist-packages (from seaborn->pugnlp) (1.18.5)\n","Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim->pugnlp) (3.0.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pypandoc->pugnlp) (50.3.2)\n","Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->pugnlp) (1.3)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->pugnlp) (0.17.0)\n","Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly->pugnlp) (1.3.3)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->pugnlp) (2.8.1)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->pugnlp) (2018.9)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pugnlp) (2.4.7)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pugnlp) (0.10.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pugnlp) (1.3.1)\n","Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->pugnlp) (3.5.1)\n","Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->pugnlp) (5.5.0)\n","Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->pugnlp) (5.0.8)\n","Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->pugnlp) (4.3.3)\n","Requirement already satisfied: pyzmq>=17.1 in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->pugnlp) (20.0.0)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->pugnlp) (2.6.1)\n","Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->pugnlp) (4.7.0)\n","Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->pugnlp) (0.2.0)\n","Requirement already satisfied: jupyter-client>=4.1 in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->pugnlp) (5.3.5)\n","Requirement already satisfied: qtpy in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->pugnlp) (1.9.0)\n","Requirement already satisfied: tornado>=4 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->pugnlp) (5.1.1)\n","Requirement already satisfied: Send2Trash in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->pugnlp) (1.5.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->pugnlp) (2.11.2)\n","Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->pugnlp) (0.9.1)\n","Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->pugnlp) (1.0.18)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->pugnlp) (1.4.3)\n","Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->pugnlp) (0.4.4)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->pugnlp) (3.2.1)\n","Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->pugnlp) (0.3)\n","Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->pugnlp) (0.8.4)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->pugnlp) (0.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->pugnlp) (2.23.0)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->pugnlp) (0.7.5)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->pugnlp) (4.4.2)\n","Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->pugnlp) (0.8.1)\n","Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->pugnlp) (4.8.0)\n","Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2.0->ipywidgets->jupyter->pugnlp) (2.6.0)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->notebook->jupyter->pugnlp) (1.1.1)\n","Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.8.1->notebook->jupyter->pugnlp) (0.6.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.0->jupyter-console->jupyter->pugnlp) (0.2.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter->pugnlp) (20.4)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter->pugnlp) (0.5.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim->pugnlp) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim->pugnlp) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim->pugnlp) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim->pugnlp) (2020.11.8)\n","Building wheels for collected packages: pypandoc, python-Levenshtein\n","  Building wheel for pypandoc (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pypandoc: filename=pypandoc-1.5-cp36-none-any.whl size=17038 sha256=9a216044f1e296670ac8684152965f102de80e008e6eaf4dd241f4b66d59b7be\n","  Stored in directory: /root/.cache/pip/wheels/bb/7d/d6/2f9af55e800d37e42e546106bcbd36a86e24e725e303d17e04\n","  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.0-cp36-cp36m-linux_x86_64.whl size=144792 sha256=d16645def5369f726167fc7c6ea2ea8a563040dcec087746b6aaed992a42b340\n","  Stored in directory: /root/.cache/pip/wheels/de/c2/93/660fd5f7559049268ad2dc6d81c4e39e9e36518766eaf7e342\n","Successfully built pypandoc python-Levenshtein\n","Installing collected packages: fuzzywuzzy, pypandoc, python-Levenshtein, pugnlp\n","Successfully installed fuzzywuzzy-0.18.0 pugnlp-0.2.6 pypandoc-1.5 python-Levenshtein-0.12.0\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/pugnlp/constants.py:158: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime instead.\n","  MIN_TIMESTAMP = pd.Timestamp(pd.datetime(1677, 9, 22, 0, 12, 44), tz='utc')\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"w-LwlCq48XUS","executionInfo":{"status":"ok","timestamp":1606655471233,"user_tz":-330,"elapsed":1527,"user":{"displayName":"Raghav Nyapati","photoUrl":"","userId":"13728981372697321629"}}},"source":["# From the nlpia package for downloading data too big for the repo\n","\n","BIG_URLS = {\n","    'w2v': (\n","        'https://www.dropbox.com/s/965dir4dje0hfi4/GoogleNews-vectors-negative300.bin.gz?dl=1',\n","        1647046227,\n","    ),\n","    'slang': (\n","        'https://www.dropbox.com/s/43c22018fbfzypd/slang.csv.gz?dl=1',\n","        117633024,\n","    ),\n","    'tweets': (\n","        'https://www.dropbox.com/s/5gpb43c494mc8p0/tweets.csv.gz?dl=1',\n","        311725313,\n","    ),\n","    'lsa_tweets': (\n","        'https://www.dropbox.com/s/rpjt0d060t4n1mr/lsa_tweets_5589798_2003588x200.tar.gz?dl=1',\n","        3112841563,  # 3112841312,\n","    ),\n","    'imdb': (\n","        'https://www.dropbox.com/s/yviic64qv84x73j/aclImdb_v1.tar.gz?dl=1',\n","        3112841563,  # 3112841312,\n","    ),\n","}"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"S86Eukrs8XUS","executionInfo":{"status":"ok","timestamp":1606655455011,"user_tz":-330,"elapsed":1725,"user":{"displayName":"Raghav Nyapati","photoUrl":"","userId":"13728981372697321629"}}},"source":["# These functions are part of the nlpia package which can be pip installed and run from there.\n","def dropbox_basename(url):\n","    filename = os.path.basename(url)\n","    match = re.findall(r'\\?dl=[0-9]$', filename)\n","    if match:\n","        return filename[:-len(match[0])]\n","    return filename\n","\n","def download_file(url, data_path='.', filename=None, size=None, chunk_size=4096, verbose=True):\n","    \"\"\"Uses stream=True and a reasonable chunk size to be able to download large (GB) files over https\"\"\"\n","    if filename is None:\n","        filename = dropbox_basename(url)\n","    file_path = os.path.join(data_path, filename)\n","    if url.endswith('?dl=0'):\n","        url = url[:-1] + '1'  # noninteractive download\n","    if verbose:\n","        tqdm_prog = tqdm\n","        print('requesting URL: {}'.format(url))\n","    else:\n","        tqdm_prog = no_tqdm\n","    r = requests.get(url, stream=True, allow_redirects=True)\n","    size = r.headers.get('Content-Length', None) if size is None else size\n","    print('remote size: {}'.format(size))\n","\n","    stat = path_status(file_path)\n","    print('local size: {}'.format(stat.get('size', None)))\n","    if stat['type'] == 'file' and stat['size'] == size:  # TODO: check md5 or get the right size of remote file\n","        r.close()\n","        return file_path\n","\n","    print('Downloading to {}'.format(file_path))\n","\n","    with open(file_path, 'wb') as f:\n","        for chunk in r.iter_content(chunk_size=chunk_size):\n","            if chunk:  # filter out keep-alive chunks\n","                f.write(chunk)\n","\n","    r.close()\n","    return file_path\n","\n","def untar(fname):\n","    if fname.endswith(\"tar.gz\"):\n","        with tarfile.open(fname) as tf:\n","            tf.extractall()\n","    else:\n","        print(\"Not a tar.gz file: {}\".format(fname))"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":306},"id":"osbEjVhr8XUS","executionInfo":{"status":"error","timestamp":1606655460171,"user_tz":-330,"elapsed":1541,"user":{"displayName":"Raghav Nyapati","photoUrl":"","userId":"13728981372697321629"}},"outputId":"b007ab12-29d5-4097-f6f4-b536f94158e8"},"source":["download_file(BIG_URLS['w2v'][0])"],"execution_count":8,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-02547dc5e22b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdownload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBIG_URLS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'w2v'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-7-483157fae55c>\u001b[0m in \u001b[0;36mdownload_file\u001b[0;34m(url, data_path, filename, size, chunk_size, verbose)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;34m\"\"\"Uses stream=True and a reasonable chunk size to be able to download large (GB) files over https\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdropbox_basename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'?dl=0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-483157fae55c>\u001b[0m in \u001b[0;36mdropbox_basename\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# These functions are part of the nlpia package which can be pip installed and run from there.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdropbox_basename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mmatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\?dl=[0-9]$'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kHLk53IR8XUS","executionInfo":{"status":"ok","timestamp":1606653505992,"user_tz":-330,"elapsed":32635,"user":{"displayName":"Raghav Nyapati","photoUrl":"","userId":"13728981372697321629"}},"outputId":"75e335ca-9058-4793-e321-c0b772143386"},"source":["untar(download_file(BIG_URLS['imdb'][0]))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["requesting URL: https://www.dropbox.com/s/yviic64qv84x73j/aclImdb_v1.tar.gz?dl=1\n","remote size: 84125825\n","local size: None\n","Downloading to ./aclImdb_v1.tar.gz\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bFF2xTnf8XUS","executionInfo":{"status":"ok","timestamp":1606653599664,"user_tz":-330,"elapsed":2131,"user":{"displayName":"Raghav Nyapati","photoUrl":"","userId":"13728981372697321629"}},"outputId":"ec721f06-0ed9-44fa-9199-9cdea23b72dc"},"source":["import glob\n","import os\n","\n","from random import shuffle\n","\n","def pre_process_data(filepath):\n","    \"\"\"\n","    This is dependent on your training data source but we will try to generalize it as best as possible.\n","    \"\"\"\n","    positive_path = os.path.join(filepath, 'pos')\n","    negative_path = os.path.join(filepath, 'neg')\n","    \n","    pos_label = 1\n","    neg_label = 0\n","    \n","    dataset = []\n","    \n","    for filename in glob.glob(os.path.join(positive_path, '*.txt')):\n","        with open(filename, 'r') as f:\n","            dataset.append((pos_label, f.read()))\n","            \n","    for filename in glob.glob(os.path.join(negative_path, '*.txt')):\n","        with open(filename, 'r') as f:\n","            dataset.append((neg_label, f.read()))\n","    \n","    shuffle(dataset)\n","    \n","    return dataset\n","\n","dataset = pre_process_data('./aclImdb/train')\n","print(dataset[0])"],"execution_count":8,"outputs":[{"output_type":"stream","text":["(1, 'I should admit first I am a huge fan of The Dandy Warhols, and that is the reason I came watching this film.<br /><br />The uniqueness of this film, compared to other modern rockumentaries, is that it\\'s not just about one page of a band\\'s history (like \"I Am Trying To Break Your Heart\", about Wilco), but rather covers long period of the band\\'s history. In this movie, director/producer Ondi Timoner closely followed friends/rivals The Brian Jonestown Massacre (BJM) and The Dandy Warhols (DW) for more than 8 years (1995 - 2003) and shoot tremendous 1500 hours of raw video, cut than to 1:45 hours (the future DVD release will contain much more material than the original film). The result is astonishing - there are no fillers - the film is 100% pure and genuine archive footage, which gives you feeling as film progresses that you live with the bands, through all these years.<br /><br />Both bands in the start of their careers promised to \"make a revolution\" in the music making, and not to sell their souls to the devil of \"record industry\". However, their paths quickly diverged - The Dandy Warhols signed a contract with Capitol Records and became relatively popular (especially in Europe) after only one album, while The Brian Jonestown Massacre (with its self-destruction-bound leader Anton Newcombe) dissolved into oblivion (at least how it is portrayed in the film). And the movie follows the descent of The Brian Jonestown Massacre, contrasted by the ascent of The Dandy Warhols.<br /><br />First, I was delighted by the movie and its approach of telling the story of Anton Newcombe (for example, Courtney Taylor - the leader of The Dandy Warhols - narrates), but after some thinking I realized that something is wrong with this film.<br /><br />First, it treats Anton Newcombe as a disappeared person. The project started in 1995 as a documentary about several promising emerging groups, in which Anton Newcombe and Ondi Timoner were equal partners (that was the reason why all these years Ondi Timoner had unmediated access to the both bands). It was Anton Newcombe who brought The Dandy Warhols into the project. In the end he was ignored completely, as if he was kicked out of the project. Everybody talk about BJM, but he does not take part in the discussion. I guess he wasn\\'t even informed when the group started the final editing process. There are always both sides of the story, and here we have only one... Of course, as one would expect, Anton does not approve the final result and sees this movie as a betrayal of his former friends.<br /><br />Second, the film is very Dandy Warhols-biased. Sure, the winner takes it all, but the fact that Courtney Taylor (leader of DW) narrates (even though it seems a good choice - it provides a feeling of seemingly closer involvement) and that bands\\' late history is represented nonproportionally (BJM is covered till 1997, and DW - till 2003), does not add objectivity to the film.<br /><br />Third, the movie is (somewhat) shallow. What does it want to teach us? As one critic said: \"... movie examines old questions: where does genius fit into a commodified world? Can it thrive and get its due, or does it need to self-destruct to preserve its integrity?\" No, IT DOES NOT EXAMINE these questions! It just depicts a story of a brilliant, but unsuccessful musician, narrated by a less brilliant, but successful one, who indulges in self-assurance and eternal coolness of an ego greater than mountain.<br /><br />Anyway, the movie was fun - it\\'s raw, it\\'s fresh, it\\'s stylish, it\\'s ... just god damn interesting, at least for the DW or BJM fans. For the rest of the crowd - I don\\'t know...')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dvbnMuGV8XUT","executionInfo":{"status":"ok","timestamp":1606654404797,"user_tz":-330,"elapsed":10099,"user":{"displayName":"Raghav Nyapati","photoUrl":"","userId":"13728981372697321629"}}},"source":["from nltk.tokenize import TreebankWordTokenizer\n","from gensim.models import KeyedVectors\n","word_vectors = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True, limit=200000)\n","\n","def tokenize_and_vectorize(dataset):\n","    tokenizer = TreebankWordTokenizer()\n","    vectorized_data = []\n","    expected = []\n","    for sample in dataset:\n","        tokens = tokenizer.tokenize(sample[1])\n","        sample_vecs = []\n","        for token in tokens:\n","            try:\n","                sample_vecs.append(word_vectors[token])\n","\n","            except KeyError:\n","                pass  # No matching token in the Google w2v vocab\n","            \n","        vectorized_data.append(sample_vecs)\n","\n","    return vectorized_data"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"ALa5z8d68XUT","executionInfo":{"status":"ok","timestamp":1606654554175,"user_tz":-330,"elapsed":1597,"user":{"displayName":"Raghav Nyapati","photoUrl":"","userId":"13728981372697321629"}}},"source":["def collect_expected(dataset):\n","    \"\"\" Peel of the target values from the dataset \"\"\"\n","    expected = []\n","    for sample in dataset:\n","        expected.append(sample[0])\n","    return expected"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":180},"id":"autOhrHw8XUT","executionInfo":{"status":"error","timestamp":1606654560594,"user_tz":-330,"elapsed":1696,"user":{"displayName":"Raghav Nyapati","photoUrl":"","userId":"13728981372697321629"}},"outputId":"68e883c7-c2c3-4ea9-d103-34ba95c9e47b"},"source":["vectorized_data = tokenize_and_vectorize(dataset)\n","expected = collect_expected(dataset)"],"execution_count":5,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-91d33bd26f00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvectorized_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize_and_vectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollect_expected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"]}]},{"cell_type":"code","metadata":{"id":"P0R5kmL38XUT","executionInfo":{"status":"ok","timestamp":1606654191226,"user_tz":-330,"elapsed":2169,"user":{"displayName":"Raghav Nyapati","photoUrl":"","userId":"13728981372697321629"}}},"source":["split_point = int(len(vectorized_data)*.8)\n","\n","x_train = vectorized_data[:split_point]\n","y_train = expected[:split_point]\n","x_test = vectorized_data[split_point:]\n","y_test = expected[split_point:]"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"WAEjw8Vv8XUT","executionInfo":{"status":"ok","timestamp":1606654263208,"user_tz":-330,"elapsed":1564,"user":{"displayName":"Raghav Nyapati","photoUrl":"","userId":"13728981372697321629"}}},"source":["maxlen = 400\n","batch_size = 32         # How many samples to show the net before backpropogating the error and updating the weights\n","embedding_dims = 300    # Length of the token vectors we will create for passing into the Convnet\n","filters = 250           # Number of filters we will train\n","kernel_size = 3         # The width of the filters, actual filters will each be a matrix of weights of size: embedding_dims x kernel_size or 50 x 3 in our case\n","hidden_dims = 250       # Number of neurons in the plain feed forward net at the end of the chain\n","epochs = 2              # Number of times we will pass the entire training dataset through the network"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"0sSDGWT08XUT","executionInfo":{"status":"ok","timestamp":1606654268779,"user_tz":-330,"elapsed":1588,"user":{"displayName":"Raghav Nyapati","photoUrl":"","userId":"13728981372697321629"}}},"source":["# Must manually pad/truncate\n","\n","def pad_trunc(data, maxlen):\n","    \"\"\" For a given dataset pad with zero vectors or truncate to maxlen \"\"\"\n","    new_data = []\n","\n","    # Create a vector of 0's the length of our word vectors\n","    zero_vector = []\n","    for _ in range(len(data[0][0])):\n","        zero_vector.append(0.0)\n","\n","    for sample in data:\n"," \n","        if len(sample) > maxlen:\n","            temp = sample[:maxlen]\n","        elif len(sample) < maxlen:\n","            temp = sample\n","            additional_elems = maxlen - len(sample)\n","            for _ in range(additional_elems):\n","                temp.append(zero_vector)\n","        else:\n","            temp = sample\n","        new_data.append(temp)\n","    return new_data"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"875Xu-7y8XUT"},"source":["x_train = pad_trunc(x_train, maxlen)\n","x_test = pad_trunc(x_test, maxlen)\n","\n","x_train = np.reshape(x_train, (len(x_train), maxlen, embedding_dims))\n","y_train = np.array(y_train)\n","x_test = np.reshape(x_test, (len(x_test), maxlen, embedding_dims))\n","y_test = np.array(y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fp9i6xZQ8XUT"},"source":["print('Build model...')\n","model = Sequential()\n","\n","# we add a Convolution1D, which will learn filters\n","# word group filters of size filter_length:\n","model.add(Conv1D(filters,\n","                 kernel_size,\n","                 padding='valid',\n","                 activation='relu',\n","                 strides=1,\n","                 input_shape=(maxlen, embedding_dims)))\n","# we use max pooling:\n","model.add(GlobalMaxPooling1D())\n","# We add a vanilla hidden layer:\n","model.add(Dense(hidden_dims))\n","model.add(Dropout(0.2))\n","model.add(Activation('relu'))\n","# We project onto a single unit output layer, and squash it with a sigmoid:\n","model.add(Dense(1))\n","model.add(Activation('sigmoid'))\n","model.compile(loss='binary_crossentropy',\n","              optimizer='adam',\n","              metrics=['accuracy'])\n","model.fit(x_train, y_train,\n","          batch_size=batch_size,\n","          epochs=epochs,\n","          validation_data=(x_test, y_test))\n","model_structure = model.to_json()\n","with open(\"cnn_model.json\", \"w\") as json_file:\n","    json_file.write(model_structure)\n","\n","model.save_weights(\"cnn_weights.h5\")\n","print('Model saved.')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YYhrIHkl8XUT"},"source":["from keras.models import model_from_json\n","with open(\"cnn_model.json\", \"r\") as json_file:\n","    json_string = json_file.read()\n","model = model_from_json(json_string)\n","\n","model.load_weights('cnn_weights.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-03r38Ag8XUT"},"source":["sample_1 = \"I'm hate that the dismal weather that had me down for so long, when will it break! Ugh, when does happiness return?  The sun is blinding and the puffy clouds are too thin.  I can't wait for the weekend.\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pSe1qsXM8XUT"},"source":["# We pass a dummy value in the first element of the tuple just because our helper expects it from the way processed the initial data.  That value won't ever see the network, so it can be whatever.\n","vec_list = tokenize_and_vectorize([(1, sample_1)])\n","\n","# Tokenize returns a list of the data (length 1 here)\n","test_vec_list = pad_trunc(vec_list, maxlen)\n","\n","test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen, embedding_dims))\n","model.predict(test_vec)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z6PFl5cy8XUT"},"source":["model.predict_classes(test_vec)"],"execution_count":null,"outputs":[]}]}