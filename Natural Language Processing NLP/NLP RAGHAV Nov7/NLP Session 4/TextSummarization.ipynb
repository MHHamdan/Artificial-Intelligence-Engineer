{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"TextSummarization.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"XlIo08bpB0Qm"},"source":["## Summarization with Sumy\n","\n","### Sumy offers several algorithms and methods for summarization such as:\n","\n","\n","\n","    1. Luhn – Heurestic method\n","    2. Latent Semantic Analysis\n","    4. LexRank – Unsupervised approach inspired by algorithms PageRank and HITS\n","    5. TextRank - Graph-based summarization technique with keyword extractions in from document\n","There are many more which you can find in the github repo of [sumy](https://github.com/miso-belica/sumy)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fE-e9HKKB0Qv","scrolled":true,"executionInfo":{"status":"ok","timestamp":1606066782187,"user_tz":-330,"elapsed":11118,"user":{"displayName":"Raghav Nyapati","photoUrl":"","userId":"13728981372697321629"}},"outputId":"229cb3a1-0edd-4ca1-d4c0-428c50ba3799"},"source":["!pip install sumy #install sumy"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting sumy\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/61/20/8abf92617ec80a2ebaec8dc1646a790fc9656a4a4377ddb9f0cc90bc9326/sumy-0.8.1-py2.py3-none-any.whl (83kB)\n","\r\u001b[K     |████                            | 10kB 22.7MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 20kB 28.3MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 30kB 33.4MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 40kB 30.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 51kB 32.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 61kB 34.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 71kB 26.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 81kB 24.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 92kB 11.0MB/s \n","\u001b[?25hCollecting breadability>=0.1.20\n","  Downloading https://files.pythonhosted.org/packages/ad/2d/bb6c9b381e6b6a432aa2ffa8f4afdb2204f1ff97cfcc0766a5b7683fec43/breadability-0.1.20.tar.gz\n","Requirement already satisfied: nltk>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from sumy) (3.2.5)\n","Collecting pycountry>=18.2.23\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/73/6f1a412f14f68c273feea29a6ea9b9f1e268177d32e0e69ad6790d306312/pycountry-20.7.3.tar.gz (10.1MB)\n","\u001b[K     |████████████████████████████████| 10.1MB 25.7MB/s \n","\u001b[?25hRequirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.6/dist-packages (from sumy) (2.23.0)\n","Requirement already satisfied: docopt<0.7,>=0.6.1 in /usr/local/lib/python3.6/dist-packages (from sumy) (0.6.2)\n","Requirement already satisfied: chardet in /usr/local/lib/python3.6/dist-packages (from breadability>=0.1.20->sumy) (3.0.4)\n","Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python3.6/dist-packages (from breadability>=0.1.20->sumy) (4.2.6)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk>=3.0.2->sumy) (1.15.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->sumy) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->sumy) (2020.6.20)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->sumy) (2.10)\n","Building wheels for collected packages: breadability, pycountry\n","  Building wheel for breadability (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21681 sha256=94e9cedf41aad0beee0cec73a16979b833ce14622f0477ff7a34eca00ac655c1\n","  Stored in directory: /root/.cache/pip/wheels/5a/4d/a1/510b12c5e65e0b2b3ce539b2af66da0fc57571e528924f4a52\n","  Building wheel for pycountry (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pycountry: filename=pycountry-20.7.3-py2.py3-none-any.whl size=10746865 sha256=978fc8100a474f99c1b7fcb542aea20891c9bd16eebf3a7b0bcd73a6fea12402\n","  Stored in directory: /root/.cache/pip/wheels/33/4e/a6/be297e6b83567e537bed9df4a93f8590ec01c1acfbcd405348\n","Successfully built breadability pycountry\n","Installing collected packages: breadability, pycountry, sumy\n","Successfully installed breadability-0.1.20 pycountry-20.7.3 sumy-0.8.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZVKqAHCEB0R0","executionInfo":{"status":"ok","timestamp":1606067188060,"user_tz":-330,"elapsed":2289,"user":{"displayName":"Raghav Nyapati","photoUrl":"","userId":"13728981372697321629"}},"outputId":"630c9a7c-7c30-40bc-9ef4-3fbb6241308a"},"source":["import nltk\n","nltk.download('punkt')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SVu_YDlXB0SR","executionInfo":{"status":"ok","timestamp":1606050134188,"user_tz":-330,"elapsed":12004,"user":{"displayName":"Raghav Nyapati","photoUrl":"","userId":"13728981372697321629"}},"outputId":"c473650e-3d1a-4834-c9b9-683c5e0df3a1"},"source":["#Code to summarize a given webpage using Sumy's TextRank implementation. \n","from sumy.parsers.html import HtmlParser\n","from sumy.nlp.tokenizers import Tokenizer\n","from sumy.summarizers.text_rank import TextRankSummarizer\n","from sumy.summarizers.lex_rank import LexRankSummarizer\n","from sumy.summarizers.luhn import LuhnSummarizer\n","from sumy.summarizers.lsa import LsaSummarizer\n","\n","num_sentences_in_summary = 2\n","url = \"https://en.wikipedia.org/wiki/Automatic_summarization\"\n","parser = HtmlParser.from_url(url, Tokenizer(\"english\"))\n","\n","summarizer_list=(\"TextRankSummarizer:\",\"LexRankSummarizer:\",\"LuhnSummarizer:\",\"LsaSummarizer\") #list of summarizers\n","summarizers = [TextRankSummarizer(), LexRankSummarizer(), LuhnSummarizer(), LsaSummarizer()]\n","\n","for i,summarizer in enumerate(summarizers):\n","    print(summarizer_list[i])\n","    for sentence in summarizer(parser.document, num_sentences_in_summary):\n","        print((sentence))\n","    print(\"-\"*30)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["TextRankSummarizer:\n","For text, extraction is analogous to the process of skimming, where the summary (if available), headings and subheadings, figures, the first and last paragraphs of a section, and optionally the first and last sentences in a paragraph are read before one chooses to read the entire document in detail.\n","Once the graph is constructed, it is used to form a stochastic matrix, combined with a damping factor (as in the \"random surfer model\"), and the ranking over vertices is obtained by finding the eigenvector corresponding to eigenvalue 1 (i.e., the stationary distribution of the random walk on the graph).\n","------------------------------\n","LexRankSummarizer:\n","The main difficulty in supervised extractive summarization is that the known summaries must be manually created by extracting sentences so the sentences in an original training document can be labeled as \"in summary\" or \"not in summary\".\n","Automatic Text Summarization .\n","------------------------------\n","LuhnSummarizer:\n","This tool does not use word frequency, does not need training or preprocessing of any kind and works by generating ideograms that represent the meaning of each sentence and then summarizes using two user-supplied parameters: equivalence (when are two sentences to be considered equivalent) and relevance (how long is the desired summary).\n","A Class of Submodular Functions for Document Summarization \", The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT), 2011 ^ Sebastian Tschiatschek, Rishabh Iyer, Hoachen Wei and Jeff Bilmes, Learning Mixtures of Submodular Functions for Image Collection Summarization , In Advances of Neural Information Processing Systems (NIPS), Montreal, Canada, December - 2014.\n","------------------------------\n","LsaSummarizer\n","Hulth uses a reduced set of features, which were found most successful in the KEA (Keyphrase Extraction Algorithm) work derived from Turney’s seminal paper.\n","Although they did not replace other approaches and are often combined with them, by 2019 machine learning methods dominated the extractive summarization of single documents, which was considered to be nearing maturity.\n","------------------------------\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"i5Qyd9M2B0Ss"},"source":["Clearly there are other summarizers and options in sumy. We leave their exploration as an exercise to you!\n","\n","## Summarization example with Gensim"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TZOIdGesB0Sy","executionInfo":{"status":"ok","timestamp":1606067401990,"user_tz":-330,"elapsed":3017,"user":{"displayName":"Raghav Nyapati","photoUrl":"","userId":"13728981372697321629"}},"outputId":"a0ecb1a4-c825-4e33-975b-7612706f23cf"},"source":["!pip install gensim #installation of the library"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n","Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.15.0)\n","Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.18.5)\n","Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n","Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (3.0.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.23.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2020.6.20)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.24.3)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"YB6IqAnrB0TL"},"source":["Gensim does not have a HTML parser like sumy. So, let us use the example text from Chapter 5 (nlphistory.txt) to see what its summarized version looks like! \n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8dHkUgCoB0TP","executionInfo":{"status":"ok","timestamp":1606067428065,"user_tz":-330,"elapsed":1118,"user":{"displayName":"Raghav Nyapati","photoUrl":"","userId":"13728981372697321629"}},"outputId":"db65049c-9f30-4b9e-f15e-752abf238549"},"source":["from gensim.summarization import summarize,summarize_corpus\n","from gensim.summarization.textcleaner import split_sentences\n","from gensim import corpora\n","\n","text = open(\"nlphistory.txt\").read()\n","\n","#summarize method extracts the most relevant sentences in a text\n","print(\"Summarize:\\n\",summarize(text, word_count=200, ratio = 0.1))\n","\n","\n","#the summarize_corpus selects the most important documents in a corpus:\n","sentences = split_sentences(text)# Creates a corpus where each document is a sentence.\n","tokens = [sentence.split() for sentence in sentences]\n","dictionary = corpora.Dictionary(tokens)\n","corpus = [dictionary.doc2bow(sentence_tokens) for sentence_tokens in tokens]\n","\n","# Extracts the most important documents (shown here in BoW representation)\n","print(\"-\"*30,\"\\nSummarize Corpus\\n\",summarize_corpus(corpus,ratio=0.1))\n","\n","\n"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Summarize:\n"," Some notably successful natural language processing systems developed in the 1960s were SHRDLU, a natural language system working in restricted \"blocks worlds\" with restricted vocabularies, and ELIZA, a simulation of a Rogerian psychotherapist, written by Joseph Weizenbaum between 1964 and 1966.\n","This was due to both the steady increase in computational power (see Moore's law) and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing.[3] Some of the earliest-used machine learning algorithms, such as decision trees, produced systems of hard if-then rules similar to existing hand-written rules.\n","However, part-of-speech tagging introduced the use of hidden Markov models to natural language processing, and increasingly, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to the features making up the input data.\n","Generally, this task is much more difficult than supervised learning, and typically produces less accurate results for a given amount of input data.\n","In some areas, this shift has entailed substantial changes in how NLP systems are designed, such that deep neural network-based approaches may be viewed as a new paradigm distinct from statistical natural language processing.\n","------------------------------ \n","Summarize Corpus\n"," [[(3, 1), (7, 1), (10, 1), (11, 1), (12, 1), (22, 1), (24, 1), (27, 1), (80, 1), (94, 1), (95, 1), (193, 1), (199, 1), (214, 1), (249, 1), (262, 1), (413, 1), (418, 1), (449, 1), (450, 1), (451, 1), (452, 1), (453, 1), (454, 1), (455, 1), (456, 1), (457, 1), (458, 1), (459, 1), (460, 1), (461, 1), (462, 1), (463, 1), (464, 1)], [(11, 1), (12, 1), (13, 1), (17, 3), (38, 1), (55, 1), (57, 1), (76, 1), (82, 2), (94, 1), (164, 1), (203, 1), (206, 2), (257, 1), (258, 1), (259, 1), (260, 1), (261, 1), (262, 1), (263, 1), (264, 1), (265, 1), (266, 1), (267, 1), (268, 1), (269, 1), (270, 1), (271, 1), (272, 1), (273, 1), (274, 1), (275, 1), (276, 1), (277, 1), (278, 1), (279, 1)]]\n"],"name":"stdout"}]}]}